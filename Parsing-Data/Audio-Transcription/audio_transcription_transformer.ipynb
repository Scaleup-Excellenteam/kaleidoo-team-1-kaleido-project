{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "import torch\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model and tokenizer\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load an audio file (must be 16kHz, mono-channel)\n",
    "speech, sample_rate = sf.read(\"audio_file.wav\")\n",
    "\n",
    "# Tokenize the audio\n",
    "input_values = tokenizer(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "# Get the model's predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Decode the predicted token IDs back to text\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "print(f\"Transcription: {transcription}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
