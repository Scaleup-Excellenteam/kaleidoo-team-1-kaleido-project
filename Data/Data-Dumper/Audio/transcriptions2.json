[
    {
        "type": "audio",
        "ref": "/home/ameer/Kaleidoo/Data/Audio_Data/English/Data_Science.mp3",
        "met_data": [
            {
                "offset": "0:00:00, 0:01:20",
                "text": " Hello guys, welcome back. Analyzing data can be a challenging task. Have you ever wanted to do data analysis? Plot some visuals and train machine learning model, but don't know where to start from? Then chat zpt can help you do exactly that with clear information. So you know what you are doing in each steps. In this video, first we will load the data using the Python package called pandas. And we will...",
                "lang": "en"
            },
            {
                "offset": "0:01:20, 0:02:40",
                "text": " I find is if I give some information to chat zpt that what we are going to do that it gives better results. So let's just say char gpt that okay I copied something here okay hello we will be doing eda lot some visualizations and train and test machine. So we give some basic information to the char gpt so it's okay great. So I'm here to help some kind of thing. So it gives better results according to me.",
                "lang": "en"
            },
            {
                "offset": "0:02:40, 0:04:00",
                "text": " not in one code block. So if we give this, it gives us code is maybe the step by step. So we can copy and paste in the Jupyter notebook or Google go live in our guest. And also don't use the print statement because in notebooks, we don't want the print statement. We just want to display in a data frame. So we don't want to use the print statement. So this is what the information I went to",
                "lang": "en"
            },
            {
                "offset": "0:04:00, 0:05:20",
                "text": " it uses read csv, pd.readcsv to read the csv file. And what do we do generally when we loaded it? We want to see how many rows and how many columns are there in the dataset. So let's ask there how can I know how many rows and how many columns are there in the dataset.",
                "lang": "en"
            },
            {
                "offset": "0:05:20, 0:06:40",
                "text": " and see if it says okay there are this many rows and this many columns but there is a good way in python or in pandas but it also already provides here like this code uses the save attribute but how can we use how okay let's how can we use the save method pandas so I just give the information how",
                "lang": "en"
            },
            {
                "offset": "0:06:40, 0:08:00",
                "text": " Or get this spelling or miss type this spelling. Okay, so it has 891 rows and 12 columns. So what we do next? Next we have the data now. We loaded the data. We know that there are 891 rows and 12 columns. So what next? Let's say that we want to know some information about the data, right? So let's say that how can I know the",
                "lang": "en"
            },
            {
                "offset": "0:08:00, 0:09:20",
                "text": " will provide you the how many rows are there. What is the type of the data? It means that for example, if you see the result here in the first, there is the colon names and there is the non-nall values of the count of non-nall. I can be speak right. There is the non-nall count of the dataset and there is the D type. What is the D type of the dataset?",
                "lang": "en"
            },
            {
                "offset": "0:09:20, 0:10:40",
                "text": " equation. Let's give again if it follows what we said because it's again and again loading the data. So it says you can use the head method. Okay, that's great. It gives the n rows and columns. So let's use here. Here is an example. It is also going to give us the example and print. So that is it. So we can type data set dot head. Let's shift in term.",
                "lang": "en"
            },
            {
                "offset": "0:10:40, 0:12:00",
                "text": " like what is the data? Is there any relationship between the different columns and so on? So here, sure, exploratively data analysis involves investigating and summarizing the main characteristics of the dataset. So it keeps on explaining all these things and it says, on the first, we can check the missing values. So missing values can affect the quality of our analysis and machine learning models. So yeah, it says, let's check the missing",
                "lang": "en"
            },
            {
                "offset": "0:12:00, 0:13:20",
                "text": " So the good part of notebooks is that you can write the code as well as write some documentation as you go. So you no need to explain the indifferent format or no need to write the different documentation. You can write the documentation as well as code at the same time. So yeah, let's take some missing values here, control V and then save the int. Okay. It says that there are some of the missing values here. So the age has some missing values.",
                "lang": "en"
            },
            {
                "offset": "0:13:20, 0:14:40",
                "text": " right from the data set. So this is how you can you can go into depth of the data set. Okay, how many, how many counts are there of the data? For example, let's say what is the mean age of the passengers? So here you can go to the mean and the age and it's 29.69. So the mean age of the passengers in that hygienic shape was 29.6. So you can do more exploration. And now it says next one is visualize the distribution of the numerical columns.",
                "lang": "en"
            },
            {
                "offset": "0:14:40, 0:16:00",
                "text": " of the column has some relationship with the end of that column, right? So it says that age and fear. Let's see if there is some relationship between the age and the fear. Let's copy this. And then let's paste in the Google call app, shift enter to run the shell. And now you see that the fear is as it increased up and the age is in the ex column and fear is in the white column. So you can see what is",
                "lang": "en"
            },
            {
                "offset": "0:16:00, 0:17:20",
                "text": " meaning that in this case it is survived column. So survived. So we want to know what is the distribution of the people that survived and that didn't survived in the Titanic, right? So to examine the distribution of target column survived we can create a bar plot. So from there we can see how many survived and how many didn't survived. So let's wait for the result from the charge ZPD so it's",
                "lang": "en"
            },
            {
                "offset": "0:17:20, 0:18:40",
                "text": " how many of those survived and not survived are male and female. Right? So we want to know how many of those are male and how many of those are female. To examine how many of those survived are not how many of those who survived and not survived are male and female, we can create a stacked bar products in the distribution.",
                "lang": "en"
            },
            {
                "offset": "0:18:40, 0:20:00",
                "text": " on a highly well what are the things that we can look into the dataset right. So now we get the idea of some of the plots but now let's see let's also do the count plot of C-blinks and this spouse who in here. So now we can even go and ask okay can you plot the district view some plot of C-blinks.",
                "lang": "en"
            },
            {
                "offset": "0:20:00, 0:21:20",
                "text": " cbsp column we can use the dist plot. So you get the idea I'm I'm plotting here just for one column but you can do this for each and every column in the data set. So here it gives a c bone okay let's copy this code and then let's paste it here and then run this. So here you see the distribution of the cbsp. So number of cblings and his pauses 0 1 2 3",
                "lang": "en"
            },
            {
                "offset": "0:21:20, 0:22:40",
                "text": " based on the passenger class. Let's ask Chargity, can you plot how many people survived based on the P class? So we want to know that is there any correlation?",
                "lang": "en"
            },
            {
                "offset": "0:22:40, 0:24:00",
                "text": " and doing some extra numerical values. So here it says yes and no, no and yes, it's the survived and the count on the x-axis. And there is the p class 1, 2, 3, that's the legend. So here it says one is blue, orange, two and green. So you see that who does not survive mostly the people were in the thoughtless, meaning that that's the cheapest class in the ship. So from here we see that the people who...",
                "lang": "en"
            },
            {
                "offset": "0:24:00, 0:25:20",
                "text": " how many missing values are there? Can you plot or hit map showing the missing values? So that we know which columns has what kind of or how many missing values from the visual itself. So then after this, what we will do is we will impute the missing values with the average in this case, meaning the missing values.",
                "lang": "en"
            },
            {
                "offset": "0:25:20, 0:26:40",
                "text": " We already see the relationship right here. The passenger class one and two and three has the different number of people that survived and not survived. But if we impute with the overall age, then the data, there might be some bias in the data. So what we can do is we can impute the missing values for those classes with the average age of those classes. I hope you understand what I mean.",
                "lang": "en"
            },
            {
                "offset": "0:26:40, 0:28:00",
                "text": " if the answer or not. Sure, we can replace the missing value in the A's based on the average A's of that particular P class column, here is the function. So it's creating a function impute A's by P class. So it says P class A's and mean and for class in this, OK, there is the function. And it replaces the function Hetexo data frame. That's the main part, D of as input. Groups the data by P class and calculates the mean.",
                "lang": "en"
            },
            {
                "offset": "0:28:00, 0:29:20",
                "text": " of the age column based on what we actually asked it for. Okay, so now we deal the missing values of the age. What about the cabin? But before doing that, what I want to ask Charles ZPT is what are the main columns that can affect the target column, right? Let's just ask if it gives some answers so that it makes our life easier. So what are the main columns that might have allowed for our pilot?",
                "lang": "en"
            },
            {
                "offset": "0:29:20, 0:30:40",
                "text": " then two family members appalled. So yeah, and also it says something here, the this column seem to have a strong correlation with the survived column. So here I'm referring to this one. And then however, it's worth noting that there may be other factors that we have not explored that could also have a significant impact and survival. Such as the impact column or the fear column for the analysis and modeling would be needed to determine. So it's up to us. But I think the caveat",
                "lang": "en"
            },
            {
                "offset": "0:30:40, 0:32:00",
                "text": " Let's go here, Ctrl V, Shift Enter. So now let's do, let's say, let us it.head. It should remove the cabin column. So here is the passenger ID, survived P class and all these things. But now we don't have the cabin column here. This is how you can remove the data column.",
                "lang": "en"
            },
            {
                "offset": "0:32:00, 0:33:20",
                "text": " where we had data.info, where is the data.info here? We have this object rights name, sex and ticket and fear. They are objects. Of here is not object ticket. These are the objects. So, let's see what the activity thinks about. And I think we don't need ticket to be in our data set, but let's see what the best object is.",
                "lang": "en"
            },
            {
                "offset": "0:33:20, 0:34:40",
                "text": " So, but generally what it says here is which involves creating a new binary column. It already explains here. So let's go through this. Which involves creating a new binary column for each unique category in categorical columns. For example, we would create two new columns for the sex column one indicating whether a passenger is male and another indicating whether a passenger that is female. The values in these in those columns would be",
                "lang": "en"
            },
            {
                "offset": "0:34:40, 0:36:00",
                "text": " female that it says here. That is really interesting point here and let's go here in the second column what it's saying here we call the gate DOM-Miche function. This is really important if you are running machine learning stops. Here we call the gate DOM-Miche function on the dataset data frame is specifying the names of the categorical columns to encode as a list of strings in the columns argument. The drop-up",
                "lang": "en"
            },
            {
                "offset": "0:36:00, 0:37:20",
                "text": " So we have sex male one, meaning that it already knows that the sex female will be zero. Right? So for example here it says sex male is one, it means that the next one is already zero. So there is no point of including that into the data frame because in machine learning the more columns you have, it's harder for the machine to learn from it also. So just use the one which is",
                "lang": "en"
            },
            {
                "offset": "0:37:20, 0:38:40",
                "text": " coding part. So what are the things we can remove from this data frame now? We can remove the name, we can remove the ticket. We don't need those. So how can we do that? You can just go ahead and ask how can I remove multiple columns? Let's say for example, what we want to remove name and ticket, but what was it? Name and",
                "lang": "en"
            },
            {
                "offset": "0:38:40, 0:40:00",
                "text": " to the machine learning part. Let's ask charge zpt, what are the steps I need to take to train our machine learning model, right? Let's ask charge zpt now how to train the logistic regression model. Because now we already did the exploratory data analysis part and now we want to train the model, right? Okay, to train our logistic regression model on the dataset, we first need to split the data into training and testing sets. We can use the train test split.",
                "lang": "en"
            },
            {
                "offset": "0:40:00, 0:41:20",
                "text": " from our training data, right? So x equals to data set the drop survived, y equals to data set survived. So here we split the data into training and testing. So now it says x train x test, y train y test. So train test split it is taking x and y and it is taking the test data as 0.2 meaning that 20% of the data and random status for it will meaning that this is the common",
                "lang": "en"
            },
            {
                "offset": "0:41:20, 0:42:40",
                "text": " data from the X-trained and then it will use Y-trained, both of these, to feed the data. So let's say that we run this and then it goes on saying here finally we can use the train model to make predictions on the testing data and evaluate its performance. So this is how the machine learning model works, right? So here, okay there is some error but it's fine I think so the model is already",
                "lang": "en"
            },
            {
                "offset": "0:42:40, 0:44:00",
                "text": " the model accuracy 0.76. Great job. We didn't do anything and we trained a model logistic regression with the help of charge-upity. Okay great we have 76% accuracy. So what next? Okay so now we know that the accuracy is 0.76 right but how can we know that the model is performing will or not. Let's ask charge-upity what it thinks about.",
                "lang": "en"
            },
            {
                "offset": "0:44:00, 0:45:20",
                "text": " to know more about this please use chat zpt2 to go through this but it's giving all of this here let's just copy this and paste it here just to see what it performs okay so here you can see that the accuracy is 76 precision is 73 recall is 67 again I'm not going through how it is calculated you can go through it by yourself or ask chat zpt about this and it's also explaining here",
                "lang": "en"
            },
            {
                "offset": "0:45:20, 0:46:40",
                "text": " So it prints the confuse and metrics. So based instead of going through this accuracy, precision, all these things, it's quite easier than if we see visual, right? So here there is true labels and predicted labels, true labels in the y axis, predicted label in the x axis, and this is the confuse and metrics. And there are some numbers. So what are these? Here on the right side, Chazipiti is providing us what this is, right? So here it says that the true labels.",
                "lang": "en"
            },
            {
                "offset": "0:46:40, 0:48:00",
                "text": " In the machine learning concept, the data, for example, I haven't explained this and I just want to show you how the model performs. But one last thing before I finish this video is you need to scale the dataset. Here we just have simple numbers, but in some cases, and if you are doing with some huge numbers, for example, let's first ask ZipRace.",
                "lang": "en"
            },
            {
                "offset": "0:48:00, 0:49:20",
                "text": " to the scale of the input features. For example, in logistic regression, the optimizers that algorithm may take longer to converse and there are different methods. There are different explanations why we need to scale the data. So here it says that there is a standard scalar we can use and it's scaling the data here, scaled scalar the transform and so on. So, let's say that",
                "lang": "en"
            },
            {
                "offset": "0:49:20, 0:50:40",
                "text": " speed of the data right. So now we can do X test but we need to provide here scaled. So as you see a LED scaled and then here also no X we don't need to scale that train, white train. So here we are training the model based on the scale value. So now we feed the model. Now what we need to do is we need to predicate.",
                "lang": "en"
            },
            {
                "offset": "0:50:40, 0:51:55.746000",
                "text": " always remember that when you train the machine learning model, you need to think different things. So scaling is one of the parts and we are using the standard scale here. There are many other scaling techniques also we can use, but always think that scaling the model is really good way to train the model. I hope you, yeah, we went through many, many things here and I want to make this kind of task course from all the way from loading the model.",
                "lang": "en"
            }
        ]
    }
]