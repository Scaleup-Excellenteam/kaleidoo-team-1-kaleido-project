{
  "Audio": {},
  "Text": {
    "0d05d0bcbfc6431ab355873387cb052d.json": [
      {
        "type": "application/pdf",
        "text": "Computer Vision\nLihi Zelnik-Manor lihi@ee.technion.ac.il",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color\nColor\n`\nReadings:\n– Forsyth and Ponce, Chapter 6 – Szeliski, 2.3.2",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide Credits\nTrevor Darrell • Kristen Grauman: 3-48, 50-75, 79-86 • Bob Woodham: 49, 87-90 • and others, indirectly (Steve Palmer, Brian Wandell, etc!)",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Color\nMeasuring color\n– Spectral power distributions – Color mixing – Color matching experiments – Color spaces\nUniform color spaces\nPerception of color\n– Human photoreceptors – Environmental effects, adaptation\nUsing color in machine vision systems",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color and light\nColor of light arriving at camera depends on\n– Spectral reflectance of the surface light is leaving – Spectral radiance of light falling on that patch\nColor perceived depends on\n– Physics of light – Visual system receptors – Brain processing, environment",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color and light\nWhite light: composed of about equal energy in all wavelengths of the visible spectrum\nNewton 1665\nImage from http://micro.magnet.fsu.edu/",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Electromagnetic spectrum\nHuman Luminance Sensitivity Function\nImage credit: nasa.gov",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring spectra\nSpectroradiometer: separate input light into its different wavelengths, and measure the energy at each.\nFoundations of Vision, B. Wandell",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Spectral power distribution\nThe power per unit area at each wavelength of a radiant object\n# Photons(per ms.)\n400 500 600 700Wavelength (nm.)\nFigure © Stephen E. Palmer, 2002",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Spectral power distributions\nSome examples of the spectra of light sources\n# PhotonsC. Tungsten Lightbulb400 500 600 700\n# Photons\n# PhotonsD. Normal DaylightWavelength (nm.)B. Gallium Phosphide Crystal400 500 600 700# PhotonsWavelength (nm.)A. Ruby Laser\n.\n400 500 600 700400 500 600 700\n© Stephen E. Palmer, 2002",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The color viewed is also affected by the surface’s spectral reflectance properties.\nSpectral reflectances for some natural objects: how much of each wavelength is reflected for that surface\nForsyth & Ponce, measurements by E. Koivisto",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "d e\nt c e\nl f\ne R s n o\nt\no h P %\nSurface reflectance spectra\nSome examples of the reflectance spectra of surfaces\nRed\nYellow\nBlue\nPurple\n400 700\n400 700\n400 700\n400 700\nWavelength (nm)\n© Stephen E. Palmer, 2002",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Psychophysical Correspondence\nThere is no simple functional description for the perceived color of all lights under all viewing conditions, but …...\nA helpful constraint:\nConsider only physical spectra with normal distributions\nmean\n500600\nWavelength (nm.)# Photons400700\narea\nvariance\n© Stephen E. Palmer, 2002",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Psychophysical Correspondence\nMean\nHue\ns n o\nt\no h P #\nyellowgreenblue\nWavelength\n© Stephen E. Palmer, 2002",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Psychophysical Correspondence\nVariance\nSaturation\nhighmediumlow\nhi.med.low\ns n o\nt\no h P #\nWavelength\n© Stephen E. Palmer, 2002",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Psychophysical Correspondence\nArea\nBrightness\ns n o\nt\no h P #\nbrightdark\nB. Area Lightness\nWavelength\n© Stephen E. Palmer, 2002",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color mixing\nCartoon spectra for color names:\nSource: W. Freeman",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Additive color mixing\nColors combine by adding color spectra\nLight adds to black.\nSource: W. Freeman",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of additive color systems\nCRT phosphors\nmultiple projectors\nhttp://www.jegsworks.com\nhttp://www.crtprojectors.co.uk/",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Subtractive color mixing\nColors combine by multiplying color spectra.\nPigments remove color from incident light (white).\nSource: W. Freeman",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of subtractive color systems\nPrinting on paper • Most photographic film",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Color\nMeasuring color\n– Spectral power distributions – Color mixing – Color matching experiments – Color spaces\nUniform color spaces\nPerception of color\n– Human photoreceptors – Environmental effects, adaptation\nUsing color in machine vision systems",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Why specify color numerically?\nAccurate color reproduction is commercially valuable – Many products are identified by color\nFew color names are widely recognized by English speakers – 11: black, blue, brown, grey, green, orange, pink, purple, red, white, and\nyellow.\n– Other languages have fewer/more. – Common to disagree on appropriate color names.\nColor reproduction problems increased by prevalence of digital imaging – e.g. digital libraries of art. – How to ensure that everyone perceives the same color? – What spectral radiances produce the same response from people under\nsimple viewing conditions?\nForsyth & Ponce",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color matching experiments\nGoal:\nWhat spectral radiances produce same response in human observers?",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color matching experiments\nObserver adjusts weight (intensity) for primary lights (fixed SPD’s) to match appearance of test light.\nFoundations of Vision, by Brian Wandell, Sinauer Assoc., 1995\nAfter Judd & Wyszecki.",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color matching experiments\nGoal:\nWhat spectral radiances produce same response in human observers?\nAssumption:\nUnder simple viewing conditions only “test light” affects perception – Ignoring additional factors for now like adaptation,\ncomplex surrounding scenes, etc.",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 1\nTest light\nPrimary lights",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 1\nTest light\nPrimary lights\np1 p2 p3",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 1\nTest light\nPrimary lights\np1 p2 p3",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 1\nTest light\nPrimary lights\nThe primary color amounts needed for a match\np1 p2 p3",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 2\nTest light\nPrimary lights",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 2\nTest light\nPrimary lights\np1 p2 p3",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credit: W. Freeman\nColor matching experiment 2\nTest light\nPrimary lights\np1 p2 p3",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color matching experiment 2\nWe say a “negative” amount of p2 was needed to make the match, because we added it to the test color’s side.\nTest light\nPrimary lights\nThe primary color amounts needed for a match:\np1 p2 p3\np1 p2 p3\np1 p2 p3",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color matching\nWhat must we require of the primary lights chosen? • How are three numbers enough to represent entire 121233ecolorePPPe\nWhat must we require of the primary lights chosen? • How are three numbers enough to represent entire spectrum?\nweights",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Metamers\nLights forming a perceptual match still may be physically different\n– Match light: a combination of primaries – Test light: any light\nMetamers: pairs of lights that match perceptually but not physically\nDifferent spectrum\nSame primary mixture weights",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to compute the weights of the primaries to match any new spectral signal?\nGiven: a choice of three primaries and a target color signal\nFind: weights of the primaries needed to match the color signal\n?\np1 p2 p3\ne1\ne2 e3\nChallenge: we cannot use manual tuning for all colors in the world",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing color matches\n1.\nSelect primaries\n2. Collect weights for all monochromatic lights:\n112131ccc\nPut them in a big matrix:\n111212313()()()()()()NNNcccccc",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing color matches\n1.\nSelect primaries\n2. Collect weights for all monochromatic lights:\n112131ccc\n3. Compute the weights of any color by:\n1111121223133()()()()()()NNNNtccecceccet",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing color matches\nArbitrary new spectral signal is linear combination of the monochromatic sources.\n)()(1Nttt\nt\n…\nColor matching functions specify how to match a unit of each wavelength, so:\n)()()()()()()()()(21313212111321NNNNtttcccccceee\nCte\nKristen Grauman",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing color matches\nWhy is computing the color\nmatch for any color signal for a given set of primaries useful? – Want to paint a carton of Kodak film\nwith the Kodak yellow color. – Want to match skin color of a\nperson in a photograph printed on an ink jet printer to their true skin color.\n– Want the colors in the world, on a monitor, and in a print format to all look the same.\nAdapted from W. Freeman\nImage credit: pbs.org",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Color\nMeasuring color\n– Spectral power distributions – Color mixing – Color matching experiments – Color spaces\nUniform color spaces\nPerception of color\n– Human photoreceptors – Environmental effects, adaptation\nUsing color in machine vision systems",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Standard color spaces\nUse a common set of primaries/color matching functions\nLinear color space examples\n– RGB – CIE XYZ\nNon-linear color space\n– HSV – CIE LAB",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RGB color space\nSingle wavelength primaries • Good for devices (e.g., phosphors for monitor), but not for perception\nRGB color matching functions",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CIE XYZ color space\nEstablished by the commission international d’eclairage (CIE), 1931\nUsually projected to display: (x,y) = (X/(X+Y+Z), Y/(X+Y+Z))\nCIE XYZ Color matching functions",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Distances in color space\nAre distances between points in a color space perceptually meaningful?",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Distances in color space\nNot necessarily: CIE XYZ is not a uniform color space, so magnitude of differences in coordinates are poor indicator of color “distance”.\nMcAdam ellipses: Just noticeable differences in color",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CIE XYZ\nUniform color spaces\nCIE Lu’v’\nCIE Lab",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CIE LAB color space\nEstablished by the CIE in 1948 and then 1976 • Goal: perceptually uniform",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "HSV color space\nHue, Saturation, Value (Brightness) • Nonlinear – reflects topology of colors by coding hue as an angle\nIntuitive for color picking\nImage from mathworks.com",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Color\nMeasuring color\n– Spectral power distributions – Color mixing – Color matching experiments – Color spaces\nUniform color spaces\nPerception of color\n– Human photoreceptors – Environmental effects, adaptation\nUsing color in machine vision systems",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color\nColor of light arriving at camera depends on\n– Spectral reflectance of the surface light is leaving – Spectral radiance of light falling on that patch\nColor perceived depends on\n– Physics of light – Visual system receptors – Brain processing, environment",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Human photoreceptors\nRods responsible for intensity -Cones responsible for color -Fovea: small region (1 or 2°) at the center of the visual field containing the highest density of cones (and no rods). – Less visual acuity in the periphery\nAdapted from Seitz, Duygulu",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Two types of light-sensitive receptors\nCones\ncone-shaped less sensitive operate in high light color vision\nRods\nrod-shaped highly sensitive operate at night gray-scale vision\n© Stephen E. Palmer, 2002\nSlide credit: Alyosha Efros",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Human photoreceptors\nReact only to some\nwavelengths, with different sensitivity (light fraction absorbed)\nThree kinds of cones\nBrain fuses responses from local neighborhood of several cones for perceived color\nSensitivities vary from person to person, and with age\ny t i v i t i s n e S\nColor blindness: deficiency in at least one type of cone\nWavelength (nm)",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Human photoreceptors\nPossible evolutionary pressure for developing receptors for different wavelengths in primates\nOsorio & Vorobyev, 1996",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Trichromacy\nExperimental facts:\n– Three primaries will work for most people if we\nallow subtractive matching; “trichromatic” nature of the human visual system\n– Most people make the same matches for a given set of primaries (i.e., select the same mixtures)",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Environmental effects & adaptation\nChromatic adaptation: we adapt to a particular illuminant\nAssimilation, contrast effects, chromatic induction: nearby colors affect what is perceived; receptor excitations interact across image and time\nAfterimages\nColor matching ~= color appearance\nPhysics of light ~= perception of light",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Chromatic adaptation\nIf the visual system is exposed to a certain illuminant for a while, color system starts to adapt / skew.",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Chromatic adaptation\nhttp://www.planetperplex.com/en/color_illusions.html",
        "offset": "page60",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Chromatic adaptation\nhttp://www.planetperplex.com/en/color_illusions.html",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Chromatic adaptation\nhttp://www.planetperplex.com/en/color_illusions.html",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Brightness perception\nEdward Adelson\nhttp://web.mit.edu/persci/people/adelson/illusions_demos.html",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Brightness perception\nEdward Adelson\nhttp://web.mit.edu/persci/people/adelson/illusions_demos.html",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Brightness perception\nEdward Adelson\nhttp://web.mit.edu/persci/people/adelson/illusions_demos.html",
        "offset": "page65",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Look at blue squares\nLook at yellow squares\nContent © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page66",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Look at blue squares\nLook at yellow squares\nContent © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page67",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Look at blue squares\nLook at yellow squares\nContent © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page68",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Content © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page69",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Content © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page70",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Content © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page71",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Content © 2008 R.Beau Lotto • http://www.lottolab.org/articles/illusionsoflight.asp",
        "offset": "page72",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "After images\nTired photoreceptors send out negative response after a strong stimulus\nhttp://www.sandlotscience.com/Aftereffects/Andrus_Spiral.htm\nSource: Steve Seitz",
        "offset": "page73",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "After images\nTired photoreceptors send out negative response after a strong stimulus\nhttp://www.sandlotscience.com/Aftereffects/Andrus_Spiral.htm\nSource: Steve Seitz",
        "offset": "page74",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Name that color\nHigh level interactions affect perception and processing.",
        "offset": "page75",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Color\nMeasuring color\n– Spectral power distributions – Color mixing – Color matching experiments – Color spaces\nUniform color spaces\nPerception of color\n– Human photoreceptors – Environmental effects, adaptation\nUsing color in machine vision systems",
        "offset": "page76",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color as a low-level cue for CBIR\nBlobworld system, Carson et al, 1999",
        "offset": "page77",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color as a low-level cue for CBIR\ns t n u o c\nl\ne x i P\nR\nG\nColor intensity\nB\nColor histograms: Use distribution of colors to describe image\nNo spatial info – invariant to translation, rotation, scale",
        "offset": "page78",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color as a low-level cue for CBIR\nCompute distance between histograms:\nIntersection\nSimilar\nDifferent",
        "offset": "page79",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based image retrieval\nGiven a collection (database) of images:\n– Extract and store one color histogram per image\nGiven new query image:\n– Extract its color histogram – For each database image:\nCompute intersection between query histogram and database histogram\n– Sort intersection values (highest score = most similar) – Rank database items relative to query based on this sorted\norder",
        "offset": "page80",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based image retrieval\nExample database",
        "offset": "page81",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based image retrieval\nExample retrievals",
        "offset": "page82",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based image retrieval\nExample retrievals",
        "offset": "page83",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "http://images.google.com/\nSearch for similar images. Try:\nBuildings • Dogs • Concert",
        "offset": "page84",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Shazam for Fashion\nThere are several products doing that. • They have to deal with color similarity: – E.g.: http://www.spylight.com/",
        "offset": "page85",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Shazam for Fashion\nThere are several products doing that. • They have to deal with color similarity: – E.g.: http://www.asap54.com/",
        "offset": "page86",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based skin detection\nM. Jones and J. Rehg, Statistical Color Models with Application to Skin Detection, IJCV 2002.",
        "offset": "page87",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based skin detection",
        "offset": "page88",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based skin detection\nhttp://www.ghvandoorn.nl/skindetection.html",
        "offset": "page89",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based skin detection\nOpenCV\nhttps://www.youtube.com/watch?v=vZk9k9azonw",
        "offset": "page90",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based tracking\nD. Ramanan, D. Forsyth, and A. Zisserman. Tracking People by Learning their Appearance. PAMI 2007.\nSlide credit: L. Lazebnik",
        "offset": "page91",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Color-based tracking\nhttp://www.roborealm.com/tutorial/color_obj ect_tracking_2/slide010.php\nhttps://www.youtube.com/watch?v=WPnWD Gl3XZc",
        "offset": "page92",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Viewing Colored Objects",
        "offset": "page93",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide Credits\nTrevor Darrell • Kristen Grauman: 3-48, 50-75, 79-86 • Bob Woodham: 49, 87-90 • and others, indirectly (Steve Palmer, Brian Wandell, etc!)",
        "offset": "page94",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Color\nMeasuring color\n– Spectral power distributions – Color mixing – Color matching experiments – Color spaces\nUniform color spaces\nPerception of color\n– Human photoreceptors – Environmental effects, adaptation\nUsing color in machine vision systems",
        "offset": "page95",
        "ref": "/home/ameer/Kaleidoo/server/uploads/04-Color.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "0e7663adbbd34990b855862218604902.json": [
      {
        "type": "application/pdf",
        "text": "CS4670: Computer Vision\nNoah Snavely\nImage Resampling",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image Scaling\nThis image is too big to fit on the screen. How can we generate a half-sized version?\nSource: S. Seitz",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image sub-sampling\n1/4\nThrow away every other row and column to create a 1/2 size image - called image sub-sampling\n1/8\nSource: S. Seitz",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image sub-sampling\n1/2\n1/4 (2x zoom)\nWhy does this look so crufty?\n1/8 (4x zoom)\nSource: S. Seitz",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image sub-sampling\nSource: F. Durand",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Even worse for synthetic images\nSource: L. Zhang",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Aliasing\nOccurs when your sampling rate is not high enough to capture the amount of detail in your image • Can give you the wrong signal/image—an alias\nTo do sampling right, need to understand the structure of your signal/image\nEnter Monsieur Fourier…\nTo avoid aliasing:\n– sampling rate ≥ 2 * max frequency in the image • said another way: ≥ two samples per cycle\n– This minimum sampling rate is called the Nyquist rate\nSource: L. Zhang",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Wagon-wheel effect\n(See http://www.michaelbach.de/ot/mot_wagonWheel/index.html)\nSource: L. Zhang",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Nyquist limit – 2D example\nGood sampling\nBad sampling",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Aliasing\nWhen downsampling by a factor of two\n– Original image has frequencies that are too high\nHow can we fix this?",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian pre-filtering\nG 1/4\nGaussian 1/2\nSolution: filter the image, then subsample\nG 1/8\nSource: S. Seitz",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Subsampling with Gaussian pre-filtering\nGaussian 1/2\nG 1/4\nSolution: filter the image, then subsample\nG 1/8\nSource: S. Seitz",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1/2\nCompare with...\n1/4 (2x zoom)\n1/8 (4x zoom)\nSource: S. Seitz",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian pre-filtering • Solution: filter the image, then subsample\nblur\nF0\nsubsample\nF0 H*\nblur\nF1 subsample …\nF2\nF1 H*",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian pyramid\nblur\nF0\nsubsample\nF0 H*\nblur\nF1 subsample …\nF2\nF1 H*",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian pyramids [Burt and Adelson, 1983]\nIn computer graphics, a mip map [Williams, 1983] • A precursor to wavelet transform\nGaussian Pyramids have all sorts of applications in computer vision\nSource: S. Seitz",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian pyramids [Burt and Adelson, 1983]\nHow much space does a Gaussian pyramid take compared to the original image?\nSource: S. Seitz",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian Pyramid",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "lt",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "0G\nThe Laplacian Pyramid\n)expand(1iiiGGL\nGaussian Pyramid\n)expand(1iiiGLG\nnG\n2G\n=\n1G\n=\n=\nLaplacian Pyramid\nnnGL\n2L\n1L\n0L",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Questions?",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CS4670: Computer Vision\nNoah Snavely\nImage Interpolation",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image Scaling\nLast time:\nThis image is too big to fit on the screen. How can we generate a half-sized version?\nSource: S. Seitz",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Upsampling\nThis image is too small for this screen: • How can we make it 10 times as big? • Simplest approach: repeat each row and column 10 times\n(“Nearest neighbor interpolation”)",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image interpolation\nd = 1 in this example\n1\n2\n3\n4\n5\nRecall how a digital image is formed\nIt is a discrete point-sampling of a continuous function • If we could somehow reconstruct the original function, any new image could be generated, at any resolution and scale\nAdapted from: S. Seitz",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image interpolation\nd = 1 in this example\n1\n2\n3\n4\n5\nRecall how a digital image is formed\nIt is a discrete point-sampling of a continuous function • If we could somehow reconstruct the original function, any new image could be generated, at any resolution and scale\nAdapted from: S. Seitz",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image interpolation\n1\n1\n2\n2.5\n3\n4\n5\nWhat if we don’t know ?\nGuess an approximation: • Can be done in a principled way: filtering • Convert to a continuous function:\nReconstruct by convolution with a reconstruction filter, h\nd = 1 in this example\nAdapted from: S. Seitz",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image interpolation\n“Ideal” reconstruction\nNearest-neighbor interpolation\nLinear interpolation\nGaussian reconstruction\nSource: B. Curless",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Reconstruction filters • What does the 2D version of this hat function look like?\nperforms linear interpolation\n(tent function) performs bilinear interpolation\nOften implemented without cross-correlation\nE.g., http://en.wikipedia.org/wiki/Bilinear_interpolation\nBetter filters give better resampled images\nBicubic is common choice\nCubic reconstruction filter",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image interpolation\nOriginal image: x 10\nNearest-neighbor interpolation\nBilinear interpolation\nBicubic interpolation",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image interpolation\nAlso used for resampling",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Raster to Vector Graphics",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Depixelating Pixel Art",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Questions?",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/03-Sampling.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      }
    ],
    "14b72151129a453ebbfafa19f2be749c.json": [
      {
        "type": "application/pdf",
        "text": "1\nOptical flow\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\nFrom images to video  Feature tracking  Optical flow  Motion segmentation  Applications\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From images to video\n A video is a sequence of frames captured over time  Now our image data is a function of space (x,y) and time (t)\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of Motion fields\nForward motion\nRotation\nHorizontal translation\n4\nLihi Zelnik-Manor, Computer Vision\nCloser objects appear to move faster!!",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Motion Field & Optical Flow Field\n Underlying assumption:\nThe apparent motion field is a projection of the real 3D motion onto the 2d image\nCCD\n3D motion vector\n2D optical flow vector\nvu,u\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When does it break?\nThe screen is stationary yet displays motion\nHomogeneous objects generate zero optical flow.\nFixed sphere. Changing light source.\n6\nLihi Zelnik-Manor, Computer Vision\nNon-rigid texture motion",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature tracking vs. optical flow\n Feature tracking\n Extract visual features and “track” them over multiple frames\n Optical flow\n Compute image motion at each and every pixel\n7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Motion and perceptual organization\nEven “impoverished” motion data can evoke a strong percept\nG. Johansson, “Visual Perception of Biological Motion and a Model For Its Analysis\", Perception and Psychophysics 14, 201-211, 1973.",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking example\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Optical flow example\n Compute motion for all pixels\n10\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\nFrom images to video  Feature tracking  Optical flow  Motion segmentation  Applications\n11\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking challenges  Find good features to track\n Harris, SIFT, etc\n Large motions\n Discrete search instead of Lucas-Kanade\n Changes in shape, orientation, color\n Allow some matching flexibility\n Occlusions, dis-occlusions  Need to add/delete features\n Drift (errors accumulate over time)\n Need to know when to terminate a track\n12\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking by template matching\n The simplest way to track is by template matching  Define a small area around a pixel as the template  Match the template against each pixel within a search area in\nnext image.\n Use a match measure such as correlation, normalized\ncorrelation, or sum-of-squares difference\n Choose the maximum (or minimum) as the match\n13\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Limitations of template matching\n Slow (need to check more locations)\n Does not give subpixel alignment (or becomes much\nslower)  Even pixel alignment may not be good enough to prevent drift\n May be useful as a step in tracking if there are large\nmovements",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Lucas-Kanade Tracker\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature tracking\nI(x,y,t)\nI(x,y,t+1)\n Given two subsequent frames, estimate the point\ntranslation\nKey assumptions of Lucas-Kanade Tracker\nBrightness constancy: projection of the same point looks the same in every frame\nSmall motion: points do not move very far • Spatial coherence: points move like their neighbors",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The brightness constancy constraint\n Assumption 1:\nThe image intensity is constant\nI\nTime = t\nTime = t+dt\nyx,\ndyydxx,\ntyxI,,\n\ndttdyydxxI,,\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Small motion assumption\n The brightness constancy equation\ndttdyydxxItyxI,,,,\n18\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Small motion assumption\n The brightness constancy equation\ndttdyydxxItyxI,,,,\n Assumption 2\nMotion is small First order Taylor expansion\n,,,,IIIIxytIxytdxdydtxyt\n\n0IIIdxdydtxyt\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The motion equation\n Simplify notations:\n0dtIdyIdxItyx\n Divide by dt and denote\ndtdxu\ndtdyv\n Final equation is:\ntyxIvIuI\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The motion equation\n Can we use this equation to recover image motion at a\nsingle pixel (x,y)?\nxytuIuIvIIv\n Problem\n 1 equation per pixel, 2 unknowns  This means we cannot recover the motion component\nperpendicular to the gradient\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The aperture problem\nTime t\nTime t+1\n22\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The aperture problem\nTime t\nTime t+1\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The barber pole illusion\nhttp://en.wikipedia.org/wiki/Barberpole_illusion",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The barber pole illusion\nhttp://en.wikipedia.org/wiki/Barberpole_illusion",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The aperture problem  For points on a line of fixed intensity we can only\nrecover the normal flow\nTime t\nTime t+dt Time t+dt\n?\nWhere did the blue point move to? We need additional constraints\n26\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Solving the ambiguity\nSometimes enlarging the aperture can help\n27\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Spatial coherence assumption\n Assumption 3 [Lucas & Kanade 1981]\nAssume constant (u,v) in small neighborhood\ntyxIvIuI\ntyxIvuII\n212211ttyxyxIIvuIIII\nbAu\n28\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Lucas Kanade (1984)\nGoal: Minimize\n2ubA\nMethod: Least-Squares\nbAu\nbAAATTu\n2x2 2x1\n2x1\nbAAATT1u\n29\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When is this solvable?\nbAAATT1u\n22yyxyxxTIIIIIIAA\nWe want this matrix to be invertible \nno zero eigenvalues\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When is this solvable?\n Edge \nAAT\nbecomes singular\nxyII,\nyxII,\n0022xyyyxyxxIIIIIIII\n0 eigenvaluer with eigenvecto is xyII\n31\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When is this solvable?\n Homogeneous \n0AAT\n 0 eigenvalues\n0,yxII\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When is this solvable?\n Textured regions  two high eigenvalues\n0,yxII\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Which features can we track?\n Edge \nAAT\nbecomes singular\n Homogeneous regions  low gradients\n0AAT\n High texture \n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When assumptions break  Brightness constancy is not satisfied\nCorrelation based methods\n A point does not move like its neighbors\n what is the ideal window size?\nRegularization based methods\n The motion is not small (Taylor expansion doesn’t hold)  Aliasing\nUse multi-scale estimation\n35\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Aliasing\nTemporal aliasing causes ambiguities in optical flow because images can have many pixels with the same intensity.\nI.e., how do we know which ‘correspondence’ is correct?\nactual shift\nestimated shift\nnearest match is correct (no aliasing)\nnearest match is incorrect (aliasing)\nTo overcome aliasing: coarse-to-fine estimation.\n36\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Coarse-to-fine motion estimation\nu=1.25 pixels\nu=2.5 pixels\nu=5 pixels\nimage It image It-1\nu=10 pixels\nimage It+1 image I\nGaussian pyramid of image It\nGaussian pyramid of image It+1\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Coarse-to-fine motion estimation\nrun Lucas-Kanade\nShift features & upsample\nrun Lucas-Kanade\n. . .\nimage It image It-1\nimage It+1 image I\nGaussian pyramid of image It\nGaussian pyramid of image It+1\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Shi-Tomasi feature tracker\nFind good features (min eigenvalue of 22 Hessian)\n1. 2. Use Lucas-Kanade to track with pure translation 3. Use affine registration with first feature patch 4. Terminate tracks whose dissimilarity gets too large Start new tracks when needed\n[Shi & Tomasi, Good features to track, CVPR’94] http://www.ces.clemson.edu/~stb/klt/\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking example\nJ. Shi and C. Tomasi. Good Features to Track. CVPR 1994.",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking example\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Implementation issues\n Window size\n Small window more sensitive to noise and may miss larger\nmotions (without pyramid)\n Large window more likely to cross an occlusion boundary\n(and it’s slower)\n 15x15 to 31x31 seems typical\n Weighting the window\n Common to apply weights so that center matters more (e.g.,\nwith Gaussian)",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\nFrom images to video  Feature tracking  Optical flow  Motion segmentation  Applications\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Optical Flow Field\nWhat can be done when we need to find the motion of\neach and every pixel?\n44\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Lucas-Kanade Optical Flow\n Same as Lucas-Kanade feature tracking, but for each pixel\n As we saw, works better for textured pixels\n Operations can be done one frame at a time, rather than\npixel by pixel  Efficient",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Iterative Refinement\nIterative Lukas-Kanade Algorithm 1. Estimate displacement at each pixel by solving Lucas-\nKanade equations\n2. Warp I(t) towards I(t+1) using the estimated flow field\nBasically, just interpolation 3. Repeat until convergence\n\n46\n* From Khurram Hassan-Shafique CAP5415 Computer Vision 2003",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Coarse-to-fine motion estimation\nrun Lucas-Kanade\nwarp & upsample\nrun Lucas-Kanade\n. . .\nimage It image It-1\nimage It+1 image I\nGaussian pyramid of image It\nGaussian pyramid of image It+1\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example\n48\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Multi-resolution registration\n49\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Optical flow results\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Optical Flow Results\n51\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When assumptions break\n Brightness constancy is not satisfied\nCorrelation based methods\n A point does not move like its neighbors\n what is the ideal window size?\nRegularization based methods\n The motion is not small (Taylor expansion doesn’t hold)  Aliasing\nUse multi-scale estimation\n52\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Spatial coherence\n Neighboring points in the scene typically belong to the same\nsurface and hence typically have similar motions.\n Since they also project to nearby points in the image, we\nexpect spatial coherence in image flow.\n53\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Formalize this Idea\nNoisy 1D signal:\nu\nNoisy measurements g(x)\n54\nLihi Zelnik-Manor, Computer Vision\nx",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Regularization\nFind the “best fitting” smoothed function f(x)\nu\nNoisy measurements g(x)\n55\nLihi Zelnik-Manor, Computer Vision\nf(x)\nx",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Membrane model\nFind the “best fitting” smoothed function f(x)\nu\n56\nLihi Zelnik-Manor, Computer Vision\nf(x)\nx",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Membrane model\nFind the “best fitting” smoothed function f(x)\nu\n57\nLihi Zelnik-Manor, Computer Vision\nf(x)\nx",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Membrane model\nFind the “best fitting” smoothed function f(x)\nu\n58\nLihi Zelnik-Manor, Computer Vision\nf(x)",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Regularization\nu\nf(x)\nx\nMinimize:\nFaithful to the data\nSpatial smoothness assumption\n12211()(()())((1)())NNxxEffxgxfxfx\n59\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Discontinuities\nu\nWhat about this discontinuity? What is happening here? What can we do?\n60\nLihi Zelnik-Manor, Computer Vision\nf(x)\nx",
        "offset": "page60",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Robust estimation\nNoise distributions are often non-Gaussian, having much heavier tails. Noise\nsamples from the tails are called outliers.\n Sources of outliers (multiple motions):\n specularities / highlights  jpeg artifacts / interlacing / motion blur  multiple motions (occlusion boundaries, transparency)\nvelocity space\nu2\n+\n+\n61\nLihi Zelnik-Manor, Computer Vision\nu1",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Occlusion\nocclusion\ndisocclusion\nMultiple motions within a finite region.\n62\nLihi Zelnik-Manor, Computer Vision\nshear",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Coherent motion\nPossibly Gaussian.\n63\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Multiple motions\nDefinitely not Gaussian.\n64\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Regularization u\nf(x)\nx\nSpatial smoothness assumption\nFaithful to the data\n12211(,)(()())()((1)())(1())NNxxEflfxgxlxfxfxlx\n65\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page65",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Weak membrane model u\nf(x)\nx\nSpatial smoothness assumption\nFaithful to the data\n12211(,)(()())()((1)())(1())NNxxEflfxgxlxfxfxlx\nRobustness\n}1,0{)(xl\n66\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page66",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Analog line process Penalty function\nFamily of quadratics\nSpatial smoothness assumption\nFaithful to the data\n12211(,)(()())()((1)())(1())NNxxEflfxgxlxfxfxlx\nΨ(𝑙 𝑥 )\nRobustness\n1)(0xl\n67\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page67",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Analog line process\nInfimum defines a robust error function.\nMinima are the same:\n12211(,)(()())()((1)())(())NNxxEflfxgxlxfxfxlx\n12211()(()())((1)(),)NNxxEffxgxfxfx\n68\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page68",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Robust regularization u\nf(x)\nTreat large spatial derivatives as outliers.\nx\nFaithful to the data\nSpatial smoothness assumption\n11211()(()(),)((1)(),)NNxxEffxgxfxfx\nRobustness\n70\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page69",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Robust estimation\nProblem: Least-squares estimators penalize deviations between data & model with quadratic error fn (extremely sensitive to outliers)\nerror penalty function\ninfluence function\nRedescending error functions (e.g., Geman-McClure) help to reduce the influence of outlying measurements.\nerror penalty function\ninfluence function\n71\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page70",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Robust regularization\nFaithful to the data\nSpatial smoothness assumption\n11211()(()(),)((1)(),)NNxxEffxgxfxfx\nMinimize:\nscEEE\nWhat are 𝐸𝑐 and 𝐸𝑠 for optical flow estimation?\n72\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page71",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Regularization for optical flow\nAdd global smoothness term\n[Horn and Schunk 1981]\nError in brightness constancy equation\ndydxIvIuIEDtyxc2\nSmoothness error:\ndydxvvuuEDyxyxs2222\nMinimize:\nscEE\nSolve by calculus of variations\n73\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page72",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Robust regularization for optical flow\n[Black & Anandan 1993]\nRegularization can over-smooth across edges\nUse “smarter” regularization\n74\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page73",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Robust regularization for optical flow\n[Black & Anandan 1993]\nRegularization can over-smooth across edges\nUse “smarter” regularization\nMinimize:\n122,,xytxyxyDIuIvIuuvvdxdy\nBrightness constancy\nSmoothness\n75\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page74",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Optimization\n Gradient descent  Coarse-to-fine (pyramid)  Deterministic annealing\n76\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page75",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Recent GPU Implementation\n http://gpu4vision.icg.tugraz.at/  Real time flow exploiting robust norm + regularized\nmapping\nhttps://www.youtube.com/watch?v=ssINeWRb58M\n77\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page76",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Using optical flow\n78\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page77",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "State-of-the-art optical flow\nStart with something similar to Lucas-Kanade + gradient constancy + energy minimization with smoothing term + region matching + keypoint matching (long-range)\nRegion-based +Pixel-based +Keypoint-based\nLarge displacement optical flow, Brox et al., CVPR 2009",
        "offset": "page78",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\nFrom images to video  Optical flow  Feature tracking  Motion segmentation  Layered representation\n Applications\n80\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page79",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Motion representations\n How can we describe the motion in the scene?\n81\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page80",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Block-based motion prediction\n Break image up into square blocks  Estimate translation for each block  Use this to predict next frame, code difference (MPEG-2)\n82\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page81",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Layered motion representation\n Break image sequence up into “layers” of coherent\nmotion\n Each layer’s motion is represented by a parametric model\n83\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page82",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine motion (dense)  Recall the brightness constancy equation\n0xytIuIvI\n Assume affine motion\n123456uaaxayvaaxay\n Combine the equations\n1234560xytIaaxayIaaxayI\n Each pixel provides one equation  Solve with Least-squares\n84\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page83",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Layered motion\n Advantages\n can represent occlusions / disocclusions  each layer’s motion can be smooth  video segmentation for semantic processing\n Difficulties:\n how do we determine the correct number?  how do we assign pixels?  how do we model the motion?\n85\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page84",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How do we estimate the layers?\n1.\n2.\n3.\n4.\n5.\ncompute coarse-to-fine flow estimate affine motion for each block cluster with k-means assign pixels to best fitting affine region re-estimate affine motions in each region…\n86\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page85",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Layered motion result\n[Wang & Adelson, CVPR’93]",
        "offset": "page86",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Layered motion representation (option2)\nFor scenes with multiple parametric motions\nEstimate dominant motion parameters\nReject pixels which do not fit\nConvergence\nRestart on remaining pixels\n88\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page87",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation of Affine Motion\n=\n+\nInput\nSegmentation result\n[Zelnik-Manor & Irani, PAMI 2000 ]\n89\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page88",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\nFrom images to video  Optical flow  Feature tracking  Motion segmentation  Layered representation\n Applications\n90\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page89",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Panoramas\nInput\n91\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page90",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "id",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Camera ego-motion\nResult by MobilEye (www.mobileye.com)\n92\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page91",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Structure from Motion\nInput\nReconstructed shape\n[Zhang, et al. ICCV’03]\n93\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page92",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Stabilization\n94\nLihi Zelnik-Manor, Computer Vision\n[Zelnik-Manor & Irani, PAMI 2000 ]",
        "offset": "page93",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "sl",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "SIFT Flow\n95\nhttp://people.csail.mit.edu/celiu/ECCV2008/\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page94",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\nFrom images to video  Optical flow  Feature tracking  Motion segmentation  Layered representation\n Applications\n How do we evaluate success?\n[Baker et al. ICCV’07]\n96\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page95",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "e t i\nm e s o Y\nSynthetic video sequence\n Synthetic sequences can be used for quantitative evaluation\nImage 7\nImage 8\nGround-Truth Flow\nFlow Color Coding\n Limitation\n Hard to make these a true representative of real video and\nits noise and blur\n97\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page96",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Real video with ground-truth\n Paint scene with textured fluorescent paint  Take 2 images: One in visible light, one in UV light  Move scene in very small steps using robot  Generate ground-truth by tracking the UV images\nSetup\nLights\nImage\n98\nLihi Zelnik-Manor, Computer Vision\nCropped\nVisible\nUV",
        "offset": "page97",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Middlebury dataset\nhttp://vision.middlebury.edu/flow/\n99\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page98",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "id",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Optical flow without motion\n100\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page99",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "101\nEnd – Optical flow\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page100",
        "ref": "/home/ameer/Kaleidoo/server/uploads/21-optical-flow.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "380056cf2b1942b9908f1da056b7b47c.json": [
      {
        "type": "application/pdf",
        "text": "1\nSingle-view metrology\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective geometry\n Readings\nAmes Room\n Mundy, J.L. and Zisserman, A., Geometric Invariance in Computer Vision, Appendix:\nProjective Geometry for Machine Vision, MIT Press, Cambridge, MA, 1992, (read 23.1 - 23.5, 23.10) \navailable online: http://www.cs.cmu.edu/~ph/869/papers/zisser-mundy.pdf",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Three point perspective",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing lines\nv1\nv2\n Multiple Vanishing Points\n Any set of parallel lines on the plane define a vanishing point  The union of all of these vanishing points is the horizon line\n also called vanishing line\n Note that different planes (can) define different vanishing lines",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing lines\n Multiple Vanishing Points\n Any set of parallel lines on the plane define a vanishing point  The union of all of these vanishing points is the horizon line\n also called vanishing line\n Note that different planes (can) define different vanishing lines",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing vanishing points\nV\nC\nP0\nLine direction D\nDPPtt0",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing vanishing points\nV\nC\nP0\nLine direction D\nDPPtt0\n0/1///1ZYXZZYYXXZZYYXXtDDDttDtPDtPDtPtDPtDPtDPPP",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing vanishing points\nV\nC\nP0\nLine direction D\nDPPtt0\n0/1///1ZYXZZYYXXZZYYXXtDDDttDtPDtPDtPtDPtDPtDPPP\n Properties of\nPvM\n P is a point at infinity where the parallel lines meet, v is its projection  Depends only on line direction D  Parallel lines P0 + tD, P1 + tD intersect at P",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing vanishing lines\nv1\nl\nC\nl\nground plane\n Properties\n l is intersection of horizontal plane through C with image plane  Compute l from two sets of parallel lines on ground plane (more on that later)  All points at same height as C project to l\n points higher than C project above l\n Provides way of comparing height of objects in the scene\nv2",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Which is higher – the camera or the man in the parachute?",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fun with vanishing points",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Perspective cues",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Perspective cues",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Perspective cues",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Comparing heights\nVanishing Point",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring height\nHow high is the camera?\n5\n4\n3\n2\n1\n5.4\nCamera height\n3.3\n2.8",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing vanishing points (from lines)\nv\nq2\nq1\np2\np1\n Intersect p1q1 with p2q2\nLeast squares version\nBetter to use more than two lines and compute the “closest” point of intersection\nSee notes by Bob Collins for one good way of doing this:\n– http://www-2.cs.cmu.edu/~ph/869/www/notes/vanishing.txt",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring height without a ruler\nC\nZ\nground plane\nCompute Z from image measurements\nNeed more than vanishing points to do this",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The cross ratio  A Projective Invariant\n Something that does not change under projective transformations (including perspective projection)\nThe cross-ratio of 4 collinear points\nP1\nP2\nP3\nP4\n14232413PPPPPPPP\n1iiiiZYXP\nCan permute the point ordering\n34212431PPPPPPPP\n4! = 24 different orders (but only 6 distinct values) This is the fundamental invariant of projective geometry",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring height\n\nT (top of object)\nt r\nR (reference point)\nC\nb\nH\nR\nvZ\nB (bottom of object)\nground plane\nscene points represented as\n1ZYXP\n34122413PPPPPPPP\nTBRRBT\nRH\nscene cross ratio\ntvbrrvbtZZ\nRH\nimage cross ratio\nimage points as\n1yxp",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring height\nvanishing line (horizon)\nvx\nv\nRHZZtvbrrvbt\nimage cross ratio\nt0\nb0\nH\nvz r\nt\nb\nR\nH\nvy",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring height\nvanishing line (horizon)\nt0 t0\nvx\nv\nm0 t1\nb0\nb1\nWhat if the point on the ground plane b0 is not known?\nHere the guy is standing on the box, height of box is known • Use one side of the box to help find b0 as shown above\nvz r\nb\nvy",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Modeling from a photograph\nSt. Jerome in his Study, H. Steenwick",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Modeling from a photograph",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Modeling from a photograph\nFlagellation, Piero della Francesca",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Modeling from a photograph\nvideo by Antonio Criminisi",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Modeling from a photograph",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Modeling from a photograph",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What can we do with a single image?\n Measure height  Camera calibration  3D reconstruction ?\n29\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Lines in a 2D plane\n The line equation\n𝑎𝑥1 + 𝑏𝑥2 + 𝑐 = 0\n Vector notation\n Line 𝑙 =\n𝑎 𝑏 𝑐\n A point in homogeneous coordinates 𝑥 =\n If the point lies on the line then\n𝑥1 𝑥2 1\n𝑎 𝑏 𝑐\n30\nLihi Zelnik-Manor, Computer Vision\n𝑥\n𝑥1 𝑥2 1\n= 0\n𝑙",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Lines in a 2D plane\n Intersecting lines\n𝑥 = 𝑙 × 𝑙′\n Proof\n 𝑙 × 𝑙′𝑙 → 𝑙 × 𝑙′  𝑙 × 𝑙′ 𝑙′ → 𝑙 × 𝑙′\n𝑙 = 0 → 𝑥 ∈ 𝑙 ∙ 𝑙′ = 0 → 𝑥 ∈ 𝑙′\n 𝑥 is the intersection point\n31\nLihi Zelnik-Manor, Computer Vision\n𝑥\n𝑙′\n𝑙",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Ideal points = points at infinity\n A point 𝑥 =\n At infinity 𝑥∞ =\n𝑥1 𝑥2 1 𝑥1 𝑥2 0\n𝑙 =\n𝑎 𝑏 𝑐\n𝑙′ =\n𝑎 𝑏 𝑐′\n𝑙\n𝑙′\n The intersection between two parallel lines in 2D is a point at\ninfinity\n 𝑣 = 𝑙 × 𝑙′ = (𝑐 − 𝑐′)\n𝑏 −𝑎 0\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The line at infinity\n A set of ideal points (at infinity) lies on a line called\n\n“the line at infinity” 0 0 1\n𝑙∞ =\n Let’s verify this via the line equation\n\n𝑥1 𝑥2 0\n0 0 1\n= 0 (dot product is 0  the point is on the line)\n33\nLihi Zelnik-Manor, Computer Vision\n𝑙∞",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Parallel lines on a plane in 3D\n Vanishing point: The projection onto the image of the\nintersection of two parallel lines in 3D\n Vanishing line: Vanishing points of parallel lines that lie on the\nsame plane, lie on the vanishing line of that plane.\n𝑙∞ = Horizon = Vanishing line\nVanishing point\n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The horizon\n How can we tell if two lines in the image are parallel in the\nworld?  Find the horizon line of the corresponding plane  Check if the two lines intersect at the horizon  If yes, then they are parallel\nhorizon\n35\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing vanishing points\nC\nV\nRecall v is the projection of the direction D\nP0\nLine direction D\nDPPtt0\n0/1///1ZYXZZYYXXZZYYXXtDDDttDtPDtPDtPtDPtDPtDPPP\n Properties of\nPvM\n P is a point at infinity, v is its projection  Depends only on line direction D  Parallel lines P0 + tD, P1 + tD intersect at P",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing points\nd = line direction\n Assume camera projection matrix is 𝑀 = 𝐾 𝐼 0\n Then the projection of the vanishing point is 𝑣 = 𝐾𝑑\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Angle between 2 scene lines\n𝑣 = 𝐾𝑑\ncos 𝜃 =\n𝑣1\n𝑇𝐾𝑇𝐾𝑣2 𝑇𝐾𝑇𝐾𝑣1 𝑣2\n𝑣1\n𝑇𝐾𝑇𝐾𝑣2\nIf the lines are orthogonal then 𝜃 = 90 and 𝑣1\n𝑇𝐾𝑇𝐾𝑣2 = 0 Let’s use this!\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Angle between 2 scene lines\n𝑣1\n𝑣2\n𝑣1\n𝑇𝐾𝑇𝐾𝑣2 = 0\nconstraint on K\nFrom two vanishing points!\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Single view calibration\nMark 3 orthogonal lines, find 3 vanishing points, and solve for K using three constraints\n𝑣1 𝑣1 𝑣2\n𝑇𝐾𝑇𝐾𝑣2 = 0 𝑇𝐾𝑇𝐾𝑣3 = 0 𝑇𝐾𝑇𝐾𝑣3 = 0\n40\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What can we do with a single image?\n Measure height  Camera calibration  3D reconstruction  Manhattan world\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Points and planes in 3D\n A point 𝑥 =\n A plane 𝜋 =\n𝑥1 𝑥2 𝑥3 1 𝑎 𝑏 𝑐 𝑑\n A point that lies on a plane x ∙ 𝜋 = 0\n42\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The vanishing line\n Parallel planes intersect the plane at infinity in a common line\n– the vanishing line = horizon\n The normal to these planes can be computed from the\nhorizon\n𝑛 = 𝐾𝑇𝑙ℎ𝑜𝑟𝑖𝑧𝑜𝑛\n(K is the camera calibration matrix)\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Reconstruct surface normals\n𝑙ℎ𝑜𝑟𝑖𝑧𝑜𝑛\n If K is known we can compute plane normals\n𝑛 = 𝐾𝑇𝑙ℎ𝑜𝑟𝑖𝑧𝑜𝑛\n44\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application\n These transformations are used in single-view metrology\nCriminisi & Zisserman, 99\n45\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application\n These transformations are used in single-view metrology\nCriminisi & Zisserman, 99\n46\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application\n These transformations are used in single-view metrology\nCriminisi & Zisserman, 99\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application\n These transformations are used in single-view metrology\nCriminisi & Zisserman, 99\n48\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Single view metrology\n Pros\n Cool  Only a single image required\n Cons\n Manually select vanishing points and lines  Planar surfaces  Occlusion boundaries  …\n49\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Other approaches\n Learn appearance-based models of surfaces at various\norientations\nHoiem et al, 2005\nhttp://www.cs.uiuc.edu/homes/dhoiem/projects/software.html\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Other approaches\n A learning-based approach to single-view metrology\nSaxena, Sun, Ng, 2005\n Input = image +\ncorresponding depth map\n Learn how to match image patches to corresponding depth patches\nhttp://make3d.cs.cornell.edu/\n51\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Up-to-date applications\n52\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Inserting synthetic objects into images\nhttp://vimeo.com/28962540 Rendering synthetic objects into legacy photographs Karsch et al SIGGRAPH 2011",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image manipulation\n http://www.cse.iitb.ac.in/~jaseem/graphicsa23.pdf  Interactive Images: Cuboid proxies for Smart Image\nManipulation, Youyi Zheng, Xiang Chen, Ming-Ming Cheng, Kun Zhou, Shi-Min Hu and Niloy J. Mitra, SIGGRAPH 2012\n54\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3-Sweep: Extracting Editable Objects from a Single Photo\n Chen et al. SIGGRAPH 2013\nhttp://www.faculty.idc.ac.il/arik/site/3Sweep.asp https://vimeo.com/148236679\n55\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Augmented reality glasses (not exactly single view)\n https://www.getameta.com/\n https://www.rideonvision.com/\n https://www.microsoft.com/microsoft-hololens/en-us\n56\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "57\nEnd – Single-view metrology\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection of the line at infinity\n Perspective projection 3D2D can be written as\n 𝑀 =\n𝐴 𝑡 𝑣 𝑏 3×4\n Perspective projection of a line gives a line\n 𝑙′ = 𝑀𝑙\n Perspective projection of the line at infinity\n 𝑀𝑙∞ =\n𝐴 𝑡 𝑣 𝑏\n0 0 0 1\n=\n𝑡𝑥 𝑡𝑦 𝑡𝑧 𝑏\nnot a line at infinity!!\n This is the horizon line\n58\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/15-SingleViewMetrology.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "3bb3de129a6448629811660e4ed4c3a2.json": [
      {
        "type": "application/pdf",
        "text": "1\nImage segmentation\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today’s class\n Segmentation and grouping\n Gestalt cues  By clustering (mean-shift)  By boundaries (watershed)\n Superpixels and multiple segmentations",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gestalt psychology or gestaltism  German: Gestalt - \"form\" or \"whole”\n Berlin School, early 20th century  Kurt Koffka, Max Wertheimer, and Wolfgang Köhler\n View of brain: • whole is more than the sum of its parts • holistic • parallel • analog •\nself-organizing tendencies\nSlide from S. Saverese",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gestaltism\nThe Muller-Lyer illusion",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "We perceive the interpretation, not the senses",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Principles of perceptual organization\nFrom Steve Lehar: The Constructive Aspect of Visual Perception",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Principles of perceptual organization",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gestaltists do not believe in coincidence",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Emergence",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Grouping by invisible completion\nFrom Steve Lehar: The Constructive Aspect of Visual Perception",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Grouping involves global interpretation\nFrom Steve Lehar: The Constructive Aspect of Visual Perception",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Grouping involves global interpretation\nFrom Steve Lehar: The Constructive Aspect of Visual Perception",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gestalt cues\n Good intuition and basic principles for grouping\n Basis for many ideas in segmentation and occlusion\nreasoning\n Some (e.g., symmetry) are difficult to implement in\npractice",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image segmentation\nGoal: Group pixels into meaningful or perceptually similar regions",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation for efficiency: “superpixels”\n[Felzenszwalb and Huttenlocher 2004]\n[Hoiem et al. 2005, Mori 2005]",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation for feature support\n50x50 Patch\n50x50 Patch",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation for object proposals\n“Selective Search” [Sande, Uijlings et al. ICCV 2011, IJCV 2013]\n[Endres & Hoiem ECCV 2010, IJCV 2014]",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation for image editing\nRother et al. 2004",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Major processes for segmentation\n Bottom-up: group tokens with similar features  Top-down: group tokens that likely belong to the same\nobject\n[Levin and Weiss 2006]",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation using clustering\n Kmeans\n Mean-shift",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature Space\nSource: K. Grauman",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "K-means clustering using intensity alone and color alone\nInput image\nClusters on intensity\nClusters on color",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "K-Means pros and cons\nPros\n– Simple and fast – Easy to implement\nCons\n– Need to choose K – Sensitive to outliers\nUsage\n– Rarely used for pixel segmentation",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift segmentation\n D. Comaniciu and P. Meer, Mean Shift: A Robust Approach toward Feature\nSpace Analysis, PAMI 2002.\n Versatile technique for clustering-based segmentation",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift algorithm\n\nTry to find modes of this non-parametric density",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Kernel density estimation\nKernel\nEstimated density\nData (1-D)",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Kernel density estimation\nKernel density estimation function\nGaussian kernel",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass\nMean Shift vector",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass\nMean Shift vector",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass\nMean Shift vector",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass\nMean Shift vector",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass\nMean Shift vector",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass\nMean Shift vector",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift\nSlide by Y. Ukrainitz & B. Sarel\nRegion of interest\nCenter of mass",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing the Mean Shift\nSimple Mean Shift procedure: • Compute mean shift vector\nTranslate the Kernel window by m(x)\n2121()niiiniighghx-xxmxxx-x\nSlide by Y. Ukrainitz & B. Sarel",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Real Modality Analysis",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Attraction basin\n Attraction basin: the region for which all trajectories\nlead to the same mode\n Cluster: all data points in the attraction basin of a\nmode\nSlide by Y. Ukrainitz & B. Sarel",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Attraction basin",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift clustering\n\nThe mean shift algorithm seeks modes of the given set of points 1. Choose kernel and bandwidth\n2.\nFor each point: a) Center a window on that point b) Compute the mean of the data in the search window c) Center the search window at the new mean location d) Repeat (b,c) until convergence\n3. Assign points that lead to nearby modes to the same cluster",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift segmentation results\nhttp://www.caip.rutgers.edu/~comanici/MSPAMI/msPamiResults.html",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "http://www.caip.rutgers.edu/~comanici/MSPAMI/msPamiResults.html",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "unknown",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean-shift: other issues • Speedups\n– Binned estimation – replace points within some “bin” by point\nat center with mass\n– Fast search of neighbors – e.g., k-d tree or approximate NN – Update all windows in each iteration (faster convergence)\nOther tricks\n– Use kNN to determine window sizes adaptively\nLots of theoretical support\nD. Comaniciu and P. Meer, Mean Shift: A Robust Approach toward Feature Space Analysis, PAMI 2002.",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean shift pros and cons  Pros\n Good general-purpose segmentation  Flexible in number and shape of regions  Robust to outliers  General mode-finding algorithm (useful for other problems such as\nfinding most common surface normals)\n Cons\n Have to choose kernel size in advance  Not suitable for high-dimensional features\n When to use it\n Oversegmentation  Multiple segmentations  Tracking, clustering, filtering applications\n D. Comaniciu, V. Ramesh, P. Meer: Real-Time Tracking of Non-Rigid Objects using Mean\nShift, Best Paper Award, IEEE Conf. Computer Vision and Pattern Recognition (CVPR'00), Hilton Head Island, South Carolina, Vol. 2, 142-149, 2000",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean-shift reading\nNicely written mean-shift explanation (with math) http://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift- algorithm/\nIncludes .m code for mean-shift clustering\nMean-shift paper by Comaniciu and Meer http://www.caip.rutgers.edu/~comanici/Papers/MsRobustApproach.pdf\nAdaptive mean shift in higher dimensions http://mis.hevra.haifa.ac.il/~ishimshoni/papers/chap9.pdf",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Superpixel algorithms\n Goal is to divide the image into a large number of regions,\nsuch that each regions lie within object boundaries\n Examples\n Watershed  Felzenszwalb and Huttenlocher graph-based  Turbopixels  SLIC",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Watershed algorithm",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Watershed segmentation\nImage\nGradient\nWatershed boundaries",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Meyer’s watershed segmentation\n1. Choose local minima as region seeds 2. Add neighbors to priority queue, sorted by value 3. Take top priority pixel from queue If all labeled neighbors have same label, assign that label to pixel\n2. Add all non-marked neighbors to queue\n4. Repeat step 3 until finished (all remaining pixels in queue are on the boundary)\nMatlab: seg = watershed(bnd_im)\nMeyer 1991",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Simple trick\n Use Gaussian or median filter to reduce number of\nregions",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Watershed usage\nUse as a starting point for hierarchical segmentation\n– Ultrametric contour map (Arbelaez 2006)\nWorks with any soft boundaries – Pb (w/o non-max suppression) – Canny (w/o non-max suppression) – Etc.",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Watershed pros and cons\nPros\n– Fast (< 1 sec for 512x512 image) – Preserves boundaries\nCons\n– Only as good as the soft boundaries (which may be slow to\ncompute)\n– Not easy to get variety of regions for multiple segmentations\nUsage\n– Good algorithm for superpixels, hierarchical segmentation",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Felzenszwalb and Huttenlocher: Graph- Based Segmentation\nhttp://www.cs.brown.edu/~pff/segment/",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Felzenszwalb and Huttenlocher: Graph- Based Segmentation\nhttp://www.cs.brown.edu/~pff/segment/\n+ Good for thin regions + Fast + Easy to control coarseness of segmentations + Can include both large and small regions - Often creates regions with strange shapes - Sometimes makes very large errors",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "SLIC (Achanta et al. PAMI 2012)\nhttp://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf\nInitialize cluster centers on pixel grid in steps S - Features: Lab color, x-y position 2. Move centers to position in 3x3 window with smallest gradient\n1.\n3. Compare each pixel to cluster center within 2S pixel distance and assign to nearest\n4. Recompute cluster centers as mean color/position of pixels belonging to each cluster Stop when residual error is small 5.\n+ Fast 0.36s for 320x240 + Regular superpixels + Superpixels fit boundaries - May miss thin objects - Large number of superpixels",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Choices in segmentation algorithms  Oversegmentation\n Watershed + Pb  my favorite  Felzenszwalb and Huttenlocher 2004  pretty good\nhttp://www.cs.brown.edu/~pff/segment/\n SLIC  also a good option  Turbopixels  Mean-shift\n Larger regions\n Hierarchical segmentation (e.g., from Pb)  Normalized cuts  Mean-shift  Seed + graph cuts (discussed later)",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Multiple segmentations  When creating regions for pixel classification or object detection, don’t commit to one partitioning\n Strategies:\n Hierarchical segmentation\n Occlusion boundaries hierarchy: Hoiem et al. IJCV 2011 (uses trained classifier to\nmerge)\n Pb+watershed hierarchy: Arbeleaz et al. CVPR 2009  Selective search: FH + agglomerative clustering\n Vary segmentation parameters\n E.g., multiple graph-based segmentations or mean-shift segmentations\n Region proposals\n Propose seed superpixel, try to segment out object that contains it (Endres\nHoiem ECCV 2010, Carreira Sminchisescu CVPR 2010)",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Things to remember • Gestalt cues and principles of organization\nUses of segmentation\n– Efficiency – Better features – Propose object regions – Want the segmented object\nMean-shift segmentation\n– Good general-purpose segmentation method – Generally useful clustering, tracking technique\nWatershed segmentation\n– Good for hierarchical segmentation – Use in combination with boundary prediction",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide credits\n Slides borrowed from Derek Hoiem\n61\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "62\nEnd – segmentation part 1\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/07-Segmentation part1-KMeans-Mean shift-Watershed-Graph-Based-SLIC.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "54b5367fb8794d65a0962b26cb0e8882.json": [
      {
        "type": "application/pdf",
        "text": "Object Instance Recognition\nSlides by Derek Hoiem",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today’s class\nObject instance recognition\nExample of alignment-based category recognition",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Recall: 2D image transformations",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Object Instance Recognition\n1. Match keypoints to object model\nA1\nA2\nA3\nMatched keypoints\n2. Solve for affine transformation parameters\nAffine Parameters\n3. Score by inliers and choose solutions with score above threshold\n# Inliers\nThis Class\nChoose hypothesis with max score above threshold",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Overview of Keypoint Matching\n1. Find a set of distinctive key- points\nA1\nA2\nA3\n2. Define a region around each keypoint\n3. Extract and\nAf\nBf\nnormalize the region content\ns e x p N\nl\ni\ne.g. color\ne.g. color\n4. Compute a local descriptor from the normalized region\nN pixels\nTffdBA),(\n5. Match local descriptors\nK. Grauman, B. Leibe",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Finding the objects (overview)\nInput Image\nStored Image\n1. Match interest points from input image to database image 2. Matched points vote for rough position/orientation/scale of object Find position/orientation/scales that have at least three votes 3. 4. Compute affine registration and matches using iterative least\nsquares with outlier check\n5. Report object if there are at least T matched points",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Matching Keypoints\nWant to match keypoints between:\n1. Query image 2. Stored image containing the object\nGiven descriptor x0, find two nearest neighbors x1, x2 with distances d1, d2\nx1 matches x0 if d1/d2 < 0.8\n– This gets rid of 90% false matches, 5% of true\nmatches in Lowe’s study",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine Object Model\nAccounts for 3D rotation of a surface under orthographic projection",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine Object Model\n1yxfedcbayx\n211221111.000110000001xyxfedcbayxyxyx\nWhat is the minimum number of matched points that we need?",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Finding the objects (in detail)\n1. Match interest points from input image to database image 2. Get location/scale/orientation using Hough voting In training, each point has known position/scale/orientation wrt whole object\n– Matched points vote for the position, scale, and orientation\n–\nof the entire object Bins for x, y, scale, orientation\n•\nWide bins (0.25 object length in position, 2x scale, 30 degrees orientation) Vote for two closest bin centers in each direction (16 votes total)\n3. Geometric verification\n– –\nFor each bin with at least 3 keypoints Iterate between least squares fit and checking for inliers and outliers\n4. Report object if > T inliers (T is typically 3, can be computed to match some probabilistic threshold)",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of recognized objects",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "View interpolation\nTraining\n– Given images of different\nviewpoints\n– Cluster similar viewpoints using feature matches – Link features in adjacent\nviews\nRecognition\n– Feature matches may be spread over several training viewpoints  Use the known links to\n“transfer votes” to other viewpoints\n[Lowe01]\nSlide credit: David Lowe",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Applications\nSony Aibo\n(Evolution Robotics)\nSIFT usage – Recognize\ndocking station – Communicate\nwith visual cards\nOther uses\n– Place recognition – Loop closure in SLAM\nK. Grauman, B. Leibe\n14 Slide credit: David Lowe",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Location Recognition\nTraining\n[Lowe04] Slide credit: David Lowe",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Another application: category recognition\nGoal: identify what type of object is in the image • Approach: align to known objects and choose category with best match\n?\n“Shape matching and object recognition using low distortion correspondence”, Berg et al., CVPR 2005: http://www.cnbc.cmu.edu/cns/papers/berg-cvpr05.pdf",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Summary of algorithm\nInput: query q and exemplar e • For each: sample edge points and create “geometric blur” descriptor\nCompute match cost c to match points in q to each point in e\nCompute deformation cost H that penalizes change in orientation and scale for pairs of matched points • Solve a binary quadratic program to get correspondence that minimizes c and H, using thin-plate spline deformation\nRecord total cost for e, repeat for all exemplars, choose exemplar with minimum cost\nInput, Edge Maps\nGeometric Blur\nFeature Points\nCorrespondences",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of Matches",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of Matches",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Other ideas worth being aware of\nThin-plate splines: combines global affine warp with smooth local deformation\nRobust non-rigid point matching:\nhttp://noodle.med.yale.edu/~chui/tps-rpm.html (includes code, demo, paper)",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Key concepts\nAlignment\n– Hough transform – RANSAC – ICP\nObject instance recognition – Find keypoints, compute descriptors\n– Match descriptors – Vote for / fit affine parameters – Return object if # inliers > T",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/18-InstanceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "58bd7c4cd6b04082b13638be3ca33642.json": [
      {
        "type": "application/pdf",
        "text": "1\nFinding lines – part 1\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Edge detection\n Canny edge detector  Berkeley edge probability\n Line fitting\n Hough transform  RANSAC\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Why edges?\n We know edges are special\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edge detection - goal\n Goal:\nMap image from 2d array of pixels to a set of curves or line segments or contours.  Most semantic and shape information from the image can be\nencoded in the edges\n A more compact representation than a complete image\n Ideal:\nArtist’s Line drawing (but artists use prior knowledge)\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What can cause an edge?\nSurface normal discontinuity\nDepth discontinuity\nIllumination discontinuity\nSurface color discontinuity\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What can cause an edge?\nReflectance change: appearance information, texture\nChange in surface orientation: shape\n6\nLihi Zelnik-Manor, Computer Vision\nDepth discontinuity: object boundary\nCast shadows",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Contrast and invariance\n7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edges look like steep cliffs\n8\nLihi Zelnik-Manor, Computer Vision\nSource: S. Seitz",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Edge detection\n Canny edge detector  Berkeley edge probability\n Line fitting\n Hough transform  RANSAC\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Derivatives and edges\n An edge is a place of rapid change in the image intensity\nfunction.\nimage\nintensity function (along horizontal scanline)\nfirst derivative\nedges correspond to extrema of derivative\n10\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image gradient\nThe gradient of an image:\nThe gradient points in the direction of most rapid change in intensity\nThe gradient direction (orientation of edge normal) is given by:\nThe edge strength is given by the gradient magnitude\n11\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Differentiation and convolution\nFor 2D function, f(x,y), the partial derivative is:\n),(),(lim),(0yxfyxfxyxf\nFor discrete data, we can approximate using finite differences:\n(,)(1,)(,)fxyfxyfxyxx\nTo implement above as convolution, what would be the associated filter?\n1 1\n12\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Assorted finite difference filters\n>> My = fspecial(‘sobel’); >> outim = imfilter(double(im), My); >> imagesc(outim); >> colormap gray;\n13\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Partial derivatives of an image\nxyxf),(\nWhich shows changes with respect to x? y?\n14\nLihi Zelnik-Manor, Computer Vision\nyyxf),(",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Intensity profile of one row\nIntensity\nGradient\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Effects of noise\nConsider a single row or column of the image\n Plotting intensity as a function of position gives a signal\nWhere is the edge?\n16\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Effects of noise\n Finite difference filters respond strongly to noise\n Image noise results in pixels that look very different from\ntheir neighbors\n Generally, the larger the noise the stronger the response\n What can be done?\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Effects of noise\n Finite difference filters respond strongly to noise\n Image noise results in pixels that look very different from\ntheir neighbors\n Generally, the larger the noise the stronger the response\n What can be done?\n Smoothing the image should help – it forces pixels to look\nmore like their neighbors\n18\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Solution: smooth first\nf\ng\nfg\ndfgdx\nWhere is the edge?\nLook for peaks in\n19\nLihi Zelnik-Manor, Computer Vision\ndfgdx",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Derivative theorem of convolution\nDifferentiation property of convolution:\nf\ndgdx\ndfgdx\n20\nLihi Zelnik-Manor, Computer Vision\n\nddfgfgdxdx",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Derivative of Gaussian filter\n11\nIs this a separable filter?\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Derivative of Gaussian filters\nx-direction\ny-direction\nWhich one finds horizontal/vertical edges?\n22\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Smoothing tradeoffs\n1 pixel\n3 pixels\n7 pixels\nSmoothed derivative removes noise, but blurs edge. Also finds edges at different “scales”.\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Laplacian of Gaussian (LoG filter)\nf\n22dgdx\n22dfgdx\nWhere is the edge? Zero-crossings of bottom graph\n24\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2D edge detection filters\nGaussian\nderivative of Gaussian\ng\ndgdx\n25\nLihi Zelnik-Manor, Computer Vision\nLaplacian of Gaussian\n2222dgdgdxdy",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "DoG = Difference of Gaussians\n Can approximate Laplacian filter\n26\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What is a good edge detector?  Good detection:\n Minimize false positives (wrong detections)\n Minimize false negatives (missing real edges)\n Maximize true detections\n Good localization:\n Detected edges should be as close as possible to the\ntrue edges\n Single response:\n Return a single detection for each true edge point\n Connect detections to lines\n27\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What is a good edge detector?  Good detection\n Good localization\n Single response\nWhich of these detections is the best?\n28\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What are the parameters?\n Scale  Threshold\n29\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scale selection\n Recall: We first smooth the image with a Gaussian\nkernel to reduce noise.\n The scale of the Gaussian determines how much\nsmoothing we apply\n…\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Effect of σ on derivatives\n The apparent structures differ depending on Gaussian’s\nscale parameter.\n Large scale: larger scale edges detected  Small scale: finer features detected\nσ = 1 pixel\nσ = 3 pixels\n31\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How do we chose the scale?  It depends what we’re looking for:\n Too fine of a scale…can’t see the forest for the trees.  Too coarse of a scale…can’t tell the maple grain from the\ncherry.\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What are the parameters?\n Scale  Threshold\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Thresholding\n Choose a threshold value  Set any pixels less than thresh to zero (off)  Set any pixels greater than or equal to thresh to one (on)\n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Original image\n35\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gradient magnitude\n36\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Using a low threshold\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Using a higher threshold\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Canny edge detector\n Probably the most widely used edge detector in computer\nvision\n Key idea:\ndetect step-edges that are corrupted by additive Gaussian noise\n Theorem:\nCanny has shown that the first derivative of the Gaussian closely approximates the operator that optimizes the product of signal-to-noise ratio and localization\nJ. Canny, A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679--‐714, 1986.\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Canny edge detector\n\n\n\n\nFilter image with derivative of Gaussian Find magnitude and orientation of gradient Non-maximum suppression: (Localization)  Thin multi-pixel wide “ridges” down to single pixel width Linking and thresholding (hysteresis): (Linking)  Define two thresholds: low and high  Use the high threshold to start edge curves and the low\nthreshold to continue them\n\nMATLAB: edge(image, ‘canny’);\n\n>>help edge\n40\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example - input\noriginal image (Lena)\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example – step 1\nnorm of the gradient\n42\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example – step 2\nthresholding\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example – step 3\nthresholding\n44\nLihi Zelnik-Manor, Computer Vision\nHow to turn these thick regions of the gradient into curves?",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Non-maximum suppression\nCheck if pixel q is local maximum along gradient direction,\nselect single max across width of the edge  requires checking interpolated pixels p and r\n45\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example – step 3\nThinning (non-maximum supression)\n46\nLihi Zelnik-Manor, Computer Vision\nProblem: pixels along this edge didn’t survive the thresholding",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edge linking\n Assume the marked point is an edge point.  Then we construct the tangent to the edge curve (which is normal to the gradient at that point) and use this to predict the next points (here either r or s).\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hysteresis thresholding\n Check that maximum value of gradient value is\nsufficiently large  drop-outs? use hysteresis\n use a high threshold to start edge curves and a low threshold to\ncontinue them.\n48\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hysteresis thresholding\noriginal image\nhigh threshold (strong edges)\nlow threshold (weak edges)\n49\nLihi Zelnik-Manor, Computer Vision\nhysteresis threshold",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hysteresis thresholding\nhigh threshold (strong edges)\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hysteresis thresholding\nlow threshold (weak edges)\n51\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hysteresis thresholding\nhysteresis threshold\n52\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Canny - results\n53\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Object boundaries vs. edges\nBackground 54\nTexture\nLihi Zelnik-Manor, Computer Vision\nShadows",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Edge detection\n Canny edge detector  Berkeley edge probability\n Line fitting\n Hough transform  RANSAC\n55\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "At Berkeley:\n1. Collect Data Set of Human segmented images\n2.\nLearn Local Boundary Model for combining brightness, color and texture\n3. Global framework to capture closure, continuity 4. Detect and localize junctions Integrate low, mid and high-level information for grouping and figure-ground segmentation\nD. Martin, C. Fowlkes, D. Tal, J. Malik. \"A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics\", ICCV, 2001\nhtpp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/\n56\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Berkeley Segmentation DataSet [BSDS]\nInput\nHuman segmentation\n57\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "id",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Contour detection ~1970\n58\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Contour detection ~1990\n59\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Contour detection ~2004\n60\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page60",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Contour detection ~2008\nInclude global cues\n61\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Martin, Fowlkes, Malik PAMI 04\nGradient\nCue Combination\nBrightness\nColor\nModel\nTexture\n Goal: learn the posterior probability of a boundary\nPb(x,y,) from local information only\n Challenges:\n computing the cues,  cue combination\n62\nLihi Zelnik-Manor, Computer Vision\nPb",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pb: gradient cue\nImage\nOE = Oriented Energy\n63\nLihi Zelnik-Manor, Computer Vision\n+",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pb: brightness and color cues\nImage\nBG = Brightness Gradient\nCG = Color Gradient\n64\nLihi Zelnik-Manor, Computer Vision\nDifference of L* distributions\nBG = χ2(h1(L),h2(L))\nDifference of a* distributions\nCG = χ2(h1(a),h2(a))+ χ2(h1(b),h2(b))",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pb: texture cue\nImage\nTG = Texture Gradient\n65\nLihi Zelnik-Manor, Computer Vision\nDifference of filter distributions\nTG= 𝑓 χ2(h1(f),h2(f))",
        "offset": "page65",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pb: cue design\nImage\n66\nLihi Zelnik-Manor, Computer Vision\nMultiple orientations\nMultiple disk radii",
        "offset": "page66",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Filter banks\norientations\nscales\n“Edges”\n“Bars”\n“Spots”\n What filters to put in the bank?\n Typically we want a combination of scales and\norientations, different types of patterns.\nMatlab code available for these examples: http://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html\n67\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page67",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Filter bank\n68\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page68",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "g p\nj .\n2 p a c n\ni t s u a m o c . r e r o p x e s a x e\n/\nl\nt .\nw w w\n/ / :\np\nt t\nh m o r f\ne g a m\nI\n69\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page69",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "70\nShowing magnitude of responses\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page70",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "71\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page71",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "72\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page72",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "73\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page73",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "74\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page74",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "75\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page75",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "76\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page76",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "77\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page77",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "78\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page78",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "79\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page79",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "80\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page80",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "81\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page81",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "82\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page82",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "83\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page83",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "84\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page84",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "85\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page85",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "86\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page86",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "87\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page87",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "88\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page88",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1\n2\n3\nYou try: Can you match the texture to the response?\nFilters\nA\nB\nC\nMean abs responses\n89\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page89",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1\n2\n3\nYou try: Can you match the texture to the response?\nFilters\nA\nB\nC\nMean abs responses\n90\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page90",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "91\nLihi Zelnik-Manor, Computer Vision\n[r1, r2, …, r38]\nWe can form a feature vector from the list of responses at each pixel.",
        "offset": "page91",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Martin, Fowlkes, Malik PAMI 04\nGradient OE\nCue Combination\nBrightness BG\nColor CG\nModel\nTexture TG\n Goal: learn the posterior probability of a boundary\nPb(x,y,) from local information only\n Challenges:\n computing the cues,  cue combination\n92\nLihi Zelnik-Manor, Computer Vision\nPb",
        "offset": "page92",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "93\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page93",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "94\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page94",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Parameter tuning (cue optimization)\n Scale  Disk radius  Number of bins in histogram\nTrained on labeled data\n95\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page95",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Cue combination\n Tested many options:\n Logistic regression – the winner!  SVM (support vector machines)  Hierarchical Mixture of Experts  Classification trees\n96\nLihi Zelnik-Manor, Computer Vision\nThe logistic function",
        "offset": "page96",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Various cue combinations\n97\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page97",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example results\n98\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page98",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Forty years of contour detection\nRoberts (1965)\nSobel (1968)\nPrewitt (1970)\nMarr Hildreth (1980)\nCanny (1986)\nPerona Malik (1990)\n99\nLihi Zelnik-Manor, Computer Vision\nMartin Fowlkes Malik (2004)\nMaire Arbelaez Fowlkes Malik (2008)\n99",
        "offset": "page99",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Global pB boundary detector\nFigure from Fowlkes",
        "offset": "page100",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edge Detection with Structured Random Forests (Dollar Zitnick ICCV 2013)\n Goal: quickly predict whether each pixel is an\nedge\n Insights\n Predictions can be learned from training data  Predictions for nearby pixels should not be\nindependent\n Solution\n Train structured random forests to split data into\npatches with similar boundaries based on features\n Predict boundaries at patch level, rather than pixel level, and aggregate (average votes)\nhttp://research.microsoft.com/pubs/202540/DollarICCV13edges.pdf\nBoundaries in patch",
        "offset": "page101",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edge Detection with Structured Random Forests\n Algorithm\n1. Extract overlapping 32x32 patches at three scales\n2.\nFeatures are pixel values and pairwise differences in feature maps (LUV color, gradient magnitude, oriented gradient)\n3. Predict 𝑇 boundary maps in the central 16x16 region using 𝑇 trained decision trees\n4. Average predictions for each pixel across all patches",
        "offset": "page102",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edge Detection with Structured Random Forests Results\nBSDS 500\nNYU Depth dataset edges",
        "offset": "page103",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Edge Detection with Structured Random Forests\nGround truth\nResults (multiscale)",
        "offset": "page104",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Forty years of contour detection\nRoberts (1965)\nSobel (1968)\nPrewitt (1970)\nMarr Hildreth (1980)\nCanny (1986)\nPerona Malik (1990)\n105\nLihi Zelnik-Manor, Computer Vision\nMartin Fowlkes Malik (2004)\nMaire Arbelaez Fowlkes Malik (2008)\nDollar Zitnick (2013)",
        "offset": "page105",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Crisp Boundary Detection using Pointwise Mutual Information (Isola et al. ECCV 2014)\nPixel color combinations that are unlikely to be together are edges\nAlgorithm:\nKernel density estimation\nSpectral clustering\nhttp://web.mit.edu/phillipi/www/publications/crisp_boundaries.pdf",
        "offset": "page106",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Crisp Boundary Detection using Pointwise Mutual Information",
        "offset": "page107",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slide Credits\n Trevor Darell  Kristen Grauman  Jitendra Malik  FeiFei Li  Derek Hoiem  and Seitz, Marschner, Lazebnik and others\n108\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page108",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "109\nEnd – finding lines\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page109",
        "ref": "/home/ameer/Kaleidoo/server/uploads/05-Edge Detection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "5b615b60f32243759d5afac91caf2f48.json": [
      {
        "type": "application/pdf",
        "text": "1\nFeature detectors\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Stereo-view geometry\n Correspondence:\nGiven a point in one image, how can I find the corresponding point in another image?\n Camera geometry:\nGiven corresponding points in two images, find camera intrinsic and extrinsic parameters\n Scene geometry:\nFind coordinates of 3D point from its projection into two or multiple images.\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Motivation\n Global representations have major limitations  Instead, describe and match only local regions  Increased robustness to\n Occlusions\nArticulation\nIntra-category variations\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application: Image matching\nby Diva Sian\n6\nLihi Zelnik-Manor, Computer Vision\nby swashford",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harder case\nby Diva Sian\n7\nLihi Zelnik-Manor, Computer Vision\nby scgbt",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harder still?\n8\nNASA Mars Rover images\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Answer below (look for tiny colored squares…)\nNASA Mars Rover images with SIFT feature matches. Figure by Noah Snavely\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application: Image stitching\n10\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application: Image stitching\n Procedure:\n Detect feature points in both images\n11\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application: Image stitching\n Procedure:\n Detect feature points in both images  Find corresponding pairs\n12\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Application: Image stitching\n Procedure:\n Detect feature points in both images  Find corresponding pairs  Use these pairs to align images\n13\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "General approach  Find a set of distinctive\nkey-points\n Define a region around\neach keypoint\n Extract and normalize the region content\n Compute a local\ndescriptor from the normalized regions\n Match local descriptors\n14\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Requirements\n Problem 1:\n Detect the same point independently in both images\nno chance to match!\nWe need a repeatable detector\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Requirements\n Problem 1:\n Detect the same point independently in both images\n Problem 2:\n For each point correctly recognize the corresponding one\n?\nWe need a repeatable and distinctive detector\n16\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Invariance 1: Geometric transformations\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Invariance 2: Photometric transformations\nOften modeled as a linear transformation:\nscaling + offset of colors\n18\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "And other nuisances…\n Noise  Blur  Compression artifacts  …\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Requirements summary\n Region extraction needs to be repeatable and accurate\n Invariant to translation, rotation, scale changes  Robust or covariant to out‐of‐plane (~affine) transformations  Robust to lighting variations, noise, blur, quantization\n Locality: Features are local, therefore robust to occlusion\nand clutter.\n Quantity: We need a sufficient number of regions to cover\nthe object.\n Distinctiveness: The regions should contain “interesting”\nstructure.\n Efficiency: Close to real‐time performance.\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Many existing detectors are available  Hessian & Harris  Laplacian, DoG [Lindeberg ‘98], [Lowe ‘99]  Harris‐/Hessian‐Laplace [Mikolajczyk & Schmid ‘01]  Harris‐/Hessian‐Affine  EBR and IBR  MSER  Salient Regions  Others…\n[Beaudet ‘78], [Harris ‘88]\n[Mikolajczyk & Schmid ‘04]\n[Tuytelaars & Van Gool ‘04]\n[Matas ‘02]\n[Kadir & Brady ‘01]\n Those detectors have become a basic building block for many\nrecent applications in Computer Vision.\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n22\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Keypoint localization\n Goals:\n Repeatable detection  Precise localization  Interesting content\n Look for image regions\nthat are unusual  Lead to unambiguous\nmatches in other images\nHow to define “unusual”?\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Finding Corners\n Key property:\n In the region around a corner, image gradient has two or more\ndominant directions\n Corners are repeatable and distinctive\n[Harris et al. \"A Combined Corner and Edge Detector.“ 1988]\n24\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Corners as distinctive interest points\n Design criteria:\n We should easily recognize the point by looking through a\nsmall window (locality)\n Shifting the window in any direction should give a large change in\nintensity (good localization)\n“flat” region: no change in all directions\n25\n“edge”: no change along the edge direction Lihi Zelnik-Manor, Computer Vision\n“corner”: significant change in all directions",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector formulation\n Change of intensity for the shift [u,v]\n2,(,)(,)(,)(,)xyEuvwxyIxuyvIxy\nWindow function\nShifted intensity\nIntensity\nWindow function w(x,y) =\nor\n1 in window, 0 outside\nGaussian\n26\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector formulation This measure of change can be approximated by (Taylor expansion):\nvuMvuvuE][),(\nwhere M is a 22 matrix computed from image derivatives:\n22,(,)xxyxyxyyIIIMwxyIII\nGradient with respect to x, times gradient with respect to y\nSum over image region – area we are checking for corner\n27\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector formulation\n What will M look like for each case?\n (On the board)\n“flat” region: no change in all directions\n28\n“edge”: no change along the edge direction Lihi Zelnik-Manor, Computer Vision\n“corner”: significant change in all directions",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Interpreting the eigenvalues Classification of image points using eigenvalues of M:\n2\n“Edge” 2 >> 1\n“Corner” 1 and 2 are large, 1 ~ 2; E increases in all directions\n1 and 2 are small; E is almost constant in all directions\n“Flat” region\n“Edge” 1 >> 2\n29\nLihi Zelnik-Manor, Computer Vision\n1",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Corner response function\n221212)()(trace)det(MMR\n“Edge” R < 0\n“Corner” R > 0\nFast approximation: • Avoid computing the\neigenvalues\nα: constant (0.04 to 0.15)\n|R| small\n“Flat” region\n“Edge” R < 0\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Rotation invariance\n Eigenvalues are of the same nature as before\n“flat” region: no change in all directions\n31\n“edge”: no change along the edge direction Lihi Zelnik-Manor, Computer Vision\n“corner”: significant change in all directions",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Summary: Harris Corner Detector\n Compute second moment matrix M\n1. Compute derivatives\n,xyII\n2.\nSquare of derivatives\n22,,xyxyIIII\n2xI\n3. Gaussian filter\n()g\n2()xgI\n4. Compute R scores\n Find points with large corner response\nR > threshold\n5. Perform non-maximum supression\n32\nLihi Zelnik-Manor, Computer Vision\nxI\n2yI\n2()ygI\nyI\nxyII\n()xygII\nR",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Workflow\n33\nLihi Zelnik-Manor, Computer Vision Slide adapted form Darya Frolova, Denis Simakov, Weizmann Institute.",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Workflow Compute corner response R\n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Workflow\nFind points with large corner response: R>threshold\n35\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Workflow\nTake only the points of local maxima of R\n36\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Workflow Resulting Harris points\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris detector – example result\nA very accurate corner detector\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris detector – example result\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris detector – example result\nResults are usually good for stereo correspondences\n40\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Properties  Is it rotation invariant?\nEllipse rotates but its shape (i.e. eigenvalues) remains the same\nYes: Corner response R is invariant to image rotation\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Properties\n Is it invariant to image scale?\nAll points will be classified as edges\nNo: Not invariant to image scale!\n42\nLihi Zelnik-Manor, Computer Vision\nCorner !",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hessian detector [Beaudet 78]\n Hessian determinant\n Determined by second derivatives\n()xxxyxyyyIIHessianIII\n Key idea:\n Search for strong derivatives in\ntwo orthogonal directions\n44\nLihi Zelnik-Manor, Computer Vision\nxxI\nxyI\nyyI",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hessian detector [Beaudet 78]\n Hessian determinant\n Determined by second derivatives\n()xxxyxyyyIIHessianIII\n Key idea:\n Search for strong derivatives in\ntwo orthogonal directions\n2det()xxyyxyHessianIIII\n45\nLihi Zelnik-Manor, Computer Vision\nxxI\nxyI\nyyI",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hessian detector – example result\nResponses mainly on corners and strongly textured areas\n46\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hessian detector – example result\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "48\nEnd – Feature detectors\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/09-feature-detectors-Harris&Hassian.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "5ce592d94f254bd9aa91709c4cdb2b8c.json": [
      {
        "type": "application/pdf",
        "text": "Object Category Detection: Parts-based Models\nSlides borrowed from Derek Hoiem",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Goal: Detect all instances of objects\nCars\nFaces\nCats",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Last class: sliding window detection",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Object model: last class\nStatistical Template in Bounding Box\n– Object is some (x,y,w,h) in image – Features defined wrt bounding box coordinates\nImage\nTemplate Visualization\nImages from Felzenszwalb",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When do statistical templates make sense?\nCaltech 101 Average Object Images",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Object models: this class\nArticulated parts model\n– Object is configuration of parts – Each part is detectable\nImages from Felzenszwalb",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Deformable objects\nImages from Caltech-256\nSlide Credit: Duan Tran",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Deformable objects\nImages from D. Ramanan’s dataset\nSlide Credit: Duan Tran",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Compositional objects",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Parts-based Models\nDefine object by collection of parts modeled by\n1. Appearance 2. Spatial configuration\nSlide credit: Rob Fergus",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to model spatial relations?\nOne extreme: fixed template",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to model spatial relations?\nAnother extreme: bag of words\n=",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to model spatial relations?\nStar-shaped model\nPart\nPart\nPart\nRoot\nPart\nPart",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to model spatial relations?\nStar-shaped model\nX\n=\nX\nX\nPart\nPart\nPart\nRoot\nPart\nPart",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to model spatial relations?\nTree-shaped model",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How to model spatial relations?\nMany others...\nO(N6)\nO(N2)\nO(N3)\nO(N2)\nFergus et al. ’03 Fei-Fei et al. ‘03\nLeibe et al. ’04, ‘08 Crandall et al. ‘05 Fergus et al. ’05\nCrandall et al. ‘05\nFelzenszwalb & Huttenlocher ‘05\nCsurka ’04 Vasconcelos ‘00\nBouchard & Triggs ‘05\nCarneiro & Lowe ‘06\nfrom [Carneiro & Lowe, ECCV’06]",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today’s class\n1. Star-shaped model\n– Example: Deformable Parts Model\nFelzenswalb et al. 2010\n2. Tree-shaped model\n– Example: Pictorial structures\nFelzenszwalb Huttenlocher 2005\n3. Sequential prediction models\nPart\nPart\nPart\nRoot\nPart\nPart",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Deformable Latent Parts Model (DPM)\nDetections\nTemplate Visualization\nFelzenszwalb et al. 2008, 2010",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Review: Dalal-Triggs detector\n.\n= 0.16\nImage Window\nHOG\nSVM weights (pos/neg)\nscore\n1. Extract fixed-sized (64x128 pixel) window at each position and scale\n2. Compute HOG (histogram of gradient) features within each window\n3. Score the window with a linear SVM classifier 4. Perform non-maxima suppression to remove overlapping detections with lower scores",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Deformable parts model\nRoot filter models coarse whole-object appearance\nPart filters model finer- scale appearance of smaller patches\nFor each root window, part positions that maximize appearance score minus spatial cost are found\nTotal score is sum of scores of each filter and spatial costs\nRoot filter\nPart filters\nSpatial costs",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "DPM: mixture model\nEach positive example is modeled by one of M detectors\nIn testing, all detectors are applied with non- max suppression",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "et",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Improvement over time for HOG-based detectors\nAP on PASCAL VOC 2007\nDPM v4 (3 comp., left/right flip)\n0.4\nDPM v5\n0.35\nDPM v2 (2 comp., context)\n0.3\nDPM v3\n0.25\nDPM v1 (1 component, parts)\n0.2\n0.15\n0.1\nDalal-Triggs (1 component, no parts)\n0.05\n0 2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tree-shaped model",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pictorial Structures Model\nPart = oriented rectangle\nSpatial model = relative size/orientation\nFelzenszwalb and Huttenlocher 2005",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pictorial Structures Model\nAppearance likelihood\nGeometry likelihood",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Modeling the Appearance\nAny appearance model could be used\n– HOG Templates, etc. – Here: rectangles fit to background subtracted binary map\nCan train appearance models independently (easy, not as good) or jointly (more complicated but better)\nAppearance likelihood\nGeometry likelihood",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Part representation\nBackground subtraction",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results for person matching\n32",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results for person matching\n33",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Enhanced pictorial structures\nLearn spatial prior\nColor models from soft segmentation (initialized by location priors of each part)\nBMVC 2009",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2 minute break",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "ro",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example from Ramakrishna",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Sequential structured prediction\nCan consider pose estimation as predicting a set of related variables (called structured prediction) – Some parts easy to find (head), some are hard (wrists)\nOne solution: jointly solve for most likely variables (DPM, pictorial structures)\nAnother solution: iteratively predict each variable based in part on previous predictions",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pose machines\nRamakrishna et al. ECCV 2014",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "et",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example results",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Graphical models vs. structured prediction\nAdvantages of sequential prediction\n– Simple procedures for training and inference – Learns how much to rely on each prediction – Can model very complex relations\nAdvantages of BP/graphcut/etc\n– Elegant – Relations are explicitly modeled – Exact inference in some cases",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/22-PartsBasedModels.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "84c62442502848da9ef8442f77b362c5.json": [
      {
        "type": "application/pdf",
        "text": "1\nAlignment\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations  Panoramas\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations  Panoramas\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature matching\nWe know how to detect and describe good points Next question: How to match them?\n?\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature matching\nGiven a feature in Image1, how to find the best match in Image2\n1. Define distance function that compares two descriptors 2. Compute distances between all pairs features and find the ones with min distance\nf1\n5\nLihi Zelnik-Manor, Computer Vision\nf2",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature matching - better\nGiven a feature in Image1, how to find the best match in Image2\n1. Define distance function that compares two descriptors 2. Compute distances between all pairs features and find matches that dist(f1,best match) / dist(f1,second-best match) > threshold\nf1\nf2\n'\n6\nLihi Zelnik-Manor, Computer Vision\nf2",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Typical feature matching results\n Some matches are correct  Some matches are incorrect\n Solution: search for a set of geometrically consistent matches\n7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What we need to solve\n Given source and target images, how do we compute the\ntransformation between them?\n8\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations  Panoramas\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image alignment\n Turns out that in many cases there’s a global transformation relating points in two images:\n𝑝′ = 𝐻𝑝\n11''yxHyx\n𝑝\n𝑝′\n10\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Parametric (global) warping\n Examples of parametric transformations:\ntranslation\nIn-plane rotation\naffine\nperspective\n11\nLihi Zelnik-Manor, Computer Vision\noriginal\nAspect ratio",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scaling  Scaling a coordinate means multiplying each of its components by a scalar  Uniform scaling means this scalar is the same for all components:\n 2",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scaling  Non-uniform scaling: different scalars per component:\nX  2, Y  0.5",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scaling\n Scaling operation:\n Or, in matrix form:\nbyyaxx''\n110000001''yxbayx\nscaling matrix S",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2-D Rotation\n(x’, y’)\n\n(x, y)\nx’ = x cos() - y sin() y’ = x sin() + y cos()",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2-D Rotation\n(x’, y’)\n(x, y)\n\nf\nPolar coordinates… x = r cos (f) y = r sin (f) x’ = r cos (f + ) y’ = r sin (f + )\nTrig Identity… x’ = r cos(f) cos() – r sin(f) sin() y’ = r sin(f) cos() + r cos(f) sin()\nSubstitute… x’ = x cos() - y sin() y’ = x sin() + y cos()",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2-D Rotation This is easy to capture in matrix form:\n11000cossin0sincos1''yxyx\nR\nEven though sin() and cos() are nonlinear functions of ,\n x’ is a linear combination of x and y  y’ is a linear combination of x and y\nWhat is the inverse transformation?\n Rotation by –  For rotation matrices 𝑅−1 = 𝑅𝑇",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Basic 2D2D Transformations\n110010011''yxttyxyx\n11000cossin0sincos1''yxyx\nTranslation\nRotate in-plane\n''10011xabcxydefy\nAffine Combination of translation, scale, rotation, shear\n18\nLihi Zelnik-Manor, Computer Vision\n110000001''yxbayx\nScale/Aspect ratio\n110001011''yxbayx\nShear",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine Transformations  Affine transformations are combinations of …\n Linear transformations, and  Translations\nwyxfedcbawyx100''\n Properties of affine transformations:\n Origin does not necessarily map to origin  Lines map to lines  Parallel lines remain parallel  Ratios are preserved  Closed under composition",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective Transformations\n Projective transformations …\n Affine transformations, and  Projective warps\nwyxihgfedcbawyx'''\n Properties of projective transformations:\n Lines map to lines  Parallel lines do not necessarily remain parallel  Ratios are not preserved  Closed under composition  Projective matrix is defined up to a scale (8 DOF)",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Basic 2D2D Transformations\n110010011''yxttyxyx\n11000cossin0sincos1''yxyx\n110000001''yxbayx\nTranslation\nRotate in-plane\nScale/Aspect ratio\n''10011xabcxydefy\n''11xabcxydefywgh\nAffine\nProjective (Homography)\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2D image transformations (reference table)",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When do we get affine or homography?\n Camera does not translate (only rotation and scale)\nTransformation between coordinates in 3D:\n𝑃′ = 𝑅𝑃 + 𝑇\n𝑃′ = [𝑋′, 𝑌′, 𝑍′]\n𝑃 = [𝑋, 𝑌, 𝑍]\nWithout translation:\n𝑝′ → 𝑀′𝑃′\n𝑝 → 𝑀𝑃\n𝑃′ = 𝑅𝑃\nAnd the projections:\n𝑝′ = 𝐻𝑝\n𝑂2\n𝑂1\n𝑅, 𝑇\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When do we get affine or homography?\n Camera does not translate (only rotation and scale)\n Object is planar  Works fine for small viewpoint changes\n24\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Homographies == Planar Perspective Maps\nCalled a homography (or planar perspective map)",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting an affine transformation\nAffine model approximates perspective projection of planar objects.\n26\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations\n Least-squares  RANSAC  Panoramas\n27\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting an affine transformation\nAssuming we know the correspondences, how do we get the transformation?\n),(iiyx\n),(iiyx\n00111iiiixabcxydefy\n28\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting an affine transformation\nAssuming we know the correspondences, how do we get the transformation?\n),(iiyx\n),(iiyx\n00100001iiiiiiabxycxxydyef\n00111iiiixabcxydefy\n29\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting an affine transformation\n Solve with Least-squares\n00100001iiiiiiabxycxxydyef\n How many matches (correspondence pairs) do we need to solve for the\ntransformation parameters?\n Once we have solved for the parameters, how do we compute the coordinates of the corresponding point for any pixel ?\n),(newnewyx\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting a projective transformation  Recall working with homogenous coordinates\n1'1iiiiixabcxydefyghw\n The equations we get are\niiiiiiiiiiiaxbycxgxhyadxeyfygxhya\n Solve with SVD\n''iiiiiixxwyyw\n00010000011iiiiabcdxyexyfgh\n31\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations\n Least-squares  RANSAC  Panoramas\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC\n1. Randomly select a seed group of matches\n2. Compute transformation (using Least-squares) from seed group\n3.\nFind inliers to this transformation\n4.\nIf the number of inliers is sufficiently large, re-compute least- squares estimate of transformation on all of the inliers\n5. Keep the transformation with the largest number of inliers\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\nAll given matches\n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\nSelect one match\n35\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\ncount inliers\n36\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\nSelect one match\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\ncount inliers\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\nKeep the best transformation\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: Translation\nFind “average” translation of “good” matches\n40\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: homography\nAll matches\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: homography\nAfter RANSAC\n42\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC example: homography\nApplying the homography\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations\n Least-squares  RANSAC  Panoramas\n Multi-frame estimation  Warping  Blending\n44\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Panoramas\nObtain a wider angle view by combining multiple images.\n45\nLihi Zelnik-Manor, Computer Vision\ni\nm a g e\nf r o m S\n.\nS e i t z\n. . .",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Panoramas\n46\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "id",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Panoramas\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Panoramas\n48\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Problem: Drift\n Error accumulation\n small (vertical) errors accumulate over time  apply correction so that sum = 0 (for 360° pan.)\n49\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Aligning multiple images\n Bundle adjustment\nijx\nijr\nwarp\n'ijx\ni’th image pair\n11allpairsmatchesiniijijEfe\nijr\nf , xmax let us bound the effect of outliers\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gradient descent\n51\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Newton’s method\n52\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Newton in N dimensions\n53\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Minimizing sum of residuals\n54\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Levenberg-Marquardt\n55\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Levenberg-Marquardt\nAll unknowns\nCompute analytically\nrJ\nJacobian\nPrior parameter Covariance matrix\npC\nStep size\n\n56\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Multi-frame optimization\nRotation matrix\nCalibration matrix\nUnknown parameters for image i\n123Tiiiiif\n57\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Bundle adjustment formulations\nAll pairs optimization:\nConfidence / uncertainty of point i in image j\nMap 2D point i in image j to 2D point in image k\nFull bundle adjustment, using 3-D point positions\nMap 3D point i to 2D point in image i\nBundle adjustment using 3-D ray:\n3-D ray from point i\nAll-pairs 3-D ray formulation:\n3-D ray from points i and j\nProjected point\n58\nLihi Zelnik-Manor, Computer Vision\n3-D ray from point",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations\n Least-squares  RANSAC  Panoramas\n Multi-frame estimation  Warping  Blending\n59\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What we need to solve\n Given source and target images, and the transformation\nbetween them, how do we align them?\n Send each pixel x in image1 to its corresponding location x’ in\nimage 2\nx\nimage1\nx’\nimage 2\n60\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page60",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Forward Warping\n What if pixel lands “between” two pixels?\n Answer: add “contribution” to several pixels and normalize\n(splatting)\nx\nImage 1\nx’\nImage 2\n61\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Forward Warping\n What if pixel lands “between” two pixels?\n Answer: add “contribution” to several pixels and normalize\n(splatting)\n Limitation: Holes (some pixels are never visited)\n“Hole”\nx\nImage 1\nx’\nImage 2\n62\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Inverse Warping\n For each pixel x’ in image 2 find its origin x in image 1\n Problem: What if pixel comes from “between” two pixels?\nx\nImage 1\nx’\nImage 2\n63\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Inverse Warping\n For each pixel x’ in image 2 find its origin x in image 1\n Problem: What if pixel comes from “between” two pixels?  Answer: interpolate color value from neighbors\nx\nImage 1\nx’\nImage 2\n64\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Bilinear interpolation\nSampling at f(x,y):\n65\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page65",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Interpolation\n Possible interpolation filters:\n nearest neighbor  Bilinear interpolation  bicubic interpolation  sinc / FIR\n Needed to prevent “jaggies”\nand “texture crawl”\n66\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page66",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Matching local features  Parametric transformations  Computing parametric transformations\n Least-squares  RANSAC  Panoramas\n Multi-frame estimation  Warping  Blending\n67\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page67",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image feathering\n Weight each image proportional to its distance from the edge\n(distance map [Danielsson, CVGIP 1980]\n 1. Generate weight map for each image  2. Sum up all of the weights and divide by sum:\nweights sum up to 1:\nwi’ = wi / ( ∑i wi)\n68\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page68",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image feathering\n69\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page69",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image feathering\n+\n1 0\n=\n1 0\n70\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page70",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Panoramas – summary\n Detect features  Compute transformations between pairs of frames  Refine transformations using bundle-adjustment  Warp all images onto a single coordinate system  Blend\n71\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page71",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "72\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page72",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "73\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page73",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "74\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page74",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "75\nEnd – Alignment\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page75",
        "ref": "/home/ameer/Kaleidoo/server/uploads/12-alignment-panorama.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "943d17f7b762478989b9823635296818.json": [
      {
        "type": "application/pdf",
        "text": "Face Recognition and Feature Subspaces\nLucas by Chuck Close\nChuck Close, self portrait\nSlides from Derek Hoiem, Lana Lazebnik, Silvio Savarese, Fei-Fei Li",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "This class: face recognition\nTwo methods: “Eigenfaces” and “Fisherfaces”\nFeature subspaces: PCA and FLD\nRecent method: DeepFace\nLook at interesting findings about human face recognition",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Applications of Face Recognition\nSurveillance",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Applications of Face Recognition\nAlbum organization\nhttp://www.apple.com/ilife/iphoto/",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Can be trained to recognize pets!\nhttp://www.maclife.com/article/news/iphotos_faces_recognizes_cats",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Facebook friend-tagging with auto-suggest",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Face recognition: once you’ve detected and cropped a face, try to recognize it\nDetection\nRecognition\n“Sally”",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Face recognition: overview\nTypical scenario:\nfew examples per face, identify or verify test example\nWhat’s hard:\nchanges in expression, lighting, age, occlusion, viewpoint\nBasic approaches (all nearest neighbor)\n1. Project into a new subspace (or kernel space) (e.g., “Eigenfaces”=PCA) 2. Measure face features 3. Make 3d face model, compare shape+appearance (e.g., AAM)",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Typical face recognition scenarios\nVerification: a person is claiming a particular identity; verify whether that is true – E.g., security\nClosed-world identification: assign a face to one person from among a known set\nGeneral identification: assign a face to a known person or to “unknown”",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What makes face recognition hard?\nExpression",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What makes face recognition hard?\nLighting",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What makes face recognition hard?\nOcclusion",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What makes face recognition hard?\nViewpoint",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Simple idea for face recognition\n1. Treat face image as a vector of intensities\nx\n2. Recognize face by nearest neighbor in database\nnyy...1\nxykkkargmin",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The space of all face images\nWhen viewed as vectors of pixel values, face images are extremely high-dimensional – 100x100 image = 10,000 dimensions – Slow and lots of storage\nBut very few 10,000-dimensional vectors are valid face images\nWe want to effectively model the subspace of face images",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The space of all face images\nEigenface idea: construct a low-dimensional linear subspace that best explains the variation in the set of face images",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Principal Component Analysis (PCA)\nGiven: N data points x1, … ,xN in Rd\nGoal: find a new set of features that are linear combinations of the original ones:\nu(xi) = uT(xi – µ)\n(µ: mean of data points)\nChoose unit vector u in Rd that captures the most data variance\nForsyth & Ponce, Sec. 22.3.1, 22.3.2",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Principal Component Analysis\nDirection that maximizes the variance of the projected data:\nN\nMaximize\nsubject to ||u||=1\nProjection of data point\nN\n1/N\nCovariance matrix of data\nThe direction that maximizes the variance is the eigenvector associated with the largest eigenvalue of Σ (can be derived using Raleigh’s quotient or Lagrange multiplier)",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Implementation issue\nCovariance matrix is huge (M2 for M pixels)\nBut typically # examples << M\nSimple trick\n– X is MxN matrix of normalized training data – Solve for eigenvectors u of XTX instead of XXT – Then Xu is eigenvector of covariance XXT – Need to normalize each vector of Xu into unit length",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Eigenfaces (PCA on face images)\n1. Compute the principal components (“eigenfaces”) of the covariance matrix\n𝑿 = 𝒙𝟏 − 𝝁 𝒙𝟐 − 𝝁 … 𝒙𝒏 − 𝝁 [𝑼, 𝝀] = eig(𝑿𝑻𝑿) 𝑽 = 𝑿𝑼\n2. Keep K eigenvectors with largest eigenvalues 𝑽 = 𝑽(: , largest_eig)\n3. Represent all face images in the dataset as linear combinations of eigenfaces – Perform nearest neighbor on these coefficients\n𝑿𝒑𝒄𝒂 = 𝑽 : , largesteig\n𝑻\n𝑿\nM. Turk and A. Pentland, Face Recognition using Eigenfaces, CVPR 1991",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Eigenfaces example\nTraining images • x1,…,xN",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Eigenfaces example\nMean: μ\nTop eigenvectors: u1,…uk",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Visualization of eigenfaces\nPrincipal component (eigenvector) uk\nμ + 3σkuk\nμ – 3σkuk",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representation and reconstruction\nFace x in “face space” coordinates:\n=",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representation and reconstruction\nFace x in “face space” coordinates:\n=\nReconstruction:\n=\n+\n^ x\n=\nµ + w1u1+w2u2+w3u3+w4u4+ …",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Reconstruction\nP = 4\nP = 200\nP = 400\nAfter computing eigenfaces using 400 face images from ORL face database",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Eigenvalues (variance along eigenvectors)",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Note\nPreserving variance (minimizing MSE) does not necessarily lead to qualitatively good reconstruction.\nP = 200",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Recognition with eigenfaces\nProcess labeled training images • Find mean µ and covariance matrix Σ • Find k principal components (eigenvectors of Σ) u1,…uk • Project each training image xi onto subspace spanned by\nprincipal components: (wi1,…,wik) = (u1\nT(xi – µ), … , uk\nT(xi – µ))\nGiven novel image x • Project onto subspace:\nT(x – µ), … , uk\nT(x – µ))\n(w1,…,wk) = (u1\n^\nOptional: check reconstruction error x – x to determine whether image is really a face\nClassify as closest training face in k-dimensional subspace\nM. Turk and A. Pentland, Face Recognition using Eigenfaces, CVPR 1991",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "PCA\nGeneral dimensionality reduction technique\nPreserves most of variance with a much more compact representation – Lower storage requirements (eigenvectors + a few\nnumbers per face)\n– Faster matching\nWhat are the problems for face recognition?",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Limitations\nGlobal appearance method: not robust to misalignment, background variation",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Limitations\nThe direction of maximum variance is not always good for classification",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "A more discriminative subspace: FLD\nFisher Linear Discriminants  “Fisher Faces”\nPCA preserves maximum variance\nFLD preserves discrimination\n– Find projection that maximizes scatter between classes and minimizes scatter within classes\nReference: Eigenfaces vs. Fisherfaces, Belheumer et al., PAMI 1997",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Comparing with PCA",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Variables\nN Sample images: • c classes:\nAverage of each class:\nAverage of all data:\nNxx,,1\nc,,1\nikxkiixN1\nNkkxN11",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scatter Matrices\nScatter of class i:\nTikxikixxSik\nWithin class scatter:\nciiWSS1\nBetween class scatter:\nciTiiiBNS1",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1S\nBS\nBetween class scatter\nIllustration\n21SSSW\nx2\nWithin class scatter\n2S\nx1",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mathematical Formulation\nAfter projection\nkTkxWy\n– Between class scatter – Within class scatter\nWSWSBTB~\nWSWSWTW~\nObjective:\nWSWWSWSSWWTBTWBoptWWmax arg~~max arg\nSolution: Generalized Eigenvectors\nmiwSwSiWiiB,,1 \nRank of Wopt is limited – Rank(SB) <= |C|-1 – Rank(SW) <= N-C",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Recognition with FLD\nUse PCA to reduce dimensions to N-C\n)pca(XWpca\nCompute within-class and between-class scatter matrices for PCA coefficients\nciiWSS1\nTikxikixxSik\nSolve generalized eigenvector problem\nWSWWSWWWTBTfldWmax arg\nmiwSwSiWiiB,,1 \nProject to FLD subspace (c-1 dimensions) xWxToptˆ\nProject to FLD subspace (c-1 dimensions) xWxToptˆ\n𝑊𝑇\n𝑜𝑝𝑡 = 𝑊𝑇\n𝑓𝑙𝑑𝑊𝑇\n𝑝𝑐𝑎\n• Classify by nearest neighbor\nNote: x in step 2 refers to PCA coef; x in step 4 refers to original data",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results: Eigenface vs. Fisherface\nInput: • Train: • Test:\n160 images of 16 people 159 images 1 image\nVariation in Facial Expression, Eyewear, and Lighting\nWith glasses\nWithout glasses\n3 Lighting conditions\n5 expressions\nReference: Eigenfaces vs. Fisherfaces, Belheumer et al., PAMI 1997",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Eigenfaces vs. Fisherfaces\nReference: Eigenfaces vs. Fisherfaces, Belheumer et al., PAMI 1997",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1997 till today, what has changed?\n2006:\nFace Recognition Vendor Test 2006 http://www.nist.gov/itl/iad/ig/frvt-2006.cfm\nControlled Data",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1997 till today, what has changed?\n2006:\nFace Recognition Vendor Test 2006 http://www.nist.gov/itl/iad/ig/frvt-2006.cfm",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "State-of-the-art Face Recognizers\nMost recent research focuses on “faces in the wild”, recognizing faces in normal photos – Classification: assign identity to face – Verification: say whether two people are the same\nImportant steps\n1. Detect 2. Align 3. Represent 4. Classify",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Labeled Faces in the Wild (LFW)\nhttp://vis-www.cs.umass.edu/lfw/",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Labeled Faces in the Wild (LFW)",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Labeled Faces in the Wild (LFW)",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Labeled Faces in the Wild (LFW)",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification Taigman, Yang, Ranzato, & Wolf (Facebook, Tel Aviv), CVPR 2014\nFollowing slides adapted from Daphne Tsatsoulis",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Face Alignment\n1. Detect a face and 6 fiducial markers using a support vector regressor (SVR)\n2. Iteratively scale, rotate, and translate image until it aligns with a target face\n3. Localize 67 fiducial points in the 2D aligned crop\n4. Create a generic 3D shape model by taking the average of 3D scans from the USF Human-ID database and manually annotate the 67 anchor points 5.Fit an affine 3D-to-2D camera and use it to direct the warping of the face",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Train DNN classifier on aligned faces\nArchitecture (deep neural network classifier) • • 3 locally connected and 2 fully connected layers •\nTwo convolutional layers (with one pooling layer)\n> 120 million parameters\nTrain on dataset with 4400 individuals, ~1000 images each • Train to identify face among set of possible people\nVerification is done by comparing features at last layer for two faces",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results: Labeled Faces in the Wild Dataset\nPerforms similarly to humans! (note: humans would do better with uncropped faces)\nExperiments show that alignment is crucial (0.97 vs 0.88) and that deep features help (0.97 vs. 0.91)",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Face recognition by humans\nFace Recognition by Humans: Nineteen Results\nAll Computer Vision Researchers Should Know About\nBy Pawan Sinha, Benjamin Balas, Yuri Ostrovsky, and Richard\nRussell\nProc. IEEE, 2006\nhttp://web.mit.edu/sinhalab/Papers/19results_sinha_etal.pdf",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Result 4\nFacial features are processed holistically\nWho’s in the picture?\nhard\nEasier",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Things to remember\nPCA is a generally useful dimensionality reduction technique – But not ideal for discrimination\nFLD better for discrimination, though only ideal under Gaussian data assumptions\nComputer face recognition works very well under controlled environments (since 2006)\nAlso starting to perform at human level in uncontrolled settings (recent progress: better alignment, features, more data)",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/19-FaceRecognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "97bc5c6ef56c47678ad3a3e455613438.json": [
      {
        "type": "application/pdf",
        "text": "Image Features and Categorization\nSlides borrowed from Derek Hoiem",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today: Image features and categorization\nGeneral concepts of categorization – Why? What? How?\nImage features\n– Color, texture, gradient, shape, interest points – Histograms, feature encoding, and pooling – CNN as feature\nImage and region categorization",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What do you see in this image?\nTrees\nBear\nCamera\nMan\nCan I put stuff in it?\nRabbit\nGrass\nForest",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Describe, predict, or interact with the object based on visual cues\nIs it dangerous?\nHow fast does it run?\nIs it alive?\nIs it soft?\nDoes it have a tail?\nCan I poke with it?",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Why do we care about categories? • From an object’s category, we can make predictions about its\nbehavior in the future, beyond of what is immediately perceived.\nPointers to knowledge\n– Help to understand individual cases not previously encountered\nCommunication",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Theory of categorization\nHow do we determine if something is a member of a particular category?\nDefinitional approach\nPrototype approach\nExemplar approach",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Definitional approach: classical view of categories\nPlato & Aristotle\n– Categories are defined by a list of\nproperties shared by all elements in a category\n– Category membership is binary – Every member in the category is\nequal\nThe Categories (Aristotle) : https://en.wikipedia.org/wiki/Categori es_(Aristotle)\nAristotle by Francesco Hayez\nSlide Credit: A. A. Efros",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Prototype or sum of exemplars ?\nPrototype Model\nExemplars Model\nCategory judgments are made by comparing a new exemplar to the prototype.\nCategory judgments are made by\ncomparing a new exemplar to all the old exemplars of a category or to the exemplar that is the most appropriate\nSlide Credit: Torralba",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Levels of categorization [Rosch 70s]\nDefinition of Basic Level:\nSimilar shape: Basic level categories are the highest-level category for which their members have similar shapes.\nSimilar motor interactions: … for which people interact with its members using similar motor sequences.\n…\nCommon attributes: … there are a significant number of attributes in common between pairs of members.\nSuperordinate levels\n…\nanimal\nsimilarity\nquadruped\nBasic level\ndog\ncat\ncow\nSub Basic Superordinate\nGerman shepherd\nDoberman\nRosch et a. Principle of categorization, 1978\nSubordinate level\n…\n“Fido”\n…\n…\n…",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nCat vs Dog",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nObject recognition\nCaltech 101 Average Object Images",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nFine-grained recognition\nVisipedia Project",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nPlace recognition\nPlaces Database [Zhou et al. NIPS 2014]",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nVisual font recognition\n[Chen et al. CVPR 2014]",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nDating historical photos\n1940\n1953\n1966\n[Palermo et al. ECCV 2012]\n1977",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization\nImage style recognition\n[Karayev et al. BMVC 2014]",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Region categorization\nLayout prediction\nAssign regions to orientation Geometric context [Hoiem et al. IJCV 2007]\nAssign regions to depth Make3D [Saxena et al. PAMI 2008]",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Region categorization\nSemantic segmentation from RGBD images\n[Silberman et al. ECCV 2012]",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Region categorization\nMaterial recognition\n[Bell et al. CVPR 2015]",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Supervised learning\nLAB Histogram\nTextons\nHOG\nBag of SIFT\nExamples\n+\nImage Features\n+\nxx\nx\nx\nx\no\no\no\no\no\nx\nx\nx\nx\nClassifier\n= Category label",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Training phase\nTraining Images\nTraining\nImage Features\nTraining Labels\nClassifier Training\nTrained Classifier",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Testing phase\nTraining Images\nTest Image\nTraining\nImage Features\nTesting\nImage Features\nTraining Labels\nClassifier Training\nTrained Classifier\nTrained Classifier\nPrediction\nOutdoor",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Testing phase\nTraining Images\nTest Image\nTraining\nImage Features\nTesting\nImage Features\nTraining Labels\nClassifier Training\nTrained Classifier\nTrained Classifier\nPrediction\nOutdoor",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Q: What are good features for…\nrecognizing a beach?",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Q: What are good features for…\nrecognizing cloth fabric?",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Q: What are good features for…\nrecognizing a mug?",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What are the right features? Depends on what you want to know!\nObject: shape\n– Local shape info, shading, shadows, texture\nScene : geometric layout\n– linear perspective, gradients, line segments\nMaterial properties: albedo, feel, hardness – Color, texture\nAction: motion\n– Optical flow, tracked points",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "General principles of representation\nCoverage\n– Ensure that all relevant info is captured\nConcision\n– Minimize number of features without sacrificing\ncoverage\nDirectness\n– Ideal features are independently useful for prediction",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image representations\nTemplates\n– Intensity, gradients, etc.\nHistograms\n– Color, texture, SIFT descriptors,\netc.\nAverage of features\nImage Intensity\nGradient template",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image representations: histograms\nGlobal histogram - Represent distribution of features\n– Color, texture, depth, …\nSpace Shuttle Cargo Bay\nImages from Dave Kauchak",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image representations: histograms\nData samples in 2D\n2\ne r u a e F\nt\nFeature 1",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image representations: histograms\nProbability or count of data in each bin • Marginal histogram on feature 1\n2\ne r u a e F\nt\nFeature 1\nbin",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image representations: histograms\nMarginal histogram on feature 2\n2\ne r u a e F\nt\nFeature 1\nbin",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image representations: histograms\nJoint histogram\n2\ne r u a e F\nt\nFeature 1\nbin",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2 e r u\nt\na e F\nModeling multi-dimensional data\n2 e r u\nt\na e F\nFeature 1\nFeature 1\n2 e r u a e F\nt\nFeature 1\nJoint histogram • •\nRequires lots of data Loss of resolution to avoid empty bins\nMarginal histogram\nRequires independent features • More data/bin than joint histogram",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Modeling multi-dimensional data\nClustering • Use the same cluster centers for all images\n2\ne r u a e F\nt\nFeature 1\nbin",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Computing histogram distance\nHistogram intersection\nKmjijimhmhhh1)(),(min1),histint(\nChi-squared Histogram matching distance\nKmjijijimhmhmhmhhh122)()()]()([21),(\nEarth mover’s distance\n(Cross-bin similarity measure) – minimal cost paid to transform one distribution into the\nother\n[Rubner et al. The Earth Mover's Distance as a Metric for Image Retrieval, IJCV 2000]",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Histograms: implementation issues\nQuantization\n– Grids: fast but applicable only with few dimensions – Clustering: slower but can quantize data in higher\ndimensions\nFew Bins Need less data Coarser representation\nMany Bins Need more data Finer representation\nMatching\n– Histogram intersection or Euclidean may be faster – Chi-squared often works better – Earth mover’s distance is good for when nearby bins\nrepresent similar values",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What kind of things do we compute histograms of?\nColor\nL*a*b* color space\nHSV color space\nTexture (filter banks or HOG over regions)",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What kind of things do we compute histograms of? • Histograms of descriptors\nSIFT – [Lowe IJCV 2004]\n“Bag of visual words”",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Analogy to documents\nOf all the sensory impressions proceeding to the brain, the visual experiences are the dominant ones. Our perception of the world around us is based essentially on the messages that reach the brain from our eyes. For a long time it was thought that the retinal sensory, brain, image was transmitted point by point to visual centers in the brain; the cerebral cortex was a visual, perception, movie screen, so to speak, upon which the retinal, cerebral cortex, image in the eye was projected. Through the eye, cell, optical discoveries of Hubel and Wiesel we now know that behind the origin of the visual nerve, image perception in the brain there is a considerably Hubel, Wiesel more complicated course of events. By following the visual impulses along their path to the various cell layers of the optical cortex, Hubel and Wiesel have been able to demonstrate that the message about the image falling on the retina undergoes a step- wise analysis in a system of nerve cells stored in columns. In",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "this system each cell has its specific function and is responsible for a specific detail in the pattern of the retinal image.\nChina is forecasting a trade surplus of $90bn (£51bn) to $100bn this year, a threefold increase on 2004's $32bn. The Commerce Ministry said the surplus would be created by a predicted 30% jump in exports to $750bn, compared with a 18% rise in imports to China, trade, $660bn. The figures are likely to further annoy the US, which has long argued that surplus, commerce, China's exports are unfairly helped by a exports, imports, US, deliberately undervalued yuan. Beijing yuan, bank, domestic, agrees the surplus is too high, but says the yuan is only one factor. Bank of China foreign, increase, governor Zhou Xiaochuan said the country trade, value also needed to do more to boost domestic demand so more goods stayed within the country. China increased the value of the yuan against the dollar by 2.1% in July and permitted it to trade within a narrow band, but the US wants the yuan to be allowed to trade freely. However, Beijing has made it clear that it will take its time and tread carefully",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "before allowing the yuan to rise further in value.\nICCV 2005 short course, L. Fei-Fei",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Bag of visual words\nImage patches\nBoW\nhistogram\nCodewords",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image categorization with bag of words\nTraining 1. 2. 3. Quantize descriptors using cluster centers to get “visual words” 4. 5.\nExtract keypoints and descriptors for all training images Cluster descriptors\nRepresent each image by normalized counts of “visual words” Train classifier on labeled examples using histogram values as features\nTesting 1. 2. 3.\nExtract keypoints/descriptors and quantize into visual words Compute visual word histogram Compute label or confidence using classifier",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Bag of visual words image classification\n[Chatfieldet al. BMVC 2011]",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature encoding\nHard/soft assignment to clusters\nHistogram encoding (hard)\nKernel codebook encoding (soft)\nLocality constrained encoding\nFisher encoding (1st+2nd statistics)\n[Chatfieldet al. BMVC 2011]",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "BOA vs VLAD vs Fisher",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "sv",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "BOA vs VLAD vs Fisher",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "sv",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "BOA vs VLAD vs Fisher",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "sv",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fisher vector encoding\nFit Gaussian Mixture Models\nPosterior probability\nFirst and second order differences to cluster k\n[Perronnin et al. ECCV 2010]",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Performance comparisons\nFisher vector encoding outperforms others • Higher-order statistics helps\n[Chatfieldet al. BMVC 2011]",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "But what about spatial layout?\nAll of these images have the same color histogram",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Spatial pyramid pooling\nCompute histogram in each spatial bin",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Spatial pyramid pooling\nHigh number of features – PCA to reduce dimensionality\n[Lazebnik et al. CVPR 2006]",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pooling\nAverage/max pooling\n=avg/max\nSecond-order pooling [Joao et al. PAMI 2014]\n=avg/max\nSource: Unsupervised Feature Learning and Deep Learning",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2012 ImageNet 1K\n(Fall 2012)\n40\n35\n30\n25\nr o r r E\n20\n15\n10\n5\n0",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "cy",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2012 ImageNet 1K\n(Fall 2012)\n40\n35\n30\n25\nr o r r E\n20\n15\n10\n5\n0",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "cy",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Shallow vs. deep learning\nEngineered vs. learned features\nLabel\nClassifier\nPooling\nFeature extraction\nImage\nLabel\nDense\nDense\nDense\nConvolution\nConvolution\nConvolution\nConvolution\nConvolution\nImage",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gradient-Based Learning Applied to Document Recognition, LeCun, Bottou, Bengio and Haffner, Proc. of the IEEE, 1998\nImagenet Classification with Deep Convolutional Neural Networks, Krizhevsky, Sutskever, and Hinton, NIPS 2012\nSlide Credit: L. Zitnick",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gradient-Based Learning Applied to Document Recognition, LeCun, Bottou, Bengio and Haffner, Proc. of the IEEE, 1998\nImagenet Classification with Deep Convolutional Neural Networks, Krizhevsky, Sutskever, and Hinton, NIPS 2012\n* Rectified activations and dropout Slide Credit: L. Zitnick",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Convolutional activation features\n. This figure shows several t-SNE feature visualizations on the ILSVRC-2012 validation set. (a) LLC , (b) GIST, and features derived from our CNN: (c) DeCAF1, the first pooling layer, and (d) DeCAF6, the second to last hidden layer (best viewed in color).\n[Donahue et al. ICML 2013]",
        "offset": "page60",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Convolutional activation features\nCNN Features off-the-shelf: an Astounding Baseline for Recognition [Razavian et al. 2014]",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Things to remember\nVisual categorization help transfer knowledge\nImage features\n– Coverage, concision, directness – Color, gradients, textures, motion, descriptors – Histogram, feature encoding, and pooling – CNN as features\nImage/region categorization",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/20-ImageCategorization.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "b3cee5d7696d4bc2a713007c4ea61db0.json": [
      {
        "type": "application/pdf",
        "text": "1\nImage segmentation – part 2\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Graph theoretic segmentation\n Normalized cuts\n Segmentation as energy minimization\n Markov random fields\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Images as graphs\n Node for every pixel  Edge between every pair of pixels (or every pair of\n“sufficiently close” pixels)\n Each edge is weighted by the affinity or similarity of the\ntwo nodes\nj\nwij\ni\n3\nLihi Zelnik-Manor, Computer Vision\nSource: S. Seitz",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation by graph partitioning\n Break Graph into Segments\n Delete links that cross between segments  Easiest to break links that have low affinity  similar pixels should be in the same segments  dissimilar pixels should be in different segments\nwij i\nA\nB\nC\n4\nLihi Zelnik-Manor, Computer Vision\nj\nSource: S. Seitz",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Measuring affinity\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scale affects affinity\n Small σ: group only nearby points  Large σ: group far-away points\n6\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Graph cut\n Set of edges whose removal makes a graph disconnected\n Cost of a cut: sum of weights of cut edges\n A graph cut gives us a segmentation\n What is a “good” graph cut and how do we find one?\nA\nB\n7\nLihi Zelnik-Manor, Computer Vision\nSource: S. Seitz",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Minimum cut\nWe can segment by finding the minimum cut in a graph\n\nEfficient algorithms exist for doing this\nMinimum cut example\n8\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Minimum cut\nWe can segment by finding the minimum cut in a graph\n\nEfficient algorithms exist for doing this\nMinimum cut example\nThe cut is defined by the block diagonal structure of the affinity matrix. • Can this be generalized?\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Minimum cut drawback\n Minimum cut tends to cut off very small, isolated\ncomponents\nBetter Cut\n10\nLihi Zelnik-Manor, Computer Vision\nCuts with lesser weight than the ideal cut",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Normalized cut (Ncut)\n The drawback of mincut can be fixed by normalizing the\ncost by the weight of all the edges incident to the segment\n The normalized cut cost is:\n(,)(,)(,)(,)(,)cutABcutABNcutABassocAVassocBV\n(,)cutAB\n= sum of weights of all edges between A and B\n(,)assocAB\n= sum of weights of all edges in V that touch A\n,,11(,)(,)pqpqpAqBNcutABcutABww\nJ. Shi and J. Malik. Normalized cuts and image segmentation. PAMI 2000\n11\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Ncut as a generalized eigenvector problem\nLet W be the adjacency matrix of the graph • Let D be the diagonal matrix with diagonal entries D(i, i) = Σj W(i, j)\nThen the normalized cut cost can be written as ()(,)TTyDWyNcutAByDy\nThen the normalized cut cost can be written as ()(,)TTyDWyNcutAByDy\n1()pAypnegativeotherwise\nJ. Shi and J. Malik. Normalized cuts and image segmentation. PAMI 2000\n12",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Ncut as a generalized eigenvector problem\n Problem:\nFinding the exact minimum of the normalized cut cost is NP-complete (because y is discrete)\n Solution:\n Relax y to take on arbitrary values, then solved by a generalized\neigenvalue problem\n()NDWyDy\n The solution y is given by the eigenvector corresponding to the\nsecond smallest eigenvalue\n Continuous results need to be converted into discrete\n13\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Normalized cut algorithm\n1. Construct a weighted graph G = (V,E) from an image 2. Connect each pair of pixels, and assign weights w(i,j) = prob(i,j belong to same region)\n3. Compute diagonal matrix D(i, i) = Σj W(i, j)\n4.\nSolve (D − W)y = λDy for the eigenvector with the second smallest eigenvalue\n5. Threshold eigenvector to get a discrete cut 6. Recursively partition the segmented parts, if necessary\n14\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example results\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example results\n16\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results: Berkeley Segmentation Engine\nhttp://www.cs.berkeley.edu/~fowlkes/BSE/\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Normalized cuts: Pros and cons\nPros  Generic framework, can be used with many different\nfeatures and affinity formulations\n No model or data distribution\nCons\n High storage requirement and time complexity\n\nBias towards partitioning into equal segments\n18\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Eigenvectors carry contour information\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Avoiding Ncut drawback\n Do not try to find regions from the eigenvectors\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Avoiding Ncut drawback\n Do not try to find regions from the eigenvectors  Key idea:\n Reshape eigenvectors into images  Compute edge probability 𝑃𝑏 on eigenvector images  Final edge probability is the sum of all responses\n21\nLihi Zelnik-Manor, Computer Vision\neigenvectors\nedges",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example results\nFinal Pb\nPb\nAfter threshold continuous\n22\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Contour detection ~2008 (color)\nGlobal\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Graph theoretic segmentation\n Normalized cuts\n Segmentation as energy minimization\n Markov random fields\n24\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation as energy minimization\nP(foreground | image)\nNormalizing constant called “partition function”\nedgesjijiNiidatayyfdatayfZdataP,2..11),;,(),;(1),;(y\nLabels to be predicted\nIndividual predictions\nPairwise predictions",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Writing Likelihood as an “Energy”\nedgesjijiNiidatayypdataypZdataP,2..11),;,(),;(1),;(y\nlog\nedgesjijiiidatayydataydataEnergy,21),;,(),;(),;(y\nCost of assignment yi\nCost of pairwise assignment yi ,yj",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Notes on energy-based formulation\nedgesjijiiidatayydataydataEnergy,21),;,(),;(),;(y\n Primarily used when you only care about the most likely\nsolution (not the confidences)\n Can think of it as a general cost function\n Can have larger “cliques” than 2\n Clique is the set of variables that go into a potential function",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Markov Random Fields\nNode yi: pixel label\nEdge: constrained pairs\nCost to assign a label to each pixel\nCost to assign a pair of labels to connected pixels\nedgesjijiiidatayydataydataEnergy,21),;,(),;(),;(y",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Markov Random Fields  Example: “label smoothing” grid\nUnary potential 0: -logP(yi = 0 | data) 1: -logP(yi = 1 | data)\nPairwise Potential 0 1 0 0 K 1 K 0\nK>0\nedgesjijiiidatayydataydataEnergy,21),;,(),;(),;(y",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Solving MRFs with graph cuts\nSource (Label 0)\nCost to assign to 1\nCost to split nodes\nCost to assign to 0\nSink (Label 1)\nedgesjijiiidatayydataydataEnergy,21),;,(),;(),;(y",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Solving MRFs with graph cuts\nSource (Label 0)\nCost to assign to 1\nCost to split nodes\nCost to assign to 0\nSink (Label 1)\nedgesjijiiidatayydataydataEnergy,21),;,(),;(),;(y",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "GrabCut segmentation\nUser provides rough indication of foreground region.\nGoal: Automatically provide a pixel-level segmentation.",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "GrabCut\n Convert MRF into source-sink graph\nSink\nSource\nPairwise potentials\nSingle-node potentials\n Minimun cost can be computed in polynomial time\n[Kwatra et al., SIGGRAPH 2003]\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Graph cuts\nBoykov and Jolly (2001)\nImage\nForeground (source)\nBackground (sink)\nCut: separating source and sink; Energy: collection of edges\nMin Cut: Global minimal energy in polynomial time\nMin Cut\nSource: Rother",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Binary segmentation as energy minimization\n Define a labeling L as an assignment of each pixel with\na 0-1 label (background or foreground)\n Problem statement: find the labeling L that minimizes\n{ { smoothness cost match cost (how similar is each pixel to the foreground / background?)",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": ": “distance” from pixel to foreground pixels { usually computed by\n: “distance” from pixel to background pixels\ncreating a color model from user-labeled pixels",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Colour Model\nR\nForeground & Background\nIterated graph cut\nR\nForeground\nBackground\nG\nBackground\nGaussian Mixture Model (typically 5-8 components)\nG\nSource: Rother",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": " Neighboring pixels should generally have the same labels\n Unless the pixels have very different intensities\n: similarity in intensity of p and q\n= 10.0\n= 0.1",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Binary segmentation as energy minimization\n For this problem, we can easily find the global minimum!\n Use max flow / min cut algorithm",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Graph min cut problem\n Given a weighted graph G with source and sink nodes (s\nand t), partition the nodes into two sets, S and T such that the sum of edge weights spanning the partition is minimized  and s\nS and t T",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation by min cut\nmin cut\nt (“background”)\ns (“foreground”)\n Graph\n node for each pixel, link between adjacent pixels  specify a few pixels as foreground and background\n create an infinite cost link from each bg pixel to the t node  create an infinite cost link from each fg pixel to the s node  create finite cost links from s and t to each other node\n compute min cut that separates s from t\n The min-cut max-flow theorem [Ford and Fulkerson 1956]",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Segmentation by min cut\nmin cut\nt\ns\n The partitions S and T formed by the min cut give the\noptimal foreground and background segmentation\n I.e., the resulting labels minimize",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "GrabCut segmentation 1. Define graph\n\n\nusually 4-connected or 8-connected Divide diagonal potentials by sqrt(2)\n2. Define unary potentials\n Color histogram or mixture of Gaussians for background\n));(());((log)(_backgroundforegroundxcPxcPxpotentialunary\nand foreground\n3. Define pairwise potentials\n22212)()(exp),(_ycxckkyxpotentialedge\n4. Apply graph cuts 5. Return to 2, using current labels to compute foreground, background models",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "GrabCut\nGrabcut [Rother et al., SIGGRAPH 2004]",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "GrabCut\n Implemented in MS office …. Let’s try it\n46\nLihi Zelnik-Manor, Computer Vision\nReported results",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Easier examples",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "More difficult Examples\nCamouflage & Low Contrast\nFine structure\nInitial Rectangle\nInitial Result\nHarder Case",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Graph cuts with multiple labels\n Alpha expansion\nRepeat until no change For 𝛼 = 1. . 𝑀\nAssign each pixel to current label or 𝛼 (2-class graphcut)\n Achieves “strong” local minimum\n Alpha-beta swap\nRepeat until no change\nFor 𝛼 = 1. . 𝑀, 𝛽 = 1. . 𝑀 (except 𝛼)\nRe-assign all pixels currently labeled as 𝛼 or 𝛽 to one of those two labels while keeping all other pixels fixed",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Other application: synthesis\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Using graph cuts for recognition\nTextonBoost (Shotton et al. 2009 IJCV)",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Using graph cuts for recognition\nUnary Potentials from classifier\nAlpha Expansion Graph Cuts\n(note: edge potentials are from input image also; this is illustration from paper)\nTextonBoost (Shotton et al. 2009 IJCV)",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Summary: MRF and graph-cuts\n Pros:\n Powerful, based on probabilistic model (MRF)  Applicable to a wide range of problems  Very efficient algorithms available for many problems  Becoming a standard for segmentation\n Cons\n Graph-cuts can only solve a limited class of problems:\n Sub-modular energy functions  Can only capture part of the power of MRF\n Only approximate solutions available for multi-label case\n53\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Graph cuts: Pros and Cons\n Pros\n Very fast inference  Can incorporate data likelihoods and priors  Applies to a wide range of problems (stereo, image labeling,\nrecognition)\n Cons\n Not always applicable (associative only)  Need unary terms (not used for bottom-up segmentation, for\nexample)\n Use whenever applicable",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Further reading and resources\n Graph cuts\n http://www.cs.cornell.edu/~rdz/graphcuts.html  Classic paper: What Energy Functions can be Minimized via Graph Cuts?\n(Kolmogorov and Zabih, ECCV '02/PAMI '04)\n Belief propagation\nYedidia, J.S.; Freeman, W.T.; Weiss, Y., \"Understanding Belief Propagation and Its Generalizations”, Technical Report, 2001: http://www.merl.com/publications/TR2001-022/",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CNN for affinity learning\n Maire and Yu, 2015\n56\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CNN for affinity learning\n Maire and Yu, 2015\n57\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "End – image segmentation part 2\nNow you know how it works\n58\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/08-Segmentation part2-Normalized-cuts-Markov-random fields.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "b75a4ed9486d4d149a7fec60bec92775.json": [
      {
        "type": "application/pdf",
        "text": "1\nThe pinhole camera\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Next two classes: Single-view Geometry\nHow tall is this woman?\nHow high is the camera?\nWhat is the camera rotation?\nWhat is the focal length of the camera?\nWhich ball is closer?",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Pinhole cameras  Cameras & lenses  The geometry of pinhole cameras  Other camera models\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Pinhole cameras  Cameras & lenses  The geometry of pinhole cameras  Other camera models\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How do we see the world?\nLet’s design a camera\n– Idea 1: put a piece of film in front of an object – Do we get a reasonable image?\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pinhole camera\nAdd a barrier to block off most of the rays\n– This reduces blurring – The opening is known as the aperture\n6\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pinhole camera\nf\nc\nf = focal length c = center of the camera\nFigure from Forsyth",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Historical context\nPinhole model: Mozi (470-390 BCE), Aristotle (384-322 BCE)\nPrinciples of optics (including lenses): Alhacen (965-1039 CE)\nCamera obscura: Leonardo da Vinci (1452-1519), Johann Zahn (1631-1707)\nFirst photo: Joseph Nicephore Niepce (1822)\nDaguerréotypes (1839)\nPhotographic film (Eastman, 1889)\nCinema (Lumière Brothers, 1895)\nColor Photography (Lumière Brothers, 1908)\nTelevision (Baird, Farnsworth, Zworykin, 1920s)\nFirst consumer camera with CCD: Sony Mavica (1981)\nFirst fully digital camera: Kodak DCS100 (1990)\n8\nLihi Zelnik-Manor, Computer Vision\nAlhacen’s notes\nNiepce, “La Table Servie,” 1822\nCCD chip",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Camera obscura\nIn Latin, means ‘dark room’\n\"Reinerus Gemma-Frisius, observed an eclipse of the sun at Louvain on January 24, 1544, and later he used this illustration of the event in his book De Radio Astronomica et Geometrica, 1545. It is thought to be the first published illustration of a camera obscura...\" Hammond, John H., The Camera Obscura, A Chronicle\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Camera obscura\nJetty at Margate England, 1898.\nAn attraction in the late 19th century\n10\nLihi Zelnik-Manor, Computer Vision\nAround 1870s",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Camera obscura at home\nhttp://blog.makezine.com/archive/2006/02/how_to_room_ sized_camera_obscu.html\n11\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Dimensionality Reduction Machine (3D to 2D)\n3D world\n2D image\nPoint of observation\nFigures © Stephen E. Palmer, 2002",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection can be tricky…\nSlide source: Seitz",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection can be tricky…\nMaking of 3D sidewalk art: http://www.youtube.com/watch?v=3SNYtd0Ayt0\nSlide source: Seitz",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective Geometry\nWhat is lost?  Length\nWho is taller?\nWhich is closer?",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Length is not preserved\nA’\nC’\nB’\nFigure by David Forsyth",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective Geometry\nWhat is lost?  Length  Angles\nPerpendicular?\nParallel?",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective Geometry\nWhat is preserved?  Straight lines are still straight",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing points and lines\nParallel lines in the world intersect in the image at a “vanishing point”",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing points and lines\nVanishing Line\nVanishing Point o\nVanishing Point o\nThe projections of parallel 3D lines intersect at a vanishing point • The projection of parallel 3D planes intersect at a vanishing line • If a set of parallel 3D lines are also parallel to a particular plane, their vanishing point will lie on the vanishing line of the plane\nNot all lines that intersect are parallel • Vanishing point <-> 3D direction of a line • Vanishing line <-> 3D orientation of a surface",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing points and lines\nVanishing line\nVanishing point\nSlide from Efros, Photo from Criminisi\nVertical vanishing point (at infinity)\nVanishing point",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing points and lines\nPhoto from online Tate collection",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Note on estimating vanishing points\nUse multiple lines for better accuracy\n… but lines will not intersect at exactly the same point in practice\nOne solution: take mean of intersecting pairs\n… bad idea!\nInstead, minimize angular differences",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Vanishing objects",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection equations of ideal Pinhole\n 3d world mapped to 2d projection in image plane\nImage plane\nFocal length\nScene point\n𝑓\nCamera frame\n𝑃 =\nOptical axis\n𝑝 =\n𝑥 𝑦\n𝑃 =\n𝑋 𝑌 𝑍\n𝑝 =\n𝑥 𝑦\n25\nLihi Zelnik-Manor, Computer Vision\n𝑋 𝑌 𝑍",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection equations of ideal Pinhole\n 3d world mapped to 2d projection in image plane\n𝑓\n𝑓\n𝑋\n𝑃 =\n𝑥\n𝑍\n𝑝 =\n𝑥 𝑦\n𝑥\n𝑓\n=\n𝑋\n𝑍\n\n𝑥\n= 𝑓\n26\nLihi Zelnik-Manor, Computer Vision\n𝑋 𝑌 𝑍\n𝑋\n𝑍",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection equations of ideal Pinhole\n 3d world mapped to 2d projection in image plane\n𝑓\n𝑝 =\n𝑥 𝑦\n𝑃 =\n𝑋 𝑌 𝑍\n𝑝 =\n𝑥 𝑦\n𝑥 = 𝑓\n𝑦 = 𝑓\n𝑋 𝑍 𝑌 𝑍\n27\nLihi Zelnik-Manor, Computer Vision\n𝑃 =\n𝑋 𝑌 𝑍",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pinhole camera\n It is common to draw the image plane in front of the\nfocal point\n Moving the image plane merely scales the image\nImage plane\npinhole\nVirtual image\n28\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "When the camera is not ideal\nHow does the size of the aperture affect the image?\n29\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pinhole size / aperture\ne r u t r e p a\nk n i r h S\nProblems with small aperture:\nLess light goes through\nDiffraction effect\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Adding a lens\n A lens focuses light onto the film\n31\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Adding a lens\n A lens focuses light onto the film\n More lights goes through the center than through the boundaries\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Adding a lens - focus\nfocal point\n𝑓\n A lens focuses light onto the film\n Rays passing through the center are not deviated  All parallel rays converge to one point on a plane located at the focal\nlength f\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Adding a lens - focus\n A lens focuses light onto the film\n There is a specific distance at which objects are “in focus”\n34\nLihi Zelnik-Manor, Computer Vision\nBlurred circle",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "A lens with aperture\n1. Blurred\n2. In focus\n3. Blurred\n35\nLihi Zelnik-Manor, Computer Vision\nNo aperture\nSmall aperture\nImages from Wikipedia\nhttp://en.wikipedia.org/wiki/Depth_of_field",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "A lens with aperture\nA smaller aperture increases the range in which the object is approximately in focus\n36\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Depth from focus\n37\nLihi Zelnik-Manor, Computer Vision\nImages from same point of view, different aperture\n3d shape / depth estimates\n[figs from H. Jin and P. Favaro, 2002]",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Depth from defocus\nY. Frommer, R. Ben-Ari and N. Kiryati Shape from Focus with Adaptive Focus Measure and High Order Derivatives, BMVC 2015\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Light passing through a lens  Optic laws\n Light travels in straight lines in homogeneous medium  Reflection:\nincoming ray, surface normal, and reflected ray are co-planar  Refraction: when a ray passes from one medium to another\n Snell’s law\n𝑛1 sin 𝛼1 = 𝑛2 sin 𝛼2\n𝛼1 = incident angle 𝛼1 = refraction angle 𝑛1,2 = index of refraction\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Thin lens\nThin lens\n𝑍0\n𝑌\n𝑦\n𝑍′\n−𝑍\nSnell’s law 𝑛1 sin 𝛼1 = 𝑛2 sin 𝛼2\nSmall angles 𝑛1𝛼1 ≈ 𝑛2𝛼2\n𝑛1 = 1 (𝑎𝑖𝑟) 𝑛2 = 𝑛 (𝑙𝑒𝑛𝑠)\n40\nLihi Zelnik-Manor, Computer Vision\n𝑍′ = 𝑓 + 𝑍0 𝑅𝑎𝑑𝑖𝑢𝑠 2(𝑛 − 1)\n𝑓 =\n𝑥 = 𝑍′ 𝑋 𝑍 𝑦 = 𝑍′ 𝑌 𝑍",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Pinhole cameras  Cameras & lenses  The geometry of pinhole cameras  Other camera models\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Cameras and lenses\n42\nLihi Zelnik-Manor, Computer Vision\nSource wikipedia",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Issues with lenses: Chromatic aberration\n A lens has different refractive indices for different\nwavelength: causes color fringing\n𝑓 =\n𝑅𝑎𝑑𝑖𝑢𝑠 2(𝑛 − 1)\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Issues with lenses: Chromatic aberration\n Rays farther from the optical axis focus closer\n44\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Issues with lenses: Chromatic aberration\n Deviations are most noticeable for rays that pass through\nthe edge of the lens\nNo distortion\nPin cushion\nFisheye\n45\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Issues with lenses: vignetting\n A lens focuses light onto the film\n More lights goes through the center than through the boundaries\n46\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Pinhole cameras  Cameras & lenses  The geometry of pinhole cameras  Other camera models\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projection: world coordinatesimage coordinates\nFocal length 𝑓\nOptical Center (cx, cy)\n.\nf\n.\nZ\nY\n.\nX\n.\nx\ny\nyxp\nCamera Center (tx, ty, tz)\n𝑋 𝑌 𝑍\nZYXP\n𝑥 = 𝑓\n𝑦 = 𝑓\n𝑋 𝑍 𝑌 𝑍",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Pinhole camera\n Is this a linear transformation?\n𝑋 𝑌 𝑍\n𝑥 = 𝑓\n𝑦 = 𝑓\n𝑋 𝑍 𝑌 𝑍\n No – division by Z is not linear!\n How can we make it linear?\n49\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Homogeneous coordinates\nhomogeneous image coordinates\nhomogeneous scene coordinates\nConverting from homogeneous coordinates\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Using homogeneous coordinates\n Projection is a matrix multiplication using homogeneous\ncoordinates:\n00000000101XffXYffYZZ\n𝑀𝑃 = 𝑝\nProjection matrix\n'fXZpfYZ\n51\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From world to image coordinates\n𝑓\nImage coordinate system is not always aligned with optical axis\n'xyfXcZpfYcZ\n52\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From world to image coordinates\n𝑓\nPixels scale could differ from metric measurements\n'xyfXkcZpfYkcZ\n53\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From world to image coordinates\n𝑓\nPixels could be non-square\n'xxyyXXkfccZZpYYkfccZZ\n54\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From world to image coordinates\n𝑓\nCamera axes could be not-orthogonal\n'xyXsYcZZpYcZ\n55\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From world to image coordinates\nWe can write this in matrix form\n0'0000101xxyyXsYcXZZscYYpccZZZ\n56\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The calibration matrix\n0'0000101xyXscYpcZ\n'0pMPKIP\nK\nThis matrix includes 5 camera parameters and is called: • Calibration matrix • Camera matrix\n57\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From world to image coordinates\n𝑓\nP\n'p\n'0pMPKIP\nSo far the world coordinate system was aligned with the lens • Can we represent the scene in “world” coordinate system?\n58\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "World coordinates\n𝑓\nP\nwP\n'p\n[]wPRTP\nIn 4D homogeneous coordinates\n'wpMPKRTP\nInternal parameters\nExternal parameters\n59\nLihi Zelnik-Manor, Computer Vision\nworld",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Camera translation\n'wpMPKRTP\n1100010001100001WWWzyxyxZYXtttccyxw\n60\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page60",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "3D Rotation of Points\nRotation around the coordinate axes, counter-clockwise:\nz\ny\nP’\n\nP\n1000cossin0sincos)(cos0sin010sin0cos)(cossin0sincos0001)(zyxRRR\nSlide Credit: Saverese",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Camera translation and rotation\n'wpMPKRTP\n1100001333231232221131211WWWzyxyxZYXtrrrtrrrtrrrccyxw\n62\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective camera equations\n𝑓\nP\n'p\n313333314134'wpMPKRTP\n11 degrees of freedom\n63\nLihi Zelnik-Manor, Computer Vision\nwP\nworld",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Projective camera equations\n𝑓\nP\n'p\n313333314134'wpMPKRTP\nM is defined up to scale! Multiplying M by a scalar won’t change the image\n64\nLihi Zelnik-Manor, Computer Vision\nwP\nworld\n1323'MPMPpMPMP",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Theorem (Faugeras, 1993)\nMKRTKRKTAb\n65\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page65",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Properties of projection\n Points project to points  Straight lines project to straight lines\n66\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page66",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Properties of projection\n Angles are not preserved  Parallel lines meet\n67\nLihi Zelnik-Manor, Computer Vision\nVanishing point",
        "offset": "page67",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Perspective effects\n Far away objects appear smaller",
        "offset": "page68",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Pinhole cameras  Cameras & lenses  The geometry of pinhole cameras  Other camera models\n69\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page69",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Weak perspective projection\n Assumption:\nAll points have the same depth\nImage plane\nWorld points\n00''fxXZfyYZ\n0010000100001/0/1XXYYZfZf\n70\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page70",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Orthographic (affine) projection\n Assumption\nDistance from center of projection to image plane is infinite\n''xXyY\n10000100000111XXYYZ\n71\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page71",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Weak perspective example\nThe kangxi emperor's southern inspection tour (1691-1698) Wang Hui\n72\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page72",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine or perspective?\n Affine\n Simpler math  Accurate enough when object is small and distant  Useful for recognition\n Pinhole\n Used for 3D reconstruction\n73\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page73",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "74\nEnd – Pinhole camera\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page74",
        "ref": "/home/ameer/Kaleidoo/server/uploads/13-PinholeCamera.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "c3de16b5681d44ce91d6eeda231f4c32.json": [
      {
        "type": "application/pdf",
        "text": "CS4670/5670: Intro to Computer Vision\nNoah Snavely\nLecture 1: Images and image filtering\nHybrid Images, Oliva et al., http://cvcl.mit.edu/hybridimage.htm",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CS4670: Computer Vision\nNoah Snavely\nLecture 1: Images and image filtering\nHybrid Images, Oliva et al., http://cvcl.mit.edu/hybridimage.htm",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CS4670: Computer Vision\nNoah Snavely\nLecture 1: Images and image filtering\nHybrid Images, Oliva et al., http://cvcl.mit.edu/hybridimage.htm",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "CS4670: Computer Vision\nNoah Snavely\nLecture 1: Images and image filtering\nHybrid Images, Oliva et al., http://cvcl.mit.edu/hybridimage.htm",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Reading\nSzeliski, Chapter 3.1-3.2",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What is an image?",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What is an image?\nWe’ll focus on these in this class\n(More on this process later)\nDigital Camera\nThe Eye\nSource: A. Efros",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What is an image?\nA grid (matrix) of intensity values\n255 255 255 255 255 255 255 255 255 255 255 255\n255 255 255 255 255 255 255 255 255 255 255 255\n255 255 255\n20\n0 255 255 255 255 255 255 255\n255 255 255\n75\n75\n75 255 255 255 255 255 255\n=\n255 255\n75\n95\n95\n75 255 255 255 255 255 255\n255 255\n96 127 145 175 255 255 255 255 255 255\n255 255 127 145 175 175 175 255 255 255 255 255\n255 255 127 145 200 200 175 175\n95 255 255 255\n255 255 127 145 200 200 175 175\n95\n47 255 255\n255 255 127 145 145 175 127 127\n95\n47 255 255\n255 255\n74 127 127 127\n95\n95\n95\n47 255 255\n255 255 255\n74\n74\n74\n74\n74\n74 255 255 255\n255 255 255 255 255 255 255 255 255 255 255 255\n255 255 255 255 255 255 255 255 255 255 255 255\n(common to use one byte per value: 0 = black, 255 = white)",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "What is an image?\nWe can think of a (grayscale) image as a function, f, from R2 to R: – f (x,y) gives the intensity at position (x,y)\nf (x, y)\nx\ny\n– A digital image is a discrete (sampled, quantized)\nversion of this function",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image transformations\nAs with any function, we can apply operators to an image\ng (x,y) = f (x,y) + 20\ng (x,y) = f (-x,y)\nWe’ll talk about a special kind of operator, convolution (linear filtering)",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Question: Noise reduction • Given a camera and a still scene, how can you\nreduce noise?\nTake lots of images and average them! What’s the next best thing?\nSource: S. Seitz",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Image filtering\nModify the pixels in an image based on some function of a local neighborhood of each pixel\n10\n5\n3\nSome function\n4\n5\n1\n7\n1\n1\n7\nLocal image data\nModified image data\nSource: L. Zhang",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Linear filtering\nOne simple version: linear filtering (cross-correlation, convolution)\n– Replace each pixel by a linear combination (a weighted\nsum) of its neighbors\nThe prescription for the linear combination is called the “kernel” (or “mask”, “filter”)\n10\n5\n3\n0\n0\n0\n4\n6\n1\n0\n0.5 0\n8\n1\n1\n8\n0\n1\n0.5\nLocal image data\nkernel\nModified image data\nSource: L. Zhang",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Cross-correlation\nLet be the image, be the kernel (of size 2k+1 x 2k+1), and be the output image. The cross-correlation operation is defined as:\nCan think of as a “dot product” between local neighborhood and kernel for each pixel\nShort notation:",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Convolution\nSame as cross-correlation, except that the kernel is “flipped” (horizontally and vertically)\nThis is called a convolution operation:\nConvolution is commutative and associative",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Convolution\nAdapted from F. Durand",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "1\n1\n1\n1\n1\n1\n1\n1 *\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nMean filtering\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n90\n90\n90\n90\n90\n0\n0\n0\n0\n0\n90\n90\n90\n90\n90\n0\n90\n90\n90\n90\n90\n90\n90\n90\n90\n0\n0\n0\n0\n0\n0\n=\n0\n90\n90\n90\n90\n90\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n90\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10 20 30 30 30 20 10\n0\n20 40 60 60 60 40 20\n0\n30 60 90 90 90 60 30\n0\n30 50 80 80 90 60 30\n0\n30 50 80 80 90 60 30\n0\n20 30 50 50 60 40 20\n10 20 30 30 30 30 20 10\n10 10 10\n0\n0\n0\n0\n0",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Original\nLinear filters: examples\n0\n0\n0\n0\n1\n0\n0\n0\n0\n=\nIdentical image\nSource: D. Lowe",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Original\nLinear filters: examples\n0\n0\n0\n0\n0\n0\n0\n1\n0\n=\nShifted left By 1 pixel\nSource: D. Lowe",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Linear filters: examples\n1\n1\n1\n1\n1\n1\n1\n1\n1\n=\nOriginal\nBlur (with a mean filter)\nSource: D. Lowe",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Original\nLinear filters: examples\n0\n0\n0\n0\n2\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n=\nSharpening filter (accentuates edges)\nSource: D. Lowe",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Sharpening\nSource: D. Lowe",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Smoothing with box filter revisited\nSource: D. Forsyth",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian Kernel\nSource: C. Rasmussen",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian filters\n= 1 pixel\n= 5 pixels\n= 10 pixels\n= 30 pixels",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mean vs. Gaussian filtering",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Gaussian filter\nRemoves “high-frequency” components from the image (low-pass filter)\nConvolution with self is another Gaussian\n=\n– Convolving twice with Gaussian kernel of width\n= convolving once with kernel of width\nSource: K. Grauman",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Sharpening revisited\nWhat does blurring take away?\n–\n=\noriginal\nsmoothed (5x5)\nLet’s add it back:\n+ α\n=\noriginal\ndetail\ndetail\nsharpened\nSource: S. Lazebnik",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "image\nblurred image\nscaled impulse\nSharpen filter\nGaussian\nunit impulse (identity)\nLaplacian of Gaussian (LoG)",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "unfiltered\nfiltered\nSharpen filter",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "no",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "“Optical” Convolution\nCamera shake\n=\nSource: Fergus, et al. “Removing Camera Shake from a Single Photograph”, SIGGRAPH 2006\nBokeh: Blur in out-of-focus regions of an image.\nSource: http://www.diyphotography.net/diy_create_your_own_bokeh/",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Questions?\nFor next time:\n– Read Szeliski, Chapter 3.1-3.2",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/02-Filtering.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "cf1590c210cf46e3b80928f8cea7a51a.json": [
      {
        "type": "application/pdf",
        "text": "1\nFinding lines – part 2\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Edge detection\n Canny edge detector  Berkeley edge probability\n Line fitting\n Hough transform  (Generalized) Hough transform application  RANSAC\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting a parametric model to data\n# parameters 2 parametric equation\ny = mx + b\n3 r2 = (x-x0)2 + (y-y0)2\nparameters m, b\nr, x0, y0\nImage credit Zhaozheng Yin @ wisc.edu\nImage credit Yuan-Liang Tang on Mathworks\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fitting a parametric model to data\n Design questions:\n What is a good model to represent our data?  Do we plan to fit multiple instances?\n Challenges:\n Which features belong to the model? To which instance?  How many instances are there?  Computational complexity (typically we cannot examine all\npossible models).\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting applications\ndetection of power lines in helicopter navigation systems\nImage credit: Horev et al. SIAM’15\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting applications\nlane detection from car cameras in crashpreventing systems\n6\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting applications\ndetection of long filaments in high-throughput biological imaging\n7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting applications\nSports\n8\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting applications\n“Interactive 3D Architectural Modeling from Unordered Photo Collections” Sinha et al. 2008\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Challenges of line fitting\n10\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Challenges of line fitting\n Which points on which line?  Noisy edge detection:\n Clutter  Missed parts  Points are only approximately\nalong the line\n Large search space.  How many lines are there?\n11\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Voting\n Problem:\n We cannot try all possible models\n Solution by voting:\n Features (points) vote for models they are compatible with  Search for models with lots of votes\nfeature space\nmodel space\n2\nVoting\nr e e m a r a p\nt\nl\ne d o M\nModel parameter 1\n12\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Hough transform\n\n14\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Hough transform\n\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Hough transform\n\n16\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Finding lines with the Hough transform\n Discretize Hough space  Each edge point votes for all possible parameters in Hough\nspace\n Parameters with lots of votes indicate lines in image space\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Polar representation for lines\n\nd\n18\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The Hough-transform algorithm\n\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example\nFor a given point (x0,y0)\nd(q)=x0cosq+y0sinq\ny\nd\nx\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example\nFor a given point (x0,y0)\nd(q)=x0cosq+y0sinq\ny\nd\nx\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example: an image with straight lines\n22\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Properties\n Noise and clutter votes are inconsistent, so will not\naccumulate.\n Can handle occlusions if not all points are present as long as\nmodel gets enough votes.\n Efficient.\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example: a real image\n24\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example: a real image\n25\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Impact of noise on Hough transform\n What difficulties does noise introduce?\n26\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Impact of noise on Hough transform\n Here everything is “noise” but we still see peaks in the\nvote space\n27\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Voting: Practical tips\n Use only trustworthy points\n E.g., edges points with significant gradient magnitude\n(alternatively weight votes)\n Szeliski suggests using edgels instead of points\n Choose a good quantization grid\n Not too coarse – too many lines fall in the same bucket  Not too fine – collinear points vote for different lines\n Smooth the voting (vote also for neighbors)  Non-maxima suppression  Refit line using accumulated votes  Reduce number of parameters, if possible\n28\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Hough transform summary\n Pros\n Can handle occlusions  Some robustness to noise  Can detect multiple lines in a single pass over the image\n Cons\n Clutter can produce spurious peaks in parameter space  Hard to select the right quantization\n29\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Generalized Hough Transform\n Can be extended to other parametric models such as:\ncircles, ellipses, rectangles etc.\n Complexity increases exponentially with the number of\nparameters.\n Can be used to detect complex non-parametric models as\ndescribed in Leibe et al. “Combined object categorization and segmentation with an implicit shape model”.\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Edge detection\n Canny edge detector  Berkeley edge probability\n Line fitting\n Hough transform  RANSAC\n31\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANdom SAmple Consensus [Fischler & Bolles 1981]\n Key ideas:\n Look for “inliers” and use only them  If we fit a model to “outliers” we will not get a good fitting\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC algorithm\nLoop: 1.Randomly select a group of points 2.Fit a model to the selected group 3.Find the inliers of the computed model 4.If number of inliers is large enough re-compute model using only inliers 5.Compute number of inliers of updated model The winner: model with the largest number of inlier\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting with RANSAC\n Input:\nA set of edge points\n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting with RANSAC\n Step 1:\nSelect two points\n35\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting with RANSAC\n Step 2:\nFit a line to the selected points\n36\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting with RANSAC\n Step 3:\nIdentify inliers\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting with RANSAC\n Step 4:\nFit line to inliers\n38\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Line fitting with RANSAC\n Step 5:\nCount number of new inliers\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC – stopping criteria\n\n40\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC – for multiple models?\n How can we use RANSAC to compute multiple models?\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "RANSAC - summary\n Pros\n General method that works well for lots of model fitting\nproblems\n Easy to implement\n Cons\n When the percentage of outliers is high too many iterations\nare needed and failure rate increases\n42\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "43\nEnd – finding lines\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/06-Line Fitting - RANSAC - HOUGH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "d89acb377b5b44728c0dc4ad6ecafe74.json": [
      {
        "type": "application/pdf",
        "text": "1\nFeature descriptors\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local descriptors\n Selecting invariant regions  Feature descriptors:  SIFT and others\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local descriptors\n Selecting invariant regions  Feature descriptors:  SIFT and others\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The naïve descriptor – intensities vector\n The Simplest descriptor is a vector of the intensities\nwithin the patch.\nreshape\nWhat is this going to be invariant to?\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The naïve descriptor – intensities vector\n Disadvantage of the intensities vector 1. Changes significantly with illumination 2. Changes significantly with small shifts in position\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Another naïve descriptor\n Disadvantage of the intensities vector 1. Changes significantly with illumination 2. Changes significantly with small shifts in position\n Solutions\n1. Use gradients instead of intensities 2. Histograms\n0\n6\nLihi Zelnik-Manor, Computer Vision\n2p",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "A good feature descriptor: SIFT\n Scale Invariant Feature Transform  Descriptor computation:\n Divide patch into 4x4 sub-patches: 16 cells  Compute histogram of gradient orientations (8 reference\nangles) for all pixels inside each sub-patch  Resulting descriptor: 4x4x8 = 128 dimensions\nDavid G. Lowe. \"Distinctive image features from scale-invariant keypoints.” IJCV’2004. 7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "SIFT overview\n Extraordinarily robust matching technique\n Can handle changes in viewpoint up to about 60 degree out of plane rotation  Can handle significant changes in illumination\n Sometimes even day vs. night (below)  Fast and efficient—can run in real time  Lots of code available\n\nhttp://people.csail.mit.edu/albert/ladypack/wiki/index.php/Known_implementations_of_SIFT\n8\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Working with SIFT descriptors\n One image yields:\n n 128-dimensional descriptors: each one is a histogram of the gradient orientations within a patch  [n x 128 matrix]\n n scale parameters specifying the size of\neach patch  [n x 1 vector]\n n orientation parameters specifying the\nangle of the patch  [n x 1 vector]\n n 2d points giving positions of the patches\n [n x 2 matrix]\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "SURF descriptor\n Fast approximation of SIFT  Efficient computation by 2D box filters & integral images  6 times faster than SIFT  Equivalent quality for object\nidentification\nGPU implementation available Feature extraction @ 200Hz (detector + descriptor, 640×480 img) http://www.vision.ee.ethz.ch/~surf\n[Bay, ECCV’06], [Cornelis, CVGPU’08]\n10\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Local Descriptors: Shape Context\nCount the number of points inside each bin, e.g.:\nCount = 4\n. . .\nCount = 10\nLog-polar binning: more precision for nearby points, more flexibility for farther points.\nBelongie & Malik, ICCV 2001\nK. Grauman, B. Leibe",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Local Descriptors: Geometric Blur\n~\nExample descriptor\n(Idealized signal)\nBerg & Malik, CVPR 2001\nK. Grauman, B. Leibe\nCompute edges at four orientations\nExtract a patch in each channel\nApply spatially varying blur and sub-sample",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "GLOH\n Gradient Location and Orientation Histogram\n Very similar to SIFT  Log-polar location grid  3 bins in radial direction  8 bins in angular direction  Gradient orientation quantized to 16 bins\n Total dimension\n (2x8+1)*16=272 bins  PCA for dimension reduction\n13\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "More on feature detection/description\nhttp://www.robots.ox.ac.uk/~vgg/research/affine/detectors.html#binaries\n14\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Self-similarity descriptor\n All the descriptors so far captured same appearance  What can we do if the objects have the same shape but\ndifferent appearance?\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Self-similarity descriptor\n16\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Self-similarity descriptor\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Self-similarity descriptor\ntemplate\nresults\n18\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Self-similarity descriptor\ntemplate\nresults\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Self-similarity descriptor\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Advantages of local features  Useful\n It is critical to find distinctive and repeatable local regions for\nmulti‐view matching  Complexity reduction\n Selection of distinctive points reduces number of regions to\nprocess\n Compact description\n Describe images, objects, parts without requiring segmentation;\n Robustness\n To clutter & occlusion  Similar descriptors in spite of moderate view changes, noise,\nblur, etc.\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "22\nEnd – Feature descriptors\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/11-descriptors-SIFT-SURF-GLOH.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "d9d281d5a9d243dfab6ef7a51fe3088f.json": [
      {
        "type": "application/pdf",
        "text": "1\nFrom points to regions\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n2\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris Detector: Properties\n Is it invariant to image scale?\nAll points will be classified as edges\nNo: Not invariant to image scale!\n3\nLihi Zelnik-Manor, Computer Vision\nCorner !",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "From points to regions\n The Harris and Hessian operators define interest points\n Precise localization  High repeatability\n In order to compare (and match) those points, we need\nto compute a descriptor over their local region  How can we define such a region in a scale invariant manner?  How can we detect scale invariant interest region?\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Naïve approach: Exhaustive search\n Multi-scale procedure\n Compare descriptors while varying patch size\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Naïve approach: Exhaustive search\n Multi-scale procedure\n Compare descriptors while varying patch size\n6\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Naïve approach: Exhaustive search\n Multi-scale procedure\n Compare descriptors while varying patch size\n7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Naïve approach: Exhaustive search\n Multi-scale procedure\n Compare descriptors while varying patch size\n=\n8\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Naïve approach: Exhaustive search\n Compare descriptors while varying patch size\n Computationally inefficient  Prohibitive for retrieval in large databases  Prohibitive for recognition\n=\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic scale selection\n Solution:\n Design a function on the region, which is “scale invariant” (the same for\ncorresponding regions, even if they are at different scales)\nExample: average intensity. For corresponding regions (even of different sizes) it will be the same.  For a point in one image, we can consider it as a function of region size\n(patch width)\nf\nImage 1\nf\nscale = 1/2\nregion size\n10\nLihi Zelnik-Manor, Computer Vision\nImage 2\nregion size",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "f\nAutomatic scale selection\n Common approach:\n Take a local maximum of this function  Observation: region size, for which the maximum is achieved, should be\ninvariant to image scale.\nImportant: this scale invariant region size is found in each image independently!\nImage 1\nf\nscale = 1/2\ns1\nregion size\ns2\n11\nLihi Zelnik-Manor, Computer Vision\nImage 2\nregion size",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Function responses for increasing scale (scale signature)\n13\n)),((1xIfmii\nLihi Zelnik-Manor, Computer Vision\n)),((1xIfmii",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Function responses for increasing scale (scale signature)\n14\n)),((1xIfmii\nLihi Zelnik-Manor, Computer Vision\n)),((1xIfmii",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Function responses for increasing scale (scale signature)\n15\n)),((1xIfmii\nLihi Zelnik-Manor, Computer Vision\n)),((1xIfmii",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Function responses for increasing scale (scale signature)\n16\n)),((1xIfmii\nLihi Zelnik-Manor, Computer Vision\n)),((1xIfmii",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Function responses for increasing scale (scale signature)\n17\n)),((1xIfmii\nLihi Zelnik-Manor, Computer Vision\n)),((1xIfmii",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Function responses for increasing scale (scale signature)\n18\n)),((1xIfmii\nLihi Zelnik-Manor, Computer Vision\n)),((1xIfmii",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Automatic Scale Selection\n Normalize: rescale selected regions to a fixed size\n)),((1xIfmii\n)),((1xIfmii\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "A useful scale “signature” function\n Laplacian-of-Gaussian = “blob” detector\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris – Laplace [Mikolajczyk’01]\n1. Keypoint detection:\n Multi-scale Harris corner detection\n2. Scale selection\n Scale selection based on Laplacian signature\nHarris points\nHarris-Laplace points\n21\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris – Laplace [Mikolajczyk’01]\n1. Keypoint detection:\n Multi-scale Harris corner detection\n2. Scale selection\n Scale selection based on Laplacian signature\n22\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Harris – Laplace [Mikolajczyk’01]\n1. Keypoint detection:\n Multi-scale Harris corner detection\n2. Scale selection\n Scale selection based on Laplacian signature\n23\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n24\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Characteristic scale\n We define the characteristic scale as the scale that\nproduces peak of Laplacian response\ncharacteristic scale\nT. Lindeberg (1998). \"Feature detection with automatic scale selection.\" International Journal of Computer Vision 30 (2): pp 77--116.\n25\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Laplacian-of-Gaussian (LoG)\n Interest points:\n5\nLocal maxima in scale space of Laplacian-of- Gaussian\n4\n)()(yyxxLL\n3\n2\n\n26\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Laplacian-of-Gaussian (LoG)\n Interest points:\n5\nLocal maxima in scale space of Laplacian-of- Gaussian\n4\n)()(yyxxLL\n3\n2\n\n27\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Laplacian-of-Gaussian (LoG)\n Interest points:\n5\nLocal maxima in scale space of Laplacian-of- Gaussian\n4\n)()(yyxxLL\n3\n2\n\n28\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Laplacian-of-Gaussian (LoG)\n Interest points:\n5\nLocal maxima in scale space of Laplacian-of- Gaussian\n4\n)()(yyxxLL\n3\n2\n\n29\nLihi Zelnik-Manor, Computer Vision\n List of (x, y, σ)",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "LoG detector: workflow\n30\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "LoG detector: workflow\n31\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "LoG detector: example result\n32\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "LoG technical details\n We can efficiently approximate the Laplacian with a\ndifference of Gaussians:  Laplacian:\n2(,,)(,)xxyyLGxyGxy\n DoG = Difference of Gaussians:\n(,,)(,)DoGGxykGxyL\n33\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today\n Local invariant features\n Motivation  Requirements, invariances\n Keypoint localization\n Harris corner detector  Hessian detector\n Scale invariant region selection\n Automatic scale selection  Laplacian-of-Gaussian detector  Difference-of-Gaussian detector\n34\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Key point localization with DoG\nDetect maxima of difference-of- Gaussian (DoG) in scale space\nThen reject points with low contrast (threshold)\nEliminate edge responses Subtract\nEliminate edge responses Subtract\nCandidate keypoints: list of (x,y,σ)\n35",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "DoG – efficient computation\n Computation in Gaussian scale pyramid\nSampling with step 𝜎4 = 2\nOriginal image\n36\nLihi Zelnik-Manor, Computer Vision\n[Lowe 2004]",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "DoG – example result\n37\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "DoG – example result\n38\nLihi Zelnik-Manor, Computer Vision\n(a)\n233x189 image\n(b) 832 DOG extrema\n(c) 729 left after peak value threshold\n(d) 536 left after testing ratio of principle curvatures (removing\nedge responses)",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Scale Invariant Detection: Summary  Given: two images of the same scene with a large scale\ndifference between them\n Goal: find the same interest points independently in each image\n Solution: search for maxima of suitable functions in scale and in\nspace (over the image) 1. 2. Difference of Gaussians (DoG) – fast approximation of LoG 3. Harris + Laplace 4. Hessian + Laplace\nLaplacian of Gaussian (LoG)\n39\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Invariant regions\n So far our regions are:\n Translation invariant (since they are centered at keypoints)  Scale-invariant\n What about:  Orientation?\n40\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Rotation invariant regions\n Find local orientation  Compute a weighted histogram of gradient directions\n Find the dominant direction\n Normalize orientation\n Rotate patch according to\nthis angle\n This puts the patches into a\ncanonical orientation.\n41\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Invariant regions\n So far our regions are:\n Translation invariant (since they are centered at keypoints)  Scale-invariant  Rotation-invariant\n What about:\n Other transformations?\n42\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine invariant regions\n Above we considered:\nSimilarity transform (rotation + uniform scale)\n Now we go on to:\nAffine transform (rotation + non-uniform scale)\n43\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine adaption\n Problem\n Determine the characteristic shape of the region\n Assumption\n Shape can be described by a “local affine frame”\n Solution\n Use a circular window to compute the second-moment matrix  Compute eigenvectors to adapt the circle to an ellipse  Re-compute second-moment matrix using the new window  Iterate…\n44\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Iterative affine adaptation\n1. Detect keypoint (e.g., multi-scale Harris) 2. Automatically select the scales (e.g., Laplace signature) 3. Adapt affine shape based on second order moment matrix 4. Refine point location\n45\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine normalization (deskewing)\n Steps:\n Rotate the ellipse’s main axis to horizontal  Scale the x axis, such that it forms a circle\nrotate\nrescale\n46\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine adaptation example\nScale-invariant regions\n47\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine adaptation example\nAffine-adapted regions\n48\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine invariant regions example\n49\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Affine invariant regions - summary\n50\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "51\nEnd – From points to regions\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/10-points-to-regions.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "e0a2e8b62a3c4362955efc286237aa40.json": [
      {
        "type": "application/pdf",
        "text": "1\nCamera calibration\nLihi Zelnik-Manor, Computer Vision\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Goal of calibration\n Estimate the intrinsic and extrinsic parameters from one\nor multiple images\n𝑓\n'wPMPKRTP\n𝑝\n𝑝\n0001xyscKc\n123rRrr\nxyztTtt\n2\nLihi Zelnik-Manor, Computer Vision\nP",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The calibration problem\n Input\nn points 𝑃1, … , 𝑃𝑛 with known coordinates and known positions in the image 𝑝1, … , 𝑝𝑛\n Goal\nCompute intrinsic and extrinsic parameters\n3\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The calibration problem\n Input\nn points 𝑃1, … , 𝑃𝑛 with known coordinates and known positions in the image 𝑝1, … , 𝑝𝑛\n Goal\nCompute intrinsic and extrinsic parameters\nMPp\n1343332312423222114131211ZYXmmmmmmmmmmmmwwyxw\n4\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "The calibration problem\n Question\nHow many points 𝑃1, … , 𝑃𝑛 we need?\n Answer\n11 unknowns  11 equations  6 points\n In practice we use more than 6\nMPp\n1343332312423222114131211ZYXmmmmmmmmmmmmwwyxw\n5\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Method 1: Calibration using a reference object\n Place a known object in the scene\n identify correspondence between image and scene  compute mapping from scene to image\nIssues\nmust know scene geometry very accurately • must know 3D->2D correspondence",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Equations from a single point\n A point in the world projects onto a point in the image\niiMPp\n The equations we get are\n1323iiiiiiimPmPxpymPmP\n123mMmm\n7\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Equations from a single point\n This can be re-written more conveniently\n1323iiiiiiimPmPxpymPmP\n3132iiiiiixmPmPymPmP\n8\nLihi Zelnik-Manor, Computer Vision\n313200iiiiiixmPmPymPmP",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Equations from multiple points\n131111312131320000nnnnnnxmPmPymPmPxmPmPymPmP\nSolve with SVD\n11111112300000000TTTTTTTTTTTTnnnTTTnnnPxPPyPmmmPxPPyP\n1212231210TTnTmAmm\n9\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Method 2: Calibration using vanishing points\nFind vanishing points corresponding to orthogonal directions\nVertical vanishing point (at infinity)\nVanishing line\nVanishing point\n10\nVanishing point",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Calibration by orthogonal vanishing points  Solve for K:\nKRPp\n Use orthogonality as a constraint\n Model K with only f, cx, cy\nFor vanishing points\n0PPT\n10000yxcfcfK\n What if you don’t have three finite vanishing points?\n Two finite VP: solve f, get valid cx, cy closest to image center  One finite VP: cx, cy is at vanishing point; can’t solve for f\n11",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Calibration by orthogonal vanishing points\n Solve for R:\nKRPp\n Set directions of vanishing points\n e.g., X1 = [1, 0, 0]\n Each VP provides one column of R  Special properties of R\n inv(R)=RT  Each row and column of R has unit length\n12",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Extracting camera parameters\n Once we have found M up to scale, we can extract its\nintrinsic and extrinsic parameters\n When s=0 (no skew) we get\n\n123233cotcotsinsinxxyxzyyxzzrrcrttctMKRKTrcrtctrt\n13\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Extracting intrinsic parameters\n13233xxxzyyxzzrcrtctMrcrtctrt\nA\nb\n31a\n213223xycaacaa\n14\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Theorem (Faugeras, 1993)\nMKRTKRKTAb\n15\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "de",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Extracting intrinsic parameters\n13233xxxzyyxzzrcrtctMrcrtctrt\nA\nb\n213223sinsinaaaa\nf\n16\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Extracting extrinsic parameters\n13233xxxzyyxzzrcrtctMrcrtctrt\nA\nb\n23123231aaraarrr\n3311raTKb\n17\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Alternative: multi-plane calibration\nImages courtesy Jean-Yves Bouguet, Intel Corp.\nAdvantage\nOnly requires a plane • Don’t have to know positions/orientations • Good code available online! (including in OpenCV) – Matlab version by Jean-Yves Bouget:\nhttp://www.vision.caltech.edu/bouguetj/calib_doc/index.html\n–\nZhengyou Zhang’s web site: http://research.microsoft.com/~zhang/Calib/",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Calibration demo\n19\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Calibration demo\n20\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "it",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "21\nEnd – Camera calibration\nNow you know how it works\nLihi Zelnik-Manor, Computer Vision",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/14-CameraCalibration.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "e97e6c7d37614f17935fc4d5eca3280c.json": [
      {
        "type": "application/pdf",
        "text": "Action Recognition\nSlides borrowed from Derek Hoiem",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Last classes\nParts-based/articulated object models\nTracking objects",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking people • Person model = appearance + structure (+ dynamics)\nStructure and dynamics are general, appearance is person-specific\nTrying to acquire an appearance model “on the fly” can lead to drift\nInstead, can use the whole sequence to initialize the appearance model and then keep it fixed while tracking\nGiven strong structure and appearance models, tracking can essentially be done by repeated detection (with some smoothing)\nD. Ramanan, D. Forsyth, and A. Zisserman. Tracking People by Learning their Appearance. PAMI 2007.",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tracking people by learning their appearance\nTracker\nD. Ramanan, D. Forsyth, and A. Zisserman. Tracking People by Learning their Appearance. PAMI 2007.",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Top-down method to build model:\nExploit “easy” poses\nD. Ramanan, D. Forsyth, and A. Zisserman. Tracking People by Learning their Appearance. PAMI 2007.",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing people",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Temporal model\nParts cannot move too far",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example results\nhttp://www.ics.uci.edu/~dramanan/papers/pose/index.html",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Video\nhttp://www.ics.uci.edu/~dramanan/papers/pose/index.html",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "es",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "How can we identify actions?\nMotion\nHeld Objects\nPose\nNearby Objects",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing Motion\nOptical Flow with Motion History\nBobick Davis 2001",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing Motion\nSpace-Time Volumes\nBlank et al. 2005",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing Motion\nOptical Flow with Split Channels\noptical flow\nsplit into pos/neg channels\nblurred pos/neg flow\nEfros et al. 2003",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing Motion\nTracked Points\nMatikainen et al. 2009",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing Motion\nSpace-Time Interest Points\nMoving corner\nBall hits wall\nCorner detectors in space-time\nBalls collide\nBalls collide (different scale)\nLaptev 2005",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Representing Motion\nSpace-Time Interest Points\nLaptev 2005",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Examples of Action Recognition Systems\nFeature-based classification\nRecognition using pose and objects",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Action recognition as classification\nRetrieving actions in movies, Laptev and Perez, 2007",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Remember image categorization…\nTraining Images\nTraining\nTraining Labels\nImage Features\nClassifier Training\nTesting\nImage Features\nTrained Classifier\nTest Image\nTrained Classifier\nPrediction\nOutdoor",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Remember spatial pyramids….\nCompute histogram in each spatial bin",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Features for Classifying Actions\n1. Spatio-temporal pyramids\n– Image Gradients – Optical Flow",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Features for Classifying Actions\n2. Spatio-temporal interest points\nCorner detectors in space-time\nDescriptors based on Gaussian derivative filters over x, y, time",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Classification\nBoosted stubs for pyramids of optical flow, gradient\nNearest neighbor for STIP",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Searching the video for an action\n1. Detect keyframes using a trained HOG detector in each frame\n2. Classify detected keyframes as positive (e.g., “drinking”) or negative (“other”)",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Accuracy in searching video\nWith keyframe detection\nWithout keyframe detection",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "“Talk on phone”\n“Get out of car”\nLearning realistic human actions from movies, Laptev et al. 2008",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Approach\nSpace-time interest point detectors • Descriptors – HOG, HOF\nPyramid histograms (3x3x2) • SVMs with Chi-Squared Kernel\nSpatio-Temporal Binning\nInterest Points",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Results",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "et",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Take-home messages\nAction recognition is an open problem.\n– How to define actions? – How to infer them? – What are good visual cues? – How do we incorporate higher level reasoning?",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Take-home messages\nSome work done, but it is just the beginning of exploring the problem. So far… – Actions are mainly categorical (could be framed in\nterms of effect or intent)\n– Most approaches are classification using simple\nfeatures (spatial-temporal histograms of gradients or flow, s-t interest points, SIFT in images)\n– Just a couple works on how to incorporate pose\nand objects\n– Not much idea of how to reason about long-term\nactivities or to describe video sequences",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/23-Action Recognition.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ],
    "e99b70684bc24e51b3fef70834f8bc07.json": [
      {
        "type": "application/pdf",
        "text": "Object Category Detection: Statistical Templates\nSlides borrowed from Derek Hoiem and then modified some",
        "offset": "page1",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Today’s class: Object Category Detection\nOverview of object category detection\nStatistical template matching\n– Dalal-Triggs pedestrian detector (basic concept) – Viola-Jones detector (cascades, integral images) – R-CNN detector (object proposals/CNN)",
        "offset": "page2",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Object Category Detection\nFocus on object search: “Where is it?” • Build templates that quickly differentiate object patch from background patch\nDog Model\nObject or Non-Object?",
        "offset": "page3",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Challenges in modeling the object class\nIllumination\nObject pose\nClutter\nOcclusions\nIntra-class appearance\nViewpoint\nSlide from K. Grauman, B. Leibe",
        "offset": "page4",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Challenges in modeling the non-object class\nTrue Detections\nBad Localization\nConfused with Similar Object\nMisc. Background\nConfused with Dissimilar Objects",
        "offset": "page5",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "General Process of Object Recognition\nSpecify Object Model\nWhat are the object parameters?\nGenerate Hypotheses\nScore Hypotheses\nResolve Detections",
        "offset": "page6",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Specifying an object model\n1. Statistical Template in Bounding Box\n– Object is some (x,y,w,h) in image – Features defined wrt bounding box coordinates\nImage\nTemplate Visualization\nImages from Felzenszwalb",
        "offset": "page7",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Specifying an object model\n2. Articulated parts model\n– Object is configuration of parts – Each part is detectable\nImages from Felzenszwalb",
        "offset": "page8",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Specifying an object model\n3. Hybrid template/parts model\nDetections\nTemplate Visualization\nFelzenszwalb et al. 2008",
        "offset": "page9",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Specifying an object model\n4. 3D-ish model • Object is collection of 3D planar patches under affine transformation",
        "offset": "page10",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "General Process of Object Recognition\nSpecify Object Model\nGenerate Hypotheses\nPropose an alignment of the model to the image\nScore Hypotheses\nResolve Detections",
        "offset": "page11",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Generating hypotheses\n1. Sliding window\n– Test patch at each location and scale",
        "offset": "page12",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Generating hypotheses\n1. Sliding window\n– Test patch at each location and scale",
        "offset": "page13",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Generating hypotheses\n2. Voting from patches/keypoints\nInterest Points\nMatched Codebook Entries\nProbabilistic Voting\ny\ns\n3D Voting Space (continuous)\nx\nISM model by Leibe et al.",
        "offset": "page14",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Generating hypotheses\n3. Region-based proposal\nEndres Hoiem 2010",
        "offset": "page15",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "General Process of Object Recognition\nSpecify Object Model\nGenerate Hypotheses\nScore Hypotheses\nMainly-gradient based or CNN features, usually based on summary representation, many classifiers\nResolve Detections",
        "offset": "page16",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "General Process of Object Recognition\nSpecify Object Model\nGenerate Hypotheses\nScore Hypotheses\nResolve Detections\nRescore each proposed object based on whole set",
        "offset": "page17",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Resolving detection scores\n1. Non-max suppression\nScore = 0.8\nScore = 0.1\nScore = 0.8",
        "offset": "page18",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Resolving detection scores\n2. Context/reasoning\nHoiem et al. 2006\ns r e\nt\ne m\nmeters",
        "offset": "page19",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Object category detection in computer vision Goal: detect all pedestrians, cars, monkeys, etc in image",
        "offset": "page20",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Basic Steps of Category Detection\n1. Align\n– E.g., choose position, scale orientation – How to make this\ntractable?\n2. Compare\n– Compute similarity to an example object or to a summary representation\n– Which differences in appearance are important?\nAligned Possible Objects\nExemplar Summary",
        "offset": "page21",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Sliding window: a simple alignment solution",
        "offset": "page22",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Each window is separately classified",
        "offset": "page23",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Statistical Template\nObject model = sum of scores of features at fixed positions\n+3 +2 -2 -1 -2.5 = -0.5\n? > 7.5\nNon-object\n+4 +1 +0.5 +3 +0.5 = 10.5\n? > 7.5\nObject",
        "offset": "page24",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Design challenges\nHow to efficiently search for likely objects\n– Even simple models require searching hundreds of thousands of\npositions and scales • Feature design and scoring\n– How should appearance be modeled? What features\ncorrespond to the object?\nHow to deal with different viewpoints?\n– Often train different models for a few different viewpoints\nImplementation details – Window size – Aspect ratio – Translation/scale step size – Non-maxima suppression",
        "offset": "page25",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Example: Dalal-Triggs pedestrian detector\n1. Extract fixed-sized (64x128 pixel) window at each position and scale\n2. Compute HOG (histogram of gradient) features within each window\n3. Score the window with a linear SVM classifier 4. Perform non-maxima suppression to remove overlapping detections with lower scores Navneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page26",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slides by Pete Barnum\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page27",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tested with\n– RGB – LAB – Grayscale\nSlightly better performance vs. grayscale\nGamma Normalization and Compression\n– Square root – Log\nVery slightly better performance vs. no adjustment",
        "offset": "page28",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Outperforms\ncentered\nuncentered\ncubic-corrected\nSlides by Pete Barnum\ndiagonal\nSobel\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page29",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Histogram of gradient orientations\nOrientation: 9 bins (for unsigned angles)\nHistograms in 8x8 pixel cells\n– Votes weighted by magnitude – Bilinear interpolation between\ncells\nSlides by Pete Barnum\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page30",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Normalize with respect to surrounding cells\nSlides by Pete Barnum\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page31",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slides by Pete Barnum\nX=\n# orientations\n# features = 15 x 7 x 9 x 4 = 3780\n# cells\n# normalizations by neighboring cells\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page32",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "pos w\nneg w\nSlides by Pete Barnum\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page33",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Slides by Pete Barnum\npedestrian\nNavneet Dalal and Bill Triggs, Histograms of Oriented Gradients for Human Detection, CVPR05",
        "offset": "page34",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Detection examples",
        "offset": "page35",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "2 minute break\nSomething to think about… • Sliding window detectors work\n– very well for faces – fairly well for cars and pedestrians – badly for cats and dogs\nWhy are some classes easier than others?",
        "offset": "page36",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Face detection",
        "offset": "page37",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "fr",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Viola-Jones sliding window detector\nFast detection through two mechanisms • Quickly eliminate unlikely windows • Use features that are fast to compute\nViola and Jones. Rapid Object Detection using a Boosted Cascade of Simple Features (2001).",
        "offset": "page38",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Cascade for Fast Detection\nYes\nStage 1 H1(x) > t1?\nStage 2 H2(x) > t2?\n…\nStage N HN(x) > tN?\nNo\nNo\nNo\nExamples\nReject\nReject\nReject\nChoose threshold for low false negative rate • Fast classifiers early in cascade • Slow classifiers later, but most examples don’t get there\nYes\nPass",
        "offset": "page39",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Features that are fast to compute\n“Haar-like features”\n– Differences of sums of intensity – Thousands, computed at various positions and\nscales within detection window\n1 +1\nTwo-rectangle features\nThree-rectangle features\nEtc.",
        "offset": "page40",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Features that are fast to compute\n“Haar-like features”\n– Differences of sums of intensity – Thousands, computed at various positions and\nscales within detection window\n1 +1\nTwo-rectangle features",
        "offset": "page41",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Integral Images\nii = cumsum(cumsum(im, 1), 2)\nx, y\nii(x,y) = Sum of the values in the grey region\nHow to compute B-A?\nHow to compute A+D-B-C?",
        "offset": "page42",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Feature selection with Adaboost\nCreate a large pool of features (180K) • Select features that are discriminative and work well together – “Weak learner” = feature + threshold + parity\n– Choose weak learner that minimizes error on the\nweighted training set\n– Reweight",
        "offset": "page43",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Adaboost",
        "offset": "page44",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "so",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Top 2 selected features",
        "offset": "page45",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Viola-Jones details\n38 stages with 1, 10, 25, 50 … features – 6061 total used out of 180K candidates – 10 features evaluated on average\nTraining Examples\n– 4916 positive examples – 10000 negative examples collected after each stage\nScanning\n– Scale detector rather than image – Scale steps = 1.25 (factor between two consecutive scales) – Translation 1*scale (# pixels between two consecutive windows)\nNon-max suppression: average coordinates of overlapping boxes\nTrain 3 classifiers and take vote",
        "offset": "page46",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Viola Jones Results\nSpeed = 15 FPS (in 2001)\nMIT + CMU face dataset",
        "offset": "page47",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Viola Jones Results",
        "offset": "page48",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Viola Jones Results",
        "offset": "page49",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "ca",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Fails in commercial face detection\nhttp://www.oddee.com/item_98248.aspx",
        "offset": "page50",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Detection with CNNs",
        "offset": "page51",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "R-CNN (Girshick et al. CVPR 2014)\nReplace sliding windows with “selective search” region proposals (Uijilings et al. IJCV 2013)\nExtract rectangles around regions and resize to 227x227 • Extract features with fine-tuned CNN (that was initialized with network trained on ImageNet before training)\nClassify last layer of network features with SVM\nhttp://arxiv.org/pdf/1311.2524.pdf",
        "offset": "page52",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Region proposals\nHierarchical image segmentation • Random sampling of regions as object proposals\nUijilings et al. IJCV 2013",
        "offset": "page53",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Sliding window vs. region proposals\nSliding window • Comprehensive search over position, scale (sometimes aspect, though expensive) • Typically 100K candidates • Simple • Speed boost through\nRegion proposals • Search over regions guided by image contours/patterns with varying aspect/size • Typically 2-10K candidates • Random (not repeatable) • Requires a preprocess\nconvolution often possible\n(currently 1-5s)\nRepeatable • Even with many candidates, may not be a good fit to object\nOften requires resizing patch to fit fixed size • More likely to provide candidates with very good object fit",
        "offset": "page54",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Improvements in Object Detection\n0.7\n) 7 0 0 2 C O V ( n o i s i c e r P e g a r e v A n a e M\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nHOG Template\n0\n2005\n2007\n2008\n2009\n2010\n2012\nStatistical Template Matching\nHOG: Dalal-Triggs 2005\n2013\n2014",
        "offset": "page55",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Improvements in Object Detection\n0.7\n) 7 0 0 2 C O V ( n o i s i c e r P e g a r e v A n a e M\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nDeformable Parts Model (v1-v5)\nHOG Template\n0\n2005\n2007\n2008\n2009\n2010\n2012\nBetter Models of Complex Categories\nHOG: Dalal-Triggs 2005 DPM: Felzenszwalb et al. 2008-2012\n2013\n2014",
        "offset": "page56",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Regionlets, Wang et al\nFeature histograms are built in variable regions (vs fixed size cells, 8x8 HOG for example)\nFeature extraction regions are normalized to detection windows. • Deformation handling is learned from data. • Regionlets model is not limited by a fixed scale or aspect ratio.",
        "offset": "page57",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Improvements in Object Detection\n0.7\n) 7 0 0 2 C O V ( n o i s i c e r P e g a r e v A n a e M\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nDeformable Parts Model (v1-v5)\nRegionlets\nHOG Template\n0\n2005\n2007\n2008\n2009\n2010\n2012\n2013\n2014\nBetter Features\nHOG: Dalal-Triggs 2005 DPM: Felzenszwalb et al. 2008-2012\nRegionlets: Wang et al. 2013 R-CNN: Girshick et al. 2014",
        "offset": "page58",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Improvements in Object Detection\n0.7\n) 7 0 0 2 C O V ( n o i s i c e r P e g a r e v A n a e M\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nDeformable Parts Model (v1-v5)\nR-CNN\nRegionlets\nHOG Template\n0\n2005\n2007\n2008\n2009\n2010\n2012\n2013\n2014\nKey Advance: Learn effective features from massive amounts of labeled data and adapt to new tasks with less data\nBetter Features\nHOG: Dalal-Triggs 2005 DPM: Felzenszwalb et al. 2008-2012\nRegionlets: Wang et al. 2013 R-CNN: Girshick et al. 2014",
        "offset": "page59",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Mistakes are often reasonable Bicycle: AP = 72.8%\nR-CNN results",
        "offset": "page61",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Strengths and Weaknesses of Statistical Template Approach\nStrengths • Works very well for non-deformable objects: faces,\ncars, upright pedestrians\nFast detection\nWeaknesses • Sliding window has difficulty with deformable objects (proposals works with flexible features works better)\nNot robust to occlusion • Requires lots of training data",
        "offset": "page62",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Tricks of the trade\nDetails in feature computation really matter\n– E.g., normalization in Dalal-Triggs improves detection rate by\n27% at fixed false positive rate\nTemplate size\n– Typical choice for sliding window is size of smallest detectable\nobject\n– For CNNs, typically based on what pretrained features are\navailable\n“Jittering” to create synthetic positive examples\n– Create slightly rotated, translated, scaled, mirrored versions as\nextra positive examples\nBootstrapping to get hard negative examples Randomly sample negative examples Train detector Sample negative examples that score > -1 Repeat until all high-scoring negative examples fit in memory\n1. 2. 3. 4.",
        "offset": "page63",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Influential Works in Detection\nSung-Poggio (1994, 1998) : ~2100 citations – Basic idea of statistical template detection (I think), bootstrapping to get “face-like”\nnegative examples, multiple whole-face prototypes (in 1994)\nRowley-Baluja-Kanade (1996-1998) : ~4200\n– “Parts” at fixed position, non-maxima suppression, simple cascade, rotation, pretty\ngood accuracy, fast\nSchneiderman-Kanade (1998-2000,2004) : ~2250 – Careful feature/classifier engineering, excellent results, cascade\nViola-Jones (2001, 2004) : ~20,000\n– Haar-like features, Adaboost as feature selection, hyper-cascade, very fast, easy to\nimplement\nDalal-Triggs (2005) : ~11000\n– Careful feature engineering, excellent results, HOG feature, online code Felzenszwalb-Huttenlocher (2000): ~1600 – Efficient way to solve part-based detectors Felzenszwalb-McAllester-Ramanan (2008,2010)? ~4000 – Excellent template/parts-based blend • Girshick-Donahue-Darrell-Malik (2014 ) ~300\n– Region proposals + fine-tuned CNN features (marks significant advance in accuracy",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "over hog-based methods)",
        "offset": "page64",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "da",
        "filetype": "application/pdf"
      },
      {
        "type": "application/pdf",
        "text": "Summary: statistical templates\nPropose Window\nExtract Features\nClassify\nSVM\nBoosted stubs\nHOG\nNeural network\nSliding window: scan image pyramid\nFast randomized features\nRegion proposals: edge/region-based, resize to fixed window\nCNN features\nPost- process\nNon-max suppression\nSegment or refine localization",
        "offset": "page65",
        "ref": "/home/ameer/Kaleidoo/server/uploads/17-TemplateDetection.pdf",
        "lang": "en",
        "filetype": "application/pdf"
      }
    ]
  },
  "Video": {}
}
